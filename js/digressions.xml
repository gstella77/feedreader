<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:media="http://search.yahoo.com/mrss/" xmlns:creativeCommons="http://backend.userland.com/creativeCommonsRssModule" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">
  <channel>

    <atom:link href="http://feeds.soundcloud.com/users/soundcloud:users:119277822/sounds.rss?before=173277531" rel="next" type="application/rss+xml" />
    <title>Linear Digressions</title>
    <link>http://lineardigressions.com</link>
    <pubDate>Mon, 09 Oct 2017 02:28:24 +0000</pubDate>
    <lastBuildDate>Mon, 09 Oct 2017 02:28:24 +0000</lastBuildDate>
    <ttl>60</ttl>
    <language>en</language>
    <copyright>All rights reserved</copyright>
    <webMaster>feeds@soundcloud.com (SoundCloud Feeds)</webMaster>
    <description>Linear Digressions is a podcast about machine learning and data science.  Machine learning is being used to solve a ton of interesting problems, and to accomplish goals that were out of reach even a few short years ago.</description>
    <itunes:subtitle>Explorations in Machine Learning and Data Science</itunes:subtitle>

    <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
    <itunes:explicit>no</itunes:explicit>
    <itunes:image href="http://benjaffe.me/static/podcast-icon.jpg" />
    <image>
      <url>http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg</url>
      <title>Linear Digressions</title>
      <link>http://lineardigressions.com</link>
    </image>

    <atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/rss+xml" href="http://feeds.feedburner.com/udacity-linear-digressions" /><feedburner:info uri="udacity-linear-digressions" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><media:copyright>All rights reserved</media:copyright><media:thumbnail url="http://benjaffe.me/static/podcast-icon.jpg" /><media:keywords>data,science,machine,learning,linear,digressions</media:keywords><media:category scheme="http://www.itunes.com/dtds/podcast-1.0.dtd">Technology</media:category><itunes:owner><itunes:email>hello@lineardigressions.com</itunes:email><itunes:name>Ben Jaffe and Katie Malone</itunes:name></itunes:owner><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><itunes:summary>In each episode, your hosts explore machine learning and data science through interesting (and often very unusual) applications.</itunes:summary><itunes:category text="Technology" /><creativeCommons:license>http://creativecommons.org/licenses/by-nc-sa/3.0/</creativeCommons:license><item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/345996498</guid>
      <title>Re - Release: Kalman Runners</title>
      <pubDate>Mon, 09 Oct 2017 02:28:24 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/c-vJ6-ayoG4/re-release-kalman-runners</link>
      <itunes:duration>00:17:53</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In honor of the Chicago marathon this weekend (and due in large part to Katie recovering from running in it...) we have a re-release of an episode about Kalman filters, which is part algorithm part elaborate metaphor for figuring out, if you're running a race but don't have a watch, how fast you're going.

Katie's Chicago race report:

miles 1-13: light ankle pain, lovely cool weather, the most fun EVAR
miles 13-17: no more ankle pain but quads start getting tight, it's a little more effort
miles 17-20: oof, really tight legs but still plenty of gas in then tank.
miles 20-23: it's warmer out now, legs hurt a lot but running through Pilsen and Chinatown is too fun to notice
mile 24: ugh cramp everything hurts
miles 25-26.2: awesome crowd support, really tired and loving every second
Final time: 3:54:35</itunes:summary>
      <itunes:subtitle>In honor of the Chicago marathon this weekend (an…</itunes:subtitle>
      <description>In honor of the Chicago marathon this weekend (and due in large part to Katie recovering from running in it...) we have a re-release of an episode about Kalman filters, which is part algorithm part elaborate metaphor for figuring out, if you're running a race but don't have a watch, how fast you're going.

Katie's Chicago race report:

miles 1-13: light ankle pain, lovely cool weather, the most fun EVAR
miles 13-17: no more ankle pain but quads start getting tight, it's a little more effort
miles 17-20: oof, really tight legs but still plenty of gas in then tank.
miles 20-23: it's warmer out now, legs hurt a lot but running through Pilsen and Chinatown is too fun to notice
mile 24: ugh cramp everything hurts
miles 25-26.2: awesome crowd support, really tired and loving every second
Final time: 3:54:35&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/c-vJ6-ayoG4" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/13oAEqT8lOI/345996498-linear-digressions-re-release-kalman-runners.mp3" fileSize="25768470" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/re-release-kalman-runners</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/13oAEqT8lOI/345996498-linear-digressions-re-release-kalman-runners.mp3" length="25768470" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/345996498-linear-digressions-re-release-kalman-runners.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/344945754</guid>
      <title>Neural Net Dropout</title>
      <pubDate>Mon, 02 Oct 2017 03:32:56 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/xFenLmvb9Ms/neural-net-dropout</link>
      <itunes:duration>00:18:53</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Neural networks are complex models with many parameters and can be prone to overfitting.  There's a surprisingly simple way to guard against this: randomly destroy connections between hidden units, also known as dropout.  It seems counterintuitive that undermining the structural integrity of the neural net makes it robust against overfitting, but in the world of neural nets, weirdness is just how things go sometimes.

Relevant links: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</itunes:summary>
      <itunes:subtitle>Neural networks are complex models with many para…</itunes:subtitle>
      <description>Neural networks are complex models with many parameters and can be prone to overfitting.  There's a surprisingly simple way to guard against this: randomly destroy connections between hidden units, also known as dropout.  It seems counterintuitive that undermining the structural integrity of the neural net makes it robust against overfitting, but in the world of neural nets, weirdness is just how things go sometimes.

Relevant links: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/xFenLmvb9Ms" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/71oB-qEdBxI/344945754-linear-digressions-neural-net-dropout.mp3" fileSize="27204160" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/neural-net-dropout</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/71oB-qEdBxI/344945754-linear-digressions-neural-net-dropout.mp3" length="27204160" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/344945754-linear-digressions-neural-net-dropout.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/343891449</guid>
      <title>Disciplined Data Science</title>
      <pubDate>Mon, 25 Sep 2017 01:49:41 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/dc1p9bDQv3o/disciplined-data-science</link>
      <itunes:duration>00:29:34</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>As data science matures as a field, it's becoming clearer what attributes a data science team needs to have to elevate their work to the next level.  Most of our episodes are about the cool work being done by other people, but this one summarizes some thinking Katie's been doing herself around how to guide data science teams toward more mature, effective practices.  We'll go through five key characteristics of great data science teams, which we collectively refer to as "disciplined data science," and why they matter.</itunes:summary>
      <itunes:subtitle>As data science matures as a field, it's becoming…</itunes:subtitle>
      <description>As data science matures as a field, it's becoming clearer what attributes a data science team needs to have to elevate their work to the next level.  Most of our episodes are about the cool work being done by other people, but this one summarizes some thinking Katie's been doing herself around how to guide data science teams toward more mature, effective practices.  We'll go through five key characteristics of great data science teams, which we collectively refer to as "disciplined data science," and why they matter.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/dc1p9bDQv3o" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/BV0eCSZqOxQ/343891449-linear-digressions-disciplined-data-science.mp3" fileSize="42592372" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/disciplined-data-science</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/BV0eCSZqOxQ/343891449-linear-digressions-disciplined-data-science.mp3" length="42592372" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/343891449-linear-digressions-disciplined-data-science.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/342828639</guid>
      <title>Hurricane Forecasting</title>
      <pubDate>Mon, 18 Sep 2017 01:37:15 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/X1XiGKjJspg/hurricane-forecasting</link>
      <itunes:duration>00:27:57</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>It's been a busy hurricane season in the Southeastern United States, with millions of people making life-or-death decisions based on the forecasts around where the hurricanes will hit and with what intensity.  In this episode we'll deconstruct those models, talking about the different types of models, the theory behind them, and how they've evolved through the years.</itunes:summary>
      <itunes:subtitle>It's been a busy hurricane season in the Southeas…</itunes:subtitle>
      <description>It's been a busy hurricane season in the Southeastern United States, with millions of people making life-or-death decisions based on the forecasts around where the hurricanes will hit and with what intensity.  In this episode we'll deconstruct those models, talking about the different types of models, the theory behind them, and how they've evolved through the years.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/X1XiGKjJspg" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TrjluVSL-D8/342828639-linear-digressions-hurricane-forecasting.mp3" fileSize="40263295" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/hurricane-forecasting</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TrjluVSL-D8/342828639-linear-digressions-hurricane-forecasting.mp3" length="40263295" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/342828639-linear-digressions-hurricane-forecasting.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/341822881</guid>
      <title>Finding Spy Planes with Machine Learning</title>
      <pubDate>Mon, 11 Sep 2017 02:11:22 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/PHrPg_MOJ2Y/finding-spy-planes-with-machine-learning</link>
      <itunes:duration>00:18:09</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>There are law enforcement surveillance aircraft circling over the United States every day, and in this episode, we'll talk about how some folks at BuzzFeed used public data and machine learning to find them.  The fun thing here, in our opinion, is the blend of intrigue (spy planes!) with tech journalism and a heavy dash of publicly available and reproducible analysis code so that you (yes, you!) can see exactly how BuzzFeed identifies the surveillance planes.</itunes:summary>
      <itunes:subtitle>There are law enforcement surveillance aircraft c…</itunes:subtitle>
      <description>There are law enforcement surveillance aircraft circling over the United States every day, and in this episode, we'll talk about how some folks at BuzzFeed used public data and machine learning to find them.  The fun thing here, in our opinion, is the blend of intrigue (spy planes!) with tech journalism and a heavy dash of publicly available and reproducible analysis code so that you (yes, you!) can see exactly how BuzzFeed identifies the surveillance planes.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/PHrPg_MOJ2Y" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/8rpEyo9NnD0/341822881-linear-digressions-finding-spy-planes-with-machine-learning.mp3" fileSize="26149649" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/finding-spy-planes-with-machine-learning</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/8rpEyo9NnD0/341822881-linear-digressions-finding-spy-planes-with-machine-learning.mp3" length="26149649" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/341822881-linear-digressions-finding-spy-planes-with-machine-learning.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/340795203</guid>
      <title>Data Provenance</title>
      <pubDate>Mon, 04 Sep 2017 01:35:00 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/idsh8SL5awQ/data-provenance</link>
      <itunes:duration>00:22:48</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Software engineers are familiar with the idea of versioning code, so you can go back later and revive a past state of the system.  For data scientists who might want to reconstruct past models, though, it's not just about keeping the modeling code.  It's also about saving a version of the data that made the model.  There are a lot of other benefits to keeping track of datasets, so in this episode we'll talk about data lineage or data provenance.</itunes:summary>
      <itunes:subtitle>Software engineers are familiar with the idea of …</itunes:subtitle>
      <description>Software engineers are familiar with the idea of versioning code, so you can go back later and revive a past state of the system.  For data scientists who might want to reconstruct past models, though, it's not just about keeping the modeling code.  It's also about saving a version of the data that made the model.  There are a lot of other benefits to keeping track of datasets, so in this episode we'll talk about data lineage or data provenance.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/idsh8SL5awQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/wBMNe9Kvz0o/340795203-linear-digressions-data-provenance.mp3" fileSize="32846609" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/data-provenance</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/wBMNe9Kvz0o/340795203-linear-digressions-data-provenance.mp3" length="32846609" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/340795203-linear-digressions-data-provenance.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/339789401</guid>
      <title>Adversarial Examples</title>
      <pubDate>Mon, 28 Aug 2017 02:25:14 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ePMK6_H2h0U/adversarial-examples</link>
      <itunes:duration>00:16:11</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Even as we rely more and more on machine learning algorithms to help with everyday decision-making, we're learning more and more about how they're frighteningly easy to fool sometimes.  Today we have a roundup of a few successful efforts to create robust adversarial examples, including what it means for an adversarial example to be robust and what this might mean for machine learning in the future.</itunes:summary>
      <itunes:subtitle>Even as we rely more and more on machine learning…</itunes:subtitle>
      <description>Even as we rely more and more on machine learning algorithms to help with everyday decision-making, we're learning more and more about how they're frighteningly easy to fool sometimes.  Today we have a roundup of a few successful efforts to create robust adversarial examples, including what it means for an adversarial example to be robust and what this might mean for machine learning in the future.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ePMK6_H2h0U" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/GV2WWwGPZWA/339789401-linear-digressions-adversarial-examples.mp3" fileSize="23312124" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/adversarial-examples</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/GV2WWwGPZWA/339789401-linear-digressions-adversarial-examples.mp3" length="23312124" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/339789401-linear-digressions-adversarial-examples.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/338764019</guid>
      <title>Jupyter Notebooks</title>
      <pubDate>Mon, 21 Aug 2017 01:09:32 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/cAjAeCYn8tQ/jupyter-notebooks</link>
      <itunes:duration>00:15:50</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This week's episode is just in time for JupyterCon in NYC, August 22-25...

Jupyter notebooks are probably familiar to a lot of data nerds out there as a great open-source tool for exploring data, doing quick visualizations, and packaging code snippets with explanations for sharing your work with others.  If you're not a data person, or you are but you haven't tried out Jupyter notebooks yet, here's your nudge to go give them a try.  In this episode we'll go back to the old days, before notebooks, and talk about all the ways that data scientists like to work that wasn't particularly well-suited to the command line + text editor setup, and talk about how notebooks have evolved over their lifetime to become even more powerful and well-suited to the data scientist's workflow.</itunes:summary>
      <itunes:subtitle>This week's episode is just in time for JupyterCo…</itunes:subtitle>
      <description>This week's episode is just in time for JupyterCon in NYC, August 22-25...

Jupyter notebooks are probably familiar to a lot of data nerds out there as a great open-source tool for exploring data, doing quick visualizations, and packaging code snippets with explanations for sharing your work with others.  If you're not a data person, or you are but you haven't tried out Jupyter notebooks yet, here's your nudge to go give them a try.  In this episode we'll go back to the old days, before notebooks, and talk about all the ways that data scientists like to work that wasn't particularly well-suited to the command line + text editor setup, and talk about how notebooks have evolved over their lifetime to become even more powerful and well-suited to the data scientist's workflow.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/cAjAeCYn8tQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/nt2M35Yv5uM/338764019-linear-digressions-jupyter-notebooks.mp3" fileSize="22812454" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/jupyter-notebooks</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/nt2M35Yv5uM/338764019-linear-digressions-jupyter-notebooks.mp3" length="22812454" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/338764019-linear-digressions-jupyter-notebooks.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/337766465</guid>
      <title>Curing Cancer with Machine Learning is Super Hard</title>
      <pubDate>Mon, 14 Aug 2017 01:49:52 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/QEXiac2p6Cw/curing-cancer-with-machine-learning-is-super-hard</link>
      <itunes:duration>00:19:20</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Today, a dispatch on what can go wrong when machine learning hype outpaces reality: a high-profile partnership between IBM Watson and MD Anderson Cancer Center has recently hit the rocks as it turns out to be tougher than expected to cure cancer with artificial intelligence.  There are enough conflicting accounts in the media to make it tough to say exactly went wrong, but it's a good chance to remind ourselves that even in a post-AI world, hard problems remain hard.</itunes:summary>
      <itunes:subtitle>Today, a dispatch on what can go wrong when machi…</itunes:subtitle>
      <description>Today, a dispatch on what can go wrong when machine learning hype outpaces reality: a high-profile partnership between IBM Watson and MD Anderson Cancer Center has recently hit the rocks as it turns out to be tougher than expected to cure cancer with artificial intelligence.  There are enough conflicting accounts in the media to make it tough to say exactly went wrong, but it's a good chance to remind ourselves that even in a post-AI world, hard problems remain hard.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/QEXiac2p6Cw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/NxixxqJDadM/337766465-linear-digressions-curing-cancer-with-machine-learning-is-super-hard.mp3" fileSize="27857430" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/curing-cancer-with-machine-learning-is-super-hard</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/NxixxqJDadM/337766465-linear-digressions-curing-cancer-with-machine-learning-is-super-hard.mp3" length="27857430" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/337766465-linear-digressions-curing-cancer-with-machine-learning-is-super-hard.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/336786494</guid>
      <title>KL Divergence</title>
      <pubDate>Mon, 07 Aug 2017 03:07:15 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/XB6YlmwP5yc/kl-divergence</link>
      <itunes:duration>00:25:38</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Kullback Leibler divergence, or KL divergence, is a measure of information loss when you try to approximate one distribution with another distribution.  It comes to us originally from information theory, but today underpins other, more machine-learning-focused algorithms like t-SNE.  And boy oh boy can it be tough to explain.  But we're trying our hardest in this episode!</itunes:summary>
      <itunes:subtitle>Kullback Leibler divergence, or KL divergence, is…</itunes:subtitle>
      <description>Kullback Leibler divergence, or KL divergence, is a measure of information loss when you try to approximate one distribution with another distribution.  It comes to us originally from information theory, but today underpins other, more machine-learning-focused algorithms like t-SNE.  And boy oh boy can it be tough to explain.  But we're trying our hardest in this episode!&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/XB6YlmwP5yc" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/2X_AwsQzyck/336786494-linear-digressions-kl-divergence.mp3" fileSize="36926726" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/kl-divergence</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/2X_AwsQzyck/336786494-linear-digressions-kl-divergence.mp3" length="36926726" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/336786494-linear-digressions-kl-divergence.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/335578907</guid>
      <title>Sabermetrics</title>
      <pubDate>Mon, 31 Jul 2017 01:15:37 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/E91S2Adl7ks/sabermetrics</link>
      <itunes:duration>00:25:48</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>It's moneyball time!  SABR (the Society for American Baseball Research) is the world's largest organization of statistics-minded baseball enthusiasts, who are constantly applying the craft of scientific analysis to trying to figure out who are the best baseball teams and players.  It can be hard to objectively measure sports greatness, but baseball has a data-rich history and plenty of nerdy fans interested in analyzing that data.  In this episode we'll dissect a few of the metrics from standard baseball and compare them to related metrics from Sabermetrics, so you can nerd out more effectively at your next baseball game.</itunes:summary>
      <itunes:subtitle>It's moneyball time!  SABR (the Society for Ameri…</itunes:subtitle>
      <description>It's moneyball time!  SABR (the Society for American Baseball Research) is the world's largest organization of statistics-minded baseball enthusiasts, who are constantly applying the craft of scientific analysis to trying to figure out who are the best baseball teams and players.  It can be hard to objectively measure sports greatness, but baseball has a data-rich history and plenty of nerdy fans interested in analyzing that data.  In this episode we'll dissect a few of the metrics from standard baseball and compare them to related metrics from Sabermetrics, so you can nerd out more effectively at your next baseball game.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/E91S2Adl7ks" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/AMxa5hyqFIU/335578907-linear-digressions-sabermetrics.mp3" fileSize="37171233" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/sabermetrics</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/AMxa5hyqFIU/335578907-linear-digressions-sabermetrics.mp3" length="37171233" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/335578907-linear-digressions-sabermetrics.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/334515037</guid>
      <title>What Data Scientists Can Learn from Software Engineers</title>
      <pubDate>Mon, 24 Jul 2017 01:52:26 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/oBxPwdngDR8/what-data-scientists-can-learn-from-software-engineers</link>
      <itunes:duration>00:23:46</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>We're back again with friend of the pod Walt, former software engineer extraordinaire and current data scientist extraordinaire, to talk about some best practices from software engineering that are ready to jump the fence over to data science.  If last week's episode was for software engineers who are interested in becoming more like data scientists, then this week's episode is for data scientists who are looking to improve their game with best practices from software engineering.</itunes:summary>
      <itunes:subtitle>We're back again with friend of the pod Walt, for…</itunes:subtitle>
      <description>We're back again with friend of the pod Walt, former software engineer extraordinaire and current data scientist extraordinaire, to talk about some best practices from software engineering that are ready to jump the fence over to data science.  If last week's episode was for software engineers who are interested in becoming more like data scientists, then this week's episode is for data scientists who are looking to improve their game with best practices from software engineering.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/oBxPwdngDR8" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/thIDnRQGCOA/334515037-linear-digressions-what-data-scientists-can-learn-from-software-engineers.mp3" fileSize="34243428" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/what-data-scientists-can-learn-from-software-engineers</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/thIDnRQGCOA/334515037-linear-digressions-what-data-scientists-can-learn-from-software-engineers.mp3" length="34243428" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/334515037-linear-digressions-what-data-scientists-can-learn-from-software-engineers.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/333510317</guid>
      <title>Software Engineering to Data Science</title>
      <pubDate>Mon, 17 Jul 2017 02:36:00 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Kej7QHzGBpE/walt-1-produced</link>
      <itunes:duration>00:19:05</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Data scientists and software engineers often work side by side, building out and scaling technical products and services that are data-heavy but also require a lot of software engineering to build and maintain.  In this episode, we'll chat with a Friend of the Pod named Walt, who started out as a software engineer but works as a data scientist now.  We'll talk about that transition from software engineering to data science, and what special capabilities software engineers have that data scientists might benefit from knowing about (and vice versa).</itunes:summary>
      <itunes:subtitle>Data scientists and software engineers often work…</itunes:subtitle>
      <description>Data scientists and software engineers often work side by side, building out and scaling technical products and services that are data-heavy but also require a lot of software engineering to build and maintain.  In this episode, we'll chat with a Friend of the Pod named Walt, who started out as a software engineer but works as a data scientist now.  We'll talk about that transition from software engineering to data science, and what special capabilities software engineers have that data scientists might benefit from knowing about (and vice versa).&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Kej7QHzGBpE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/1IS6ngwNfYA/333510317-linear-digressions-walt-1-produced.mp3" fileSize="27491925" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/walt-1-produced</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/1IS6ngwNfYA/333510317-linear-digressions-walt-1-produced.mp3" length="27491925" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/333510317-linear-digressions-walt-1-produced.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/332432194</guid>
      <title>Re-Release: Fighting Cholera with Data, 1854</title>
      <pubDate>Mon, 10 Jul 2017 00:19:56 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/iE1IIeLpqFg/re-release-fighting-cholera-with-data-1854</link>
      <itunes:duration>00:12:04</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This episode was first released in November 2014.

In the 1850s, there were a lot of things we didn’t know yet: how to create an airplane, how to split an atom, or how to control the spread of a common but deadly disease: cholera.

When a cholera outbreak in London killed scores of people, a doctor named John Snow used it as a chance to study whether the cause might be very small organisms that were spreading through the water supply (the prevailing theory at the time was miasma, or “bad air”). By tracing the geography of all the deaths from the outbreak, Snow was practicing elementary data science--and stumbled upon one of history’s most famous outliers.

In this episode, we’ll tell you more about this single data point, a case of cholera that cracked the case wide open for Snow and provided critical validation for the germ theory of disease.</itunes:summary>
      <itunes:subtitle>This episode was first released in November 2014.…</itunes:subtitle>
      <description>This episode was first released in November 2014.

In the 1850s, there were a lot of things we didn’t know yet: how to create an airplane, how to split an atom, or how to control the spread of a common but deadly disease: cholera.

When a cholera outbreak in London killed scores of people, a doctor named John Snow used it as a chance to study whether the cause might be very small organisms that were spreading through the water supply (the prevailing theory at the time was miasma, or “bad air”). By tracing the geography of all the deaths from the outbreak, Snow was practicing elementary data science--and stumbled upon one of history’s most famous outliers.

In this episode, we’ll tell you more about this single data point, a case of cholera that cracked the case wide open for Snow and provided critical validation for the germ theory of disease.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/iE1IIeLpqFg" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/4krN6fn-79A/332432194-linear-digressions-re-release-fighting-cholera-with-data-1854.mp3" fileSize="17376268" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/re-release-fighting-cholera-with-data-1854</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/4krN6fn-79A/332432194-linear-digressions-re-release-fighting-cholera-with-data-1854.mp3" length="17376268" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/332432194-linear-digressions-re-release-fighting-cholera-with-data-1854.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/331217467</guid>
      <title>Re-Release: Data Mining Enron</title>
      <pubDate>Sun, 02 Jul 2017 17:53:42 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/PP63UGAemtg/re-release-data-mining-enron</link>
      <itunes:duration>00:32:16</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This episode was first release in February 2015.

In 2000, Enron was one of the largest and companies in the world, praised far and wide for its innovations in energy distribution and many other markets. By 2002, it was apparent that many bad apples had been cooking the books, and billions of dollars and thousands of jobs disappeared.

In the aftermath, surprisingly, one of the greatest datasets in all of machine learning was born--the Enron emails corpus. Hundreds of thousands of emails amongst top executives were made public; there's no realistic chance any dataset like this will ever be made public again.

But the dataset that was released has gone on to immortality, serving as the basis for a huge variety of advances in machine learning and other fields.</itunes:summary>
      <itunes:subtitle>This episode was first release in February 2015.
…</itunes:subtitle>
      <description>This episode was first release in February 2015.

In 2000, Enron was one of the largest and companies in the world, praised far and wide for its innovations in energy distribution and many other markets. By 2002, it was apparent that many bad apples had been cooking the books, and billions of dollars and thousands of jobs disappeared.

In the aftermath, surprisingly, one of the greatest datasets in all of machine learning was born--the Enron emails corpus. Hundreds of thousands of emails amongst top executives were made public; there's no realistic chance any dataset like this will ever be made public again.

But the dataset that was released has gone on to immortality, serving as the basis for a huge variety of advances in machine learning and other fields.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/PP63UGAemtg" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/8IEoNdyYXjc/331217467-linear-digressions-re-release-data-mining-enron.mp3" fileSize="46466854" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/re-release-data-mining-enron</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/8IEoNdyYXjc/331217467-linear-digressions-re-release-data-mining-enron.mp3" length="46466854" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/331217467-linear-digressions-re-release-data-mining-enron.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/330116463</guid>
      <title>Factorization Machines</title>
      <pubDate>Mon, 26 Jun 2017 02:23:14 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/mIJKnAemnNk/factorization-machines</link>
      <itunes:duration>00:19:54</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>What do you get when you cross a support vector machine with matrix factorization?  You get a factorization machine, and a darn fine algorithm for recommendation engines.</itunes:summary>
      <itunes:subtitle>What do you get when you cross a support vector m…</itunes:subtitle>
      <description>What do you get when you cross a support vector machine with matrix factorization?  You get a factorization machine, and a darn fine algorithm for recommendation engines.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/mIJKnAemnNk" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/4h165N4XnsQ/330116463-linear-digressions-factorization-machines.mp3" fileSize="28653642" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/factorization-machines</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/4h165N4XnsQ/330116463-linear-digressions-factorization-machines.mp3" length="28653642" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/330116463-linear-digressions-factorization-machines.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/328763446</guid>
      <title>Anscombe's Quartet</title>
      <pubDate>Mon, 19 Jun 2017 02:19:56 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/SLANCFdkrwE/anscombes-quartet-produced</link>
      <itunes:duration>00:15:39</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Anscombe's Quartet is a set of four datasets that have the same mean, variance and correlation but look very different.  It's easy to think that having a good set of summary statistics (like mean, variance and correlation) can tell you everything important about a dataset, or at least enough to know if two datasets are extremely similar or extremely different, but Anscombe's Quartet will always be standing behind you, laughing at how silly that idea is.

Anscombe's Quartet was devised in 1973 as an example of how summary statistics can be misleading, but today we can even do one better: the Datasaurus Dozen is a set of twelve datasets, all extremely visually distinct, that have the same summary stats as a source dataset that, there's no other way to put this, looks like a dinosaur.  It's an example of how datasets can be generated to look like almost anything while still preserving arbitrary summary statistics.  In other words, Anscombe's Quartets can be generated at-will and we all should be reminded to visualize our data (not just compute summary statistics) if we want to claim to really understand it.</itunes:summary>
      <itunes:subtitle>Anscombe's Quartet is a set of four datasets that…</itunes:subtitle>
      <description>Anscombe's Quartet is a set of four datasets that have the same mean, variance and correlation but look very different.  It's easy to think that having a good set of summary statistics (like mean, variance and correlation) can tell you everything important about a dataset, or at least enough to know if two datasets are extremely similar or extremely different, but Anscombe's Quartet will always be standing behind you, laughing at how silly that idea is.

Anscombe's Quartet was devised in 1973 as an example of how summary statistics can be misleading, but today we can even do one better: the Datasaurus Dozen is a set of twelve datasets, all extremely visually distinct, that have the same summary stats as a source dataset that, there's no other way to put this, looks like a dinosaur.  It's an example of how datasets can be generated to look like almost anything while still preserving arbitrary summary statistics.  In other words, Anscombe's Quartets can be generated at-will and we all should be reminded to visualize our data (not just compute summary statistics) if we want to claim to really understand it.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/SLANCFdkrwE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/J5O07pnqPb8/328763446-linear-digressions-anscombes-quartet-produced.mp3" fileSize="22556663" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/anscombes-quartet-produced</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/J5O07pnqPb8/328763446-linear-digressions-anscombes-quartet-produced.mp3" length="22556663" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/328763446-linear-digressions-anscombes-quartet-produced.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/327634971</guid>
      <title>Traffic Metering Algorithms</title>
      <pubDate>Mon, 12 Jun 2017 03:01:49 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/XDk15cWq4nQ/traffic-metering-algorithms-1</link>
      <itunes:duration>00:18:34</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Originally release June 2016

This episode is for all you (us) traffic nerds--we're talking about the hidden structure underlying traffic on-ramp metering systems. These systems slow down the flow of traffic onto highways so that the highways don't get overloaded with cars and clog up. If you're someone who listens to podcasts while commuting, and especially if your area has on-ramp metering, you'll never look at highway access control the same way again (yeah, we know this is super nerdy; it's also super awesome).</itunes:summary>
      <itunes:subtitle>Originally release June 2016

This episode is for…</itunes:subtitle>
      <description>Originally release June 2016

This episode is for all you (us) traffic nerds--we're talking about the hidden structure underlying traffic on-ramp metering systems. These systems slow down the flow of traffic onto highways so that the highways don't get overloaded with cars and clog up. If you're someone who listens to podcasts while commuting, and especially if your area has on-ramp metering, you'll never look at highway access control the same way again (yeah, we know this is super nerdy; it's also super awesome).&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/XDk15cWq4nQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Q2FqRxp34eM/327634971-linear-digressions-traffic-metering-algorithms-1.mp3" fileSize="26750883" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/traffic-metering-algorithms-1</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Q2FqRxp34eM/327634971-linear-digressions-traffic-metering-algorithms-1.mp3" length="26750883" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/327634971-linear-digressions-traffic-metering-algorithms-1.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/326076803</guid>
      <title>Page Rank</title>
      <pubDate>Mon, 05 Jun 2017 01:46:35 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/CEORZ7xtZ4c/page-rank</link>
      <itunes:duration>00:19:58</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The year: 1998.  The size of the web: 150 million pages.  The problem: information retrieval.  How do you find the "best" web pages to return in response to a query?  A graduate student named Larry Page had an idea for how it could be done better and created a search engine as a research project.  That search engine was called Google.</itunes:summary>
      <itunes:subtitle>The year: 1998.  The size of the web: 150 million…</itunes:subtitle>
      <description>The year: 1998.  The size of the web: 150 million pages.  The problem: information retrieval.  How do you find the "best" web pages to return in response to a query?  A graduate student named Larry Page had an idea for how it could be done better and created a search engine as a research project.  That search engine was called Google.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/CEORZ7xtZ4c" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Y_s7JNOqycg/326076803-linear-digressions-page-rank.mp3" fileSize="28762730" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/page-rank</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Y_s7JNOqycg/326076803-linear-digressions-page-rank.mp3" length="28762730" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/326076803-linear-digressions-page-rank.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/324969280</guid>
      <title>Fractional Dimensions</title>
      <pubDate>Mon, 29 May 2017 02:54:46 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/C5yN1SCb6Dg/fractional-dimensions</link>
      <itunes:duration>00:20:28</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>We chat about fractional dimensions, and what the actual heck those are.</itunes:summary>
      <itunes:subtitle>We chat about fractional dimensions, and what the…</itunes:subtitle>
      <description>We chat about fractional dimensions, and what the actual heck those are.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/C5yN1SCb6Dg" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/QUgzVXcSOdM/324969280-linear-digressions-fractional-dimensions.mp3" fileSize="29473678" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/fractional-dimensions</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/QUgzVXcSOdM/324969280-linear-digressions-fractional-dimensions.mp3" length="29473678" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/324969280-linear-digressions-fractional-dimensions.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/323852156</guid>
      <title>Things You Learn When Building Models for Big Data</title>
      <pubDate>Mon, 22 May 2017 01:44:13 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/9E1zVppM7H0/things-you-learn-when-building-models-for-big-data</link>
      <itunes:duration>00:21:39</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>As more and more data gets collected seemingly every day, and data scientists use that data for modeling, the technical limits associated with machine learning on big datasets keep getting pushed back.  This week is a first-hand case study in using scikit-learn (a popular python machine learning library) on multi-terabyte datasets, which is something that Katie does a lot for her day job at Civis Analytics.  There are a lot of considerations for doing something like this--cloud computing, artful use of parallelization, considerations of model complexity, and the computational demands of training vs. prediction, to name just a few.</itunes:summary>
      <itunes:subtitle>As more and more data gets collected seemingly ev…</itunes:subtitle>
      <description>As more and more data gets collected seemingly every day, and data scientists use that data for modeling, the technical limits associated with machine learning on big datasets keep getting pushed back.  This week is a first-hand case study in using scikit-learn (a popular python machine learning library) on multi-terabyte datasets, which is something that Katie does a lot for her day job at Civis Analytics.  There are a lot of considerations for doing something like this--cloud computing, artful use of parallelization, considerations of model complexity, and the computational demands of training vs. prediction, to name just a few.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/9E1zVppM7H0" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/r3pBFtuE3Eg/323852156-linear-digressions-things-you-learn-when-building-models-for-big-data.mp3" fileSize="31172055" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/things-you-learn-when-building-models-for-big-data</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/r3pBFtuE3Eg/323852156-linear-digressions-things-you-learn-when-building-models-for-big-data.mp3" length="31172055" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/323852156-linear-digressions-things-you-learn-when-building-models-for-big-data.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/322657868</guid>
      <title>How to Find New Things to Learn</title>
      <pubDate>Mon, 15 May 2017 01:49:26 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/28dqlZPgwaY/how-to-find-new-things-to-learn</link>
      <itunes:duration>00:17:54</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>If you're anything like us, you a) always are curious to learn more about data science and machine learning and stuff, and b) are usually overwhelmed by how much content is out there (not all of it very digestible).  We hope this podcast is a part of the solution for you, but if you're looking to go farther (who isn't?) then we have a few new resources that are presenting high-quality content in a fresh, accessible way.  Boring old PDFs full of inscrutable math notation, your days are numbered!</itunes:summary>
      <itunes:subtitle>If you're anything like us, you a) always are cur…</itunes:subtitle>
      <description>If you're anything like us, you a) always are curious to learn more about data science and machine learning and stuff, and b) are usually overwhelmed by how much content is out there (not all of it very digestible).  We hope this podcast is a part of the solution for you, but if you're looking to go farther (who isn't?) then we have a few new resources that are presenting high-quality content in a fresh, accessible way.  Boring old PDFs full of inscrutable math notation, your days are numbered!&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/28dqlZPgwaY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/YP6qnr3OIFc/322657868-linear-digressions-how-to-find-new-things-to-learn.mp3" fileSize="25794175" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/how-to-find-new-things-to-learn</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/YP6qnr3OIFc/322657868-linear-digressions-how-to-find-new-things-to-learn.mp3" length="25794175" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/322657868-linear-digressions-how-to-find-new-things-to-learn.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/321493998</guid>
      <title>Federated Learning</title>
      <pubDate>Mon, 08 May 2017 01:50:40 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/QoLDwAVIW3g/federated-learning</link>
      <itunes:duration>00:14:03</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>As machine learning makes its way into more and more mobile devices, an interesting question presents itself: how can we have an algorithm learn from training data that's being supplied as users interact with the algorithm?  In other words, how do we do machine learning when the training dataset is distributed across many devices, imbalanced, and the usage associated with any one user needs to be obscured somewhat to protect the privacy of that user?  Enter Federated Learning, a set of related algorithms from Google that are designed to help out in exactly this scenario.  If you've used keyboard shortcuts or autocomplete on an Android phone, chances are you've encountered Federated Learning even if you didn't know it.</itunes:summary>
      <itunes:subtitle>As machine learning makes its way into more and m…</itunes:subtitle>
      <description>As machine learning makes its way into more and more mobile devices, an interesting question presents itself: how can we have an algorithm learn from training data that's being supplied as users interact with the algorithm?  In other words, how do we do machine learning when the training dataset is distributed across many devices, imbalanced, and the usage associated with any one user needs to be obscured somewhat to protect the privacy of that user?  Enter Federated Learning, a set of related algorithms from Google that are designed to help out in exactly this scenario.  If you've used keyboard shortcuts or autocomplete on an Android phone, chances are you've encountered Federated Learning even if you didn't know it.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/QoLDwAVIW3g" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kUbI5m79s_Y/321493998-linear-digressions-federated-learning.mp3" fileSize="20231347" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/federated-learning</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kUbI5m79s_Y/321493998-linear-digressions-federated-learning.mp3" length="20231347" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/321493998-linear-digressions-federated-learning.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/320270672</guid>
      <title>Word2Vec</title>
      <pubDate>Mon, 01 May 2017 02:17:36 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/_0C4h1Kp7fU/word2vec</link>
      <itunes:duration>00:17:59</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Word2Vec is probably the go-to algorithm for vectorizing text data these days.  Which makes sense, because it is wicked cool.  Word2Vec has it all: neural networks, skip-grams and bag-of-words implementations, a multiclass classifier that gets swapped out for a binary classifier, made-up dummy words, and a model that isn't actually used to predict anything (usually).  And all that's before we get to the part about how Word2Vec allows you to do algebra with text.  Seriously, this stuff is cool.</itunes:summary>
      <itunes:subtitle>Word2Vec is probably the go-to algorithm for vect…</itunes:subtitle>
      <description>Word2Vec is probably the go-to algorithm for vectorizing text data these days.  Which makes sense, because it is wicked cool.  Word2Vec has it all: neural networks, skip-grams and bag-of-words implementations, a multiclass classifier that gets swapped out for a binary classifier, made-up dummy words, and a model that isn't actually used to predict anything (usually).  And all that's before we get to the part about how Word2Vec allows you to do algebra with text.  Seriously, this stuff is cool.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/_0C4h1Kp7fU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/5qtHPVQ-6Iw/320270672-linear-digressions-word2vec.mp3" fileSize="25893231" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/word2vec</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/5qtHPVQ-6Iw/320270672-linear-digressions-word2vec.mp3" length="25893231" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/320270672-linear-digressions-word2vec.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/319173085</guid>
      <title>Feature Processing for Text Analytics</title>
      <pubDate>Mon, 24 Apr 2017 02:17:24 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/V9rnHIIXfm8/feature-processing-for-text-analytics</link>
      <itunes:duration>00:17:28</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>It seems like every day there's more and more machine learning problems that involve learning on text data, but text itself makes for fairly lousy inputs to machine learning algorithms.  That's why there are text vectorization algorithms, which re-format text data so it's ready for using for machine learning.  In this episode, we'll go over some of the most common and useful ways to preprocess text data for machine learning.</itunes:summary>
      <itunes:subtitle>It seems like every day there's more and more mac…</itunes:subtitle>
      <description>It seems like every day there's more and more machine learning problems that involve learning on text data, but text itself makes for fairly lousy inputs to machine learning algorithms.  That's why there are text vectorization algorithms, which re-format text data so it's ready for using for machine learning.  In this episode, we'll go over some of the most common and useful ways to preprocess text data for machine learning.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/V9rnHIIXfm8" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/ZcVu8ZwM6dE/319173085-linear-digressions-feature-processing-for-text-analytics.mp3" fileSize="25160339" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/feature-processing-for-text-analytics</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/ZcVu8ZwM6dE/319173085-linear-digressions-feature-processing-for-text-analytics.mp3" length="25160339" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/319173085-linear-digressions-feature-processing-for-text-analytics.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/318014251</guid>
      <title>Education Analytics</title>
      <pubDate>Mon, 17 Apr 2017 02:09:26 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/XYt652YQK8I/education-analytics</link>
      <itunes:duration>00:21:05</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This week we'll hop into the rapidly developing industry around predictive analytics for education.  For many of the students who eventually drop out, data science is showing that there might be early warning signs that the student is in trouble--we'll talk about what some of those signs are, and then dig into the meatier questions around discrimination, who owns a student's data, and correlation vs. causation.  Spoiler: we have more questions than we have answers on this one.

Bonus appearance from Maeby the dog, who isn't a data scientist but does like to steal food off the counter.</itunes:summary>
      <itunes:subtitle>This week we'll hop into the rapidly developing i…</itunes:subtitle>
      <description>This week we'll hop into the rapidly developing industry around predictive analytics for education.  For many of the students who eventually drop out, data science is showing that there might be early warning signs that the student is in trouble--we'll talk about what some of those signs are, and then dig into the meatier questions around discrimination, who owns a student's data, and correlation vs. causation.  Spoiler: we have more questions than we have answers on this one.

Bonus appearance from Maeby the dog, who isn't a data scientist but does like to steal food off the counter.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/XYt652YQK8I" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/AM74Iieby7Q/318014251-linear-digressions-education-analytics.mp3" fileSize="30377724" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/education-analytics</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/AM74Iieby7Q/318014251-linear-digressions-education-analytics.mp3" length="30377724" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/318014251-linear-digressions-education-analytics.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/316925326</guid>
      <title>A Technical Deep Dive on Stanley, the First Self-Driving Car</title>
      <pubDate>Mon, 10 Apr 2017 01:50:01 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/GdY1eiwtnmQ/a-technical-deep-dive-on-stanley-the-first-self-driving-car</link>
      <itunes:duration>00:40:42</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In our follow-up episode to last week's introduction to the first self-driving car, we will be doing a technical deep dive this week and talking about the most important systems for getting a car to drive itself 140 miles across the desert.  Lidar?  You betcha!  Drive-by-wire?  Of course!  Probabilistic terrain reconstruction?  Absolutely!  All this and more this week on Linear Digressions.</itunes:summary>
      <itunes:subtitle>In our follow-up episode to last week's introduct…</itunes:subtitle>
      <description>In our follow-up episode to last week's introduction to the first self-driving car, we will be doing a technical deep dive this week and talking about the most important systems for getting a car to drive itself 140 miles across the desert.  Lidar?  You betcha!  Drive-by-wire?  Of course!  Probabilistic terrain reconstruction?  Absolutely!  All this and more this week on Linear Digressions.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/GdY1eiwtnmQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Nqh9X0iyvc4/316925326-linear-digressions-a-technical-deep-dive-on-stanley-the-first-self-driving-car.mp3" fileSize="58603762" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/a-technical-deep-dive-on-stanley-the-first-self-driving-car</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Nqh9X0iyvc4/316925326-linear-digressions-a-technical-deep-dive-on-stanley-the-first-self-driving-car.mp3" length="58603762" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/316925326-linear-digressions-a-technical-deep-dive-on-stanley-the-first-self-driving-car.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/315776652</guid>
      <title>An Introduction to Stanley, the First Self-Driving Car</title>
      <pubDate>Mon, 03 Apr 2017 01:34:17 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/gV29mGeafwU/an-introduction-to-stanley-the-first-self-driving-car</link>
      <itunes:duration>00:13:07</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In October 2005, 23 cars lined up in the desert for a 140 mile race.  Not one of those cars had a driver.  This was the DARPA grand challenge to see if anyone could build an autonomous vehicle capable of navigating a desert route (and if so, whose car could do it the fastest); the winning car, Stanley, now sits in the Smithsonian Museum in Washington DC as arguably the world's first real self-driving car.  In this episode (part one of a two-parter), we'll revisit the DARPA grand challenge from 2005 and the rules and constraints of what it took for Stanley to win the competition.  Next week, we'll do a deep dive into Stanley's control systems and overall operation and what the key systems were that allowed Stanley to win the race.</itunes:summary>
      <itunes:subtitle>In October 2005, 23 cars lined up in the desert f…</itunes:subtitle>
      <description>In October 2005, 23 cars lined up in the desert for a 140 mile race.  Not one of those cars had a driver.  This was the DARPA grand challenge to see if anyone could build an autonomous vehicle capable of navigating a desert route (and if so, whose car could do it the fastest); the winning car, Stanley, now sits in the Smithsonian Museum in Washington DC as arguably the world's first real self-driving car.  In this episode (part one of a two-parter), we'll revisit the DARPA grand challenge from 2005 and the rules and constraints of what it took for Stanley to win the competition.  Next week, we'll do a deep dive into Stanley's control systems and overall operation and what the key systems were that allowed Stanley to win the race.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/gV29mGeafwU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/jgdLSvIZMX4/315776652-linear-digressions-an-introduction-to-stanley-the-first-self-driving-car.mp3" fileSize="18902864" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/an-introduction-to-stanley-the-first-self-driving-car</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/jgdLSvIZMX4/315776652-linear-digressions-an-introduction-to-stanley-the-first-self-driving-car.mp3" length="18902864" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/315776652-linear-digressions-an-introduction-to-stanley-the-first-self-driving-car.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/314630327</guid>
      <title>Feature Importance</title>
      <pubDate>Mon, 27 Mar 2017 01:53:25 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/2GArYOZuiX0/feature-importance</link>
      <itunes:duration>00:20:15</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Figuring out what features actually matter in a model is harder to figure out than you might first guess.  When a human makes a decision, you can just ask them--why did you do that?  But with machine learning models, not so much.  That's why we wanted to talk a bit about both regularization (again) and also other ways that you can figure out which models have the biggest impact on the predictions of your model.</itunes:summary>
      <itunes:subtitle>Figuring out what features actually matter in a m…</itunes:subtitle>
      <description>Figuring out what features actually matter in a model is harder to figure out than you might first guess.  When a human makes a decision, you can just ask them--why did you do that?  But with machine learning models, not so much.  That's why we wanted to talk a bit about both regularization (again) and also other ways that you can figure out which models have the biggest impact on the predictions of your model.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/2GArYOZuiX0" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/bcuCf3Rgjls/314630327-linear-digressions-feature-importance.mp3" fileSize="29159582" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/feature-importance</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/bcuCf3Rgjls/314630327-linear-digressions-feature-importance.mp3" length="29159582" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/314630327-linear-digressions-feature-importance.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/313333794</guid>
      <title>Space Codes!</title>
      <pubDate>Mon, 20 Mar 2017 02:50:57 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/fcKzF8s3Em0/space-codes</link>
      <itunes:duration>00:23:56</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>It's hard to get information to and from Mars.  Mars is very far away, and expensive to get to, and the bandwidth for passing messages with Earth is not huge.  The messages you do pass have to traverse millions of miles, which provides ample opportunity for the message to get corrupted or scrambled.  How, then, can you encode messages so that errors can be detected and corrected?  How does the decoding process allow you to actually find and correct the errors?  In this episode, we'll talk about three pieces of the process (Reed-Solomon codes, convolutional codes, and Viterbi decoding) that allow the scientists at NASA to talk to our rovers on Mars.</itunes:summary>
      <itunes:subtitle>It's hard to get information to and from Mars.  M…</itunes:subtitle>
      <description>It's hard to get information to and from Mars.  Mars is very far away, and expensive to get to, and the bandwidth for passing messages with Earth is not huge.  The messages you do pass have to traverse millions of miles, which provides ample opportunity for the message to get corrupted or scrambled.  How, then, can you encode messages so that errors can be detected and corrected?  How does the decoding process allow you to actually find and correct the errors?  In this episode, we'll talk about three pieces of the process (Reed-Solomon codes, convolutional codes, and Viterbi decoding) that allow the scientists at NASA to talk to our rovers on Mars.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/fcKzF8s3Em0" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/2_NSLWIXE5Q/313333794-linear-digressions-space-codes.mp3" fileSize="34464111" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/space-codes</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/2_NSLWIXE5Q/313333794-linear-digressions-space-codes.mp3" length="34464111" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/313333794-linear-digressions-space-codes.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/312081585</guid>
      <title>Finding (and Studying) Wikipedia Trolls</title>
      <pubDate>Mon, 13 Mar 2017 01:44:55 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/YzPYsXIQmMc/finding-and-studying-wikipedia-trolls</link>
      <itunes:duration>00:15:50</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>You may be shocked to hear this, but sometimes, people on the internet can be mean.  For some of us this is just a minor annoyance, but if you're a maintainer or contributor of a large project like Wikipedia, abusive users can be a huge problem.  Fighting the problem starts with understanding it, and understanding it starts with measuring it; the thing is, for a huge website like Wikipedia, there can be millions of edits and comments where abuse might happen, so measurement isn't a simple task.  That's where machine learning comes in: by building an "abuse classifier," and pointing it at the Wikipedia edit corpus, researchers at Jigsaw and the Wikimedia foundation are for the first time able to estimate abuse rates and curate a dataset of abusive incidents.  Then those researchers, and others, can use that dataset to study the pathologies and effects of Wikipedia trolls.</itunes:summary>
      <itunes:subtitle>You may be shocked to hear this, but sometimes, p…</itunes:subtitle>
      <description>You may be shocked to hear this, but sometimes, people on the internet can be mean.  For some of us this is just a minor annoyance, but if you're a maintainer or contributor of a large project like Wikipedia, abusive users can be a huge problem.  Fighting the problem starts with understanding it, and understanding it starts with measuring it; the thing is, for a huge website like Wikipedia, there can be millions of edits and comments where abuse might happen, so measurement isn't a simple task.  That's where machine learning comes in: by building an "abuse classifier," and pointing it at the Wikipedia edit corpus, researchers at Jigsaw and the Wikimedia foundation are for the first time able to estimate abuse rates and curate a dataset of abusive incidents.  Then those researchers, and others, can use that dataset to study the pathologies and effects of Wikipedia trolls.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/YzPYsXIQmMc" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/BtlKueq-SE0/312081585-linear-digressions-finding-and-studying-wikipedia-trolls.mp3" fileSize="22804931" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/finding-and-studying-wikipedia-trolls</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/BtlKueq-SE0/312081585-linear-digressions-finding-and-studying-wikipedia-trolls.mp3" length="22804931" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/312081585-linear-digressions-finding-and-studying-wikipedia-trolls.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/310922164</guid>
      <title>A Sprint Through What's New in Neural Networks</title>
      <pubDate>Mon, 06 Mar 2017 03:27:12 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/XdD8u8_ushI/a-sprint-through-whats-new-in-neural-networks</link>
      <itunes:duration>00:16:56</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Advances in neural networks are moving fast enough that, even though it seems like we talk about them all the time around here, it also always seems like we're barely keeping up.  So this week we have another installment in our "neural nets: they so smart!" series, talking about three topics.  And all the topics this week were listener suggestions, too!</itunes:summary>
      <itunes:subtitle>Advances in neural networks are moving fast enoug…</itunes:subtitle>
      <description>Advances in neural networks are moving fast enough that, even though it seems like we talk about them all the time around here, it also always seems like we're barely keeping up.  So this week we have another installment in our "neural nets: they so smart!" series, talking about three topics.  And all the topics this week were listener suggestions, too!&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/XdD8u8_ushI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/92MpF4a-Nq0/310922164-linear-digressions-a-sprint-through-whats-new-in-neural-networks.mp3" fileSize="24399863" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/a-sprint-through-whats-new-in-neural-networks</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/92MpF4a-Nq0/310922164-linear-digressions-a-sprint-through-whats-new-in-neural-networks.mp3" length="24399863" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/310922164-linear-digressions-a-sprint-through-whats-new-in-neural-networks.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/309721482</guid>
      <title>Stein's Paradox</title>
      <pubDate>Mon, 27 Feb 2017 02:51:41 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/IVLipZpfcSE/steins-paradox</link>
      <itunes:duration>00:27:02</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>When you're estimating something about some object that's a member of a larger group of similar objects (say, the batting average of a baseball player, who belongs to a baseball team), how should you estimate it: use measurements of the individual, or get some extra information from the group?  The James-Stein estimator tells you how to combine individual and group information make predictions that, taken over the whole group, are more accurate than if you treated each individual, well, individually.</itunes:summary>
      <itunes:subtitle>When you're estimating something about some objec…</itunes:subtitle>
      <description>When you're estimating something about some object that's a member of a larger group of similar objects (say, the batting average of a baseball player, who belongs to a baseball team), how should you estimate it: use measurements of the individual, or get some extra information from the group?  The James-Stein estimator tells you how to combine individual and group information make predictions that, taken over the whole group, are more accurate than if you treated each individual, well, individually.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/IVLipZpfcSE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-KVYfbgxRpk/309721482-linear-digressions-steins-paradox.mp3" fileSize="38944215" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/steins-paradox</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-KVYfbgxRpk/309721482-linear-digressions-steins-paradox.mp3" length="38944215" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/309721482-linear-digressions-steins-paradox.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/308598786</guid>
      <title>Empirical Bayes</title>
      <pubDate>Mon, 20 Feb 2017 03:30:06 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/UR4gowXlYoI/empirical-bayes</link>
      <itunes:duration>00:18:57</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Say you're looking to use some Bayesian methods to estimate parameters of a system.  You've got the normalization figured out, and the likelihood, but the prior... what should you use for a prior?  Empirical Bayes has an elegant answer: look to your previous experience, and use past measurements as a starting point in your prior.

Scratching your head about some of those terms, and why they matter?  Lucky for you, you're standing in front of a podcast episode that unpacks all of this.</itunes:summary>
      <itunes:subtitle>Say you're looking to use some Bayesian methods t…</itunes:subtitle>
      <description>Say you're looking to use some Bayesian methods to estimate parameters of a system.  You've got the normalization figured out, and the likelihood, but the prior... what should you use for a prior?  Empirical Bayes has an elegant answer: look to your previous experience, and use past measurements as a starting point in your prior.

Scratching your head about some of those terms, and why they matter?  Lucky for you, you're standing in front of a podcast episode that unpacks all of this.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/UR4gowXlYoI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/beGmR859sCY/308598786-linear-digressions-empirical-bayes.mp3" fileSize="27295066" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/empirical-bayes</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/beGmR859sCY/308598786-linear-digressions-empirical-bayes.mp3" length="27295066" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/308598786-linear-digressions-empirical-bayes.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/307453756</guid>
      <title>Endogenous Variables and Measuring Protest Effectiveness</title>
      <pubDate>Mon, 13 Feb 2017 03:31:00 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/WnvNPG3HEUQ/endogenous-variables-and-measuring-protest-effectiveness</link>
      <itunes:duration>00:16:28</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Have you been out protesting lately, or watching the protests, and wondered how much effect they might have on lawmakers?  It's a tricky question to answer, since usually we need randomly distributed treatments (e.g. big protests) to understand causality, but there's no reason to believe that big protests are actually randomly distributed.  In other words, protest size is endogenous to legislative response, and understanding cause and effect is very challenging.

So, what to do?  Well, at least in the case of studying Tea Party protest effectiveness, researchers have used rainfall, of all things, to understand the impact of a big protest.  In other words, rainfall is the instrumental variable in this analysis that cracks the scientific case open.  What does rainfall have to do with protests?  Do protests actually matter?  What do we mean when we talk about endogenous and instrumental variables?  We wouldn't be very good podcasters if we answered all those questions here--you gotta listen to this episode to find out.</itunes:summary>
      <itunes:subtitle>Have you been out protesting lately, or watching …</itunes:subtitle>
      <description>Have you been out protesting lately, or watching the protests, and wondered how much effect they might have on lawmakers?  It's a tricky question to answer, since usually we need randomly distributed treatments (e.g. big protests) to understand causality, but there's no reason to believe that big protests are actually randomly distributed.  In other words, protest size is endogenous to legislative response, and understanding cause and effect is very challenging.

So, what to do?  Well, at least in the case of studying Tea Party protest effectiveness, researchers have used rainfall, of all things, to understand the impact of a big protest.  In other words, rainfall is the instrumental variable in this analysis that cracks the scientific case open.  What does rainfall have to do with protests?  Do protests actually matter?  What do we mean when we talk about endogenous and instrumental variables?  We wouldn't be very good podcasters if we answered all those questions here--you gotta listen to this episode to find out.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/WnvNPG3HEUQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/iPRYROsoRJs/307453756-linear-digressions-endogenous-variables-and-measuring-protest-effectiveness.mp3" fileSize="23717753" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/endogenous-variables-and-measuring-protest-effectiveness</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/iPRYROsoRJs/307453756-linear-digressions-endogenous-variables-and-measuring-protest-effectiveness.mp3" length="23717753" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/307453756-linear-digressions-endogenous-variables-and-measuring-protest-effectiveness.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/306314257</guid>
      <title>Calibrated Models</title>
      <pubDate>Mon, 06 Feb 2017 01:56:12 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/rgoGCcy-bKc/calibrated-models</link>
      <itunes:duration>00:14:32</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Remember last week, when we were talking about how great the ROC curve is for evaluating models?  How things change...  This week, we're exploring calibrated risk models, because that's a kind of model that seems like it would benefit from some nice ROC analysis, but in fact the ROC AUC can steer you wrong there.</itunes:summary>
      <itunes:subtitle>Remember last week, when we were talking about ho…</itunes:subtitle>
      <description>Remember last week, when we were talking about how great the ROC curve is for evaluating models?  How things change...  This week, we're exploring calibrated risk models, because that's a kind of model that seems like it would benefit from some nice ROC analysis, but in fact the ROC AUC can steer you wrong there.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/rgoGCcy-bKc" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/sr1PzfJj1mo/306314257-linear-digressions-calibrated-models.mp3" fileSize="20932891" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/calibrated-models</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/sr1PzfJj1mo/306314257-linear-digressions-calibrated-models.mp3" length="20932891" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/306314257-linear-digressions-calibrated-models.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/305186244</guid>
      <title>Rock the ROC Curve</title>
      <pubDate>Mon, 30 Jan 2017 03:38:46 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/N7Tz40e92cc/rock-the-roc-curve</link>
      <itunes:duration>00:15:52</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This week: everybody's favorite WWII-era classifier metric!  But it's not just for winning wars, it's a fantastic go-to metric for all your classifier quality needs.</itunes:summary>
      <itunes:subtitle>This week: everybody's favorite WWII-era classifi…</itunes:subtitle>
      <description>This week: everybody's favorite WWII-era classifier metric!  But it's not just for winning wars, it's a fantastic go-to metric for all your classifier quality needs.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/N7Tz40e92cc" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/XItop5RwEq4/305186244-linear-digressions-rock-the-roc-curve.mp3" fileSize="22864490" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/rock-the-roc-curve</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/XItop5RwEq4/305186244-linear-digressions-rock-the-roc-curve.mp3" length="22864490" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/305186244-linear-digressions-rock-the-roc-curve.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/304054755</guid>
      <title>Ensemble Algorithms</title>
      <pubDate>Mon, 23 Jan 2017 02:31:26 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ykGn1-2d27k/ensemble-algorithms</link>
      <itunes:duration>00:13:08</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>If one machine learning model is good, are two models better?  In a lot of cases, the answer is yes.  If you build many ok models, and then bring them all together and use them in combination to make your final predictions, you've just created an ensemble model.  It feels a little bit like cheating, like you just got something for nothing, but the results don't like: algorithms like Random Forests and Gradient Boosting Trees (two types of ensemble algorithms) are some of the strongest out-of-the-box algorithms for classic supervised classification problems.  What makes a Random Forest random, and what does it mean to gradient boost a tree?  Have a listen and find out.</itunes:summary>
      <itunes:subtitle>If one machine learning model is good, are two mo…</itunes:subtitle>
      <description>If one machine learning model is good, are two models better?  In a lot of cases, the answer is yes.  If you build many ok models, and then bring them all together and use them in combination to make your final predictions, you've just created an ensemble model.  It feels a little bit like cheating, like you just got something for nothing, but the results don't like: algorithms like Random Forests and Gradient Boosting Trees (two types of ensemble algorithms) are some of the strongest out-of-the-box algorithms for classic supervised classification problems.  What makes a Random Forest random, and what does it mean to gradient boost a tree?  Have a listen and find out.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ykGn1-2d27k" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/ByE_u8MPdP0/304054755-linear-digressions-ensemble-algorithms.mp3" fileSize="18918537" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/ensemble-algorithms</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/ByE_u8MPdP0/304054755-linear-digressions-ensemble-algorithms.mp3" length="18918537" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/304054755-linear-digressions-ensemble-algorithms.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/302897486</guid>
      <title>How to evaluate a translation: BLEU scores</title>
      <pubDate>Mon, 16 Jan 2017 01:59:01 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Gw65nIVxZDE/how-to-evaluate-a-translation-bleu-scores</link>
      <itunes:duration>00:17:06</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>As anyone who's encountered a badly translated text could tell you, not all translations are created equal.  Some translations are smooth, fluent and sound like a poet wrote them; some are jerky, non-grammatical and awkward.  When a machine is doing the translating, it's awfully easy to end up with a robotic-sounding text; as the state of the art in machine translation improves, though, a natural question to ask is: according to what measure?  How do we quantify a "good" translation?

Enter the BLEU score, which is the standard metric for quantifying the quality of a machine translation.  BLEU rewards translations that have large overlap with human translations of sentences, with some extra heuristics thrown in to guard against weird pathologies (like full sentences getting translated as one word, redundancies, and repetition).  Nowadays, if there's a machine translation being evaluated or a new state-of-the-art system (like the Google neural machine translation we've discussed on this podcast before), chances are that there's a BLEU score going into that assessment.</itunes:summary>
      <itunes:subtitle>As anyone who's encountered a badly translated te…</itunes:subtitle>
      <description>As anyone who's encountered a badly translated text could tell you, not all translations are created equal.  Some translations are smooth, fluent and sound like a poet wrote them; some are jerky, non-grammatical and awkward.  When a machine is doing the translating, it's awfully easy to end up with a robotic-sounding text; as the state of the art in machine translation improves, though, a natural question to ask is: according to what measure?  How do we quantify a "good" translation?

Enter the BLEU score, which is the standard metric for quantifying the quality of a machine translation.  BLEU rewards translations that have large overlap with human translations of sentences, with some extra heuristics thrown in to guard against weird pathologies (like full sentences getting translated as one word, redundancies, and repetition).  Nowadays, if there's a machine translation being evaluated or a new state-of-the-art system (like the Google neural machine translation we've discussed on this podcast before), chances are that there's a BLEU score going into that assessment.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Gw65nIVxZDE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/aYitOLrq_eg/302897486-linear-digressions-how-to-evaluate-a-translation-bleu-scores.mp3" fileSize="24629322" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/how-to-evaluate-a-translation-bleu-scores</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/aYitOLrq_eg/302897486-linear-digressions-how-to-evaluate-a-translation-bleu-scores.mp3" length="24629322" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/302897486-linear-digressions-how-to-evaluate-a-translation-bleu-scores.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/301617561</guid>
      <title>Zero Shot Translation</title>
      <pubDate>Mon, 09 Jan 2017 03:20:57 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/iwhHeFIfznM/zero-shot-translation</link>
      <itunes:duration>00:25:32</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Take Google-size data, the flexibility of a neural net, and all (well, most) of the languages of the world, and what you end up with is a pile of surprises.  This episode is about some interesting features of Google's new neural machine translation system, namely that with minimal tweaking, it can accommodate many different languages in a single neural net, that it can do a half-decent job of translating between language pairs it's never been explicitly trained on, and that it seems to have its own internal representation of concepts that's independent of the language those concepts are being represented in.  Intrigued?  You should be...</itunes:summary>
      <itunes:subtitle>Take Google-size data, the flexibility of a neura…</itunes:subtitle>
      <description>Take Google-size data, the flexibility of a neural net, and all (well, most) of the languages of the world, and what you end up with is a pile of surprises.  This episode is about some interesting features of Google's new neural machine translation system, namely that with minimal tweaking, it can accommodate many different languages in a single neural net, that it can do a half-decent job of translating between language pairs it's never been explicitly trained on, and that it seems to have its own internal representation of concepts that's independent of the language those concepts are being represented in.  Intrigued?  You should be...&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/iwhHeFIfznM" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/JtyvA58iasI/301617561-linear-digressions-zero-shot-translation.mp3" fileSize="36766230" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/zero-shot-translation</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/JtyvA58iasI/301617561-linear-digressions-zero-shot-translation.mp3" length="36766230" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/301617561-linear-digressions-zero-shot-translation.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/300514154</guid>
      <title>Google Neural Machine Translation</title>
      <pubDate>Mon, 02 Jan 2017 01:44:23 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/lll0NUqt-ac/google-neural-machine-translation</link>
      <itunes:duration>00:18:12</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Recently, Google swapped out the backend for Google Translate, moving from a statistical phrase-based method to a recurrent neural network.  This marks a big change in methodology: the tried-and-true statistical translation methods that have been in use for decades are giving way to a neural net that, across the board, appears to be giving more fluent and natural-sounding translations.  This episode recaps statistical phrase-based methods, digs into the RNN architecture a little bit, and recaps the impressive results that is making us all sound a little better in our non-native languages.</itunes:summary>
      <itunes:subtitle>Recently, Google swapped out the backend for Goog…</itunes:subtitle>
      <description>Recently, Google swapped out the backend for Google Translate, moving from a statistical phrase-based method to a recurrent neural network.  This marks a big change in methodology: the tried-and-true statistical translation methods that have been in use for decades are giving way to a neural net that, across the board, appears to be giving more fluent and natural-sounding translations.  This episode recaps statistical phrase-based methods, digs into the RNN architecture a little bit, and recaps the impressive results that is making us all sound a little better in our non-native languages.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/lll0NUqt-ac" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/dNZJSBM5Ajg/300514154-linear-digressions-google-neural-machine-translation.mp3" fileSize="26206700" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/google-neural-machine-translation</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/dNZJSBM5Ajg/300514154-linear-digressions-google-neural-machine-translation.mp3" length="26206700" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/300514154-linear-digressions-google-neural-machine-translation.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/299603370</guid>
      <title>Data and the Future of Medicine : Interview with Precision Medicine Initiative researcher Matt Might</title>
      <pubDate>Mon, 26 Dec 2016 01:19:24 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Isy68HUaUzk/data-and-the-future-of-medicine-interview-with-precision-medicine-initiative-researcher-matt-might</link>
      <itunes:duration>00:34:54</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Today we are delighted to bring you an interview with Matt Might, computer scientist and medical researcher extraordinaire and architect of President Obama's Precision Medicine Initiative.  As the Obama Administration winds down, we're talking with Matt about the goals and accomplishments of precision medicine (and related projects like the Cancer Moonshot) and what he foresees as the future marriage of data and medicine.  Many thanks to Matt, our friends over at Partially Derivative (hi, Jonathon!) and the White House for arranging this opportunity to chat.  Enjoy!</itunes:summary>
      <itunes:subtitle>Today we are delighted to bring you an interview …</itunes:subtitle>
      <description>Today we are delighted to bring you an interview with Matt Might, computer scientist and medical researcher extraordinaire and architect of President Obama's Precision Medicine Initiative.  As the Obama Administration winds down, we're talking with Matt about the goals and accomplishments of precision medicine (and related projects like the Cancer Moonshot) and what he foresees as the future marriage of data and medicine.  Many thanks to Matt, our friends over at Partially Derivative (hi, Jonathon!) and the White House for arranging this opportunity to chat.  Enjoy!&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Isy68HUaUzk" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/vm2HXSHOMxM/299603370-linear-digressions-data-and-the-future-of-medicine-interview-with-precision-medicine-initiative-researcher-matt-might.mp3" fileSize="50259230" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/data-and-the-future-of-medicine-interview-with-precision-medicine-initiative-researcher-matt-might</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/vm2HXSHOMxM/299603370-linear-digressions-data-and-the-future-of-medicine-interview-with-precision-medicine-initiative-researcher-matt-might.mp3" length="50259230" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/299603370-linear-digressions-data-and-the-future-of-medicine-interview-with-precision-medicine-initiative-researcher-matt-might.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/298492727</guid>
      <title>Special Crossover Episode: Partially Derivative interview with White House Data Scientist DJ Patil</title>
      <pubDate>Sun, 18 Dec 2016 17:53:52 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/eaK-JOxVWEM/special-crossover-episode-partially-derivative-interview-with-white-house-data-scientist-dj-patil</link>
      <itunes:duration>00:46:09</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>We have the pleasure of bringing you a very special crossover episode this week: our friends at Partially Derivative (another great podcast about data science, you should check it out) recently interviewed White House Chief Data Scientist DJ Patil.  We think DJ's message about the importance and impact of data science is worth spreading, so it's our pleasure to bring it to you today.  A huge thanks to Jonathon Morgan and Partially Derivative for sharing this interview with us--enjoy!

Relevant links:
http://partiallyderivative.com/podcast/2016/12/13/dj-patil</itunes:summary>
      <itunes:subtitle>We have the pleasure of bringing you a very speci…</itunes:subtitle>
      <description>We have the pleasure of bringing you a very special crossover episode this week: our friends at Partially Derivative (another great podcast about data science, you should check it out) recently interviewed White House Chief Data Scientist DJ Patil.  We think DJ's message about the importance and impact of data science is worth spreading, so it's our pleasure to bring it to you today.  A huge thanks to Jonathon Morgan and Partially Derivative for sharing this interview with us--enjoy!

Relevant links:
http://partiallyderivative.com/podcast/2016/12/13/dj-patil&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/eaK-JOxVWEM" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/p73iHwe7m_M/298492727-linear-digressions-special-crossover-episode-partially-derivative-interview-with-white-house-data-scientist-dj-patil.mp3" fileSize="66464320" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/special-crossover-episode-partially-derivative-interview-with-white-house-data-scientist-dj-patil</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/p73iHwe7m_M/298492727-linear-digressions-special-crossover-episode-partially-derivative-interview-with-white-house-data-scientist-dj-patil.mp3" length="66464320" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/298492727-linear-digressions-special-crossover-episode-partially-derivative-interview-with-white-house-data-scientist-dj-patil.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/297483753</guid>
      <title>How to Lose at Kaggle</title>
      <pubDate>Mon, 12 Dec 2016 04:28:57 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/UpWNr5wbrfY/how-to-lose-at-kaggle</link>
      <itunes:duration>00:17:16</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Competing in a machine learning competition on Kaggle is a kind of rite of passage for data scientists.  Losing unexpectedly at the very end of the contest is also something that a lot of us have experienced.  It's not just bad luck: a very specific combination of overfitting on popular competitions can take someone who is in the top few spots in the final days of a contest and bump them down hundreds of slots in the final tally.</itunes:summary>
      <itunes:subtitle>Competing in a machine learning competition on Ka…</itunes:subtitle>
      <description>Competing in a machine learning competition on Kaggle is a kind of rite of passage for data scientists.  Losing unexpectedly at the very end of the contest is also something that a lot of us have experienced.  It's not just bad luck: a very specific combination of overfitting on popular competitions can take someone who is in the top few spots in the final days of a contest and bump them down hundreds of slots in the final tally.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/UpWNr5wbrfY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/dcN1ubZVQCE/297483753-linear-digressions-how-to-lose-at-kaggle.mp3" fileSize="24871321" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/how-to-lose-at-kaggle</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/dcN1ubZVQCE/297483753-linear-digressions-how-to-lose-at-kaggle.mp3" length="24871321" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/297483753-linear-digressions-how-to-lose-at-kaggle.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/296242241</guid>
      <title>Attacking Discrimination in Machine Learning</title>
      <pubDate>Mon, 05 Dec 2016 03:38:54 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/yzfFGN6aSRY/attacking-discrimination-in-machine-learning</link>
      <itunes:duration>00:23:20</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Imagine there's an important decision to be made about someone, like a bank deciding whether to extend a loan, or a school deciding to admit a student--unfortunately, we're all too aware that discrimination can sneak into these situations (even when everyone is acting with the best of intentions!).  Now, these decisions are often made with the assistance of machine learning and statistical models, but unfortunately these algorithms pick up on the discrimination in the world (it sneaks in through the data, which can capture inequities, which the algorithms then learn) and reproduce it.

This podcast covers some of the most common ways we can try to minimize discrimination, and why none of those ways is perfect at fixing the problem.  Then we'll get to a new idea called "equality of opportunity," which came out of Google recently and takes a pretty practical and well-aimed approach to machine learning bias.</itunes:summary>
      <itunes:subtitle>Imagine there's an important decision to be made …</itunes:subtitle>
      <description>Imagine there's an important decision to be made about someone, like a bank deciding whether to extend a loan, or a school deciding to admit a student--unfortunately, we're all too aware that discrimination can sneak into these situations (even when everyone is acting with the best of intentions!).  Now, these decisions are often made with the assistance of machine learning and statistical models, but unfortunately these algorithms pick up on the discrimination in the world (it sneaks in through the data, which can capture inequities, which the algorithms then learn) and reproduce it.

This podcast covers some of the most common ways we can try to minimize discrimination, and why none of those ways is perfect at fixing the problem.  Then we'll get to a new idea called "equality of opportunity," which came out of Google recently and takes a pretty practical and well-aimed approach to machine learning bias.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/yzfFGN6aSRY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/XPbvmuN9QsQ/296242241-linear-digressions-attacking-discrimination-in-machine-learning.mp3" fileSize="33607713" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/attacking-discrimination-in-machine-learning</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/XPbvmuN9QsQ/296242241-linear-digressions-attacking-discrimination-in-machine-learning.mp3" length="33607713" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/296242241-linear-digressions-attacking-discrimination-in-machine-learning.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/295082681</guid>
      <title>Recurrent Neural Nets</title>
      <pubDate>Mon, 28 Nov 2016 02:47:44 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Py8IcXQby8M/recurrent-neural-nets</link>
      <itunes:duration>00:12:36</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This week, we're doing a crash course in recurrent neural networks--what the structural pieces are that make a neural net recurrent, how that structure helps RNNs solve certain time series problems, and the importance of forgetfulness in RNNs.

Relevant links:
http://colah.github.io/posts/2015-08-Understanding-LSTMs/</itunes:summary>
      <itunes:subtitle>This week, we're doing a crash course in recurren…</itunes:subtitle>
      <description>This week, we're doing a crash course in recurrent neural networks--what the structural pieces are that make a neural net recurrent, how that structure helps RNNs solve certain time series problems, and the importance of forgetfulness in RNNs.

Relevant links:
http://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Py8IcXQby8M" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/BYc-K3RfmMY/295082681-linear-digressions-recurrent-neural-nets.mp3" fileSize="18157433" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/recurrent-neural-nets</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/BYc-K3RfmMY/295082681-linear-digressions-recurrent-neural-nets.mp3" length="18157433" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/295082681-linear-digressions-recurrent-neural-nets.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/294007486</guid>
      <title>Stealing a PIN with signal processing and machine learning</title>
      <pubDate>Mon, 21 Nov 2016 02:32:21 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/bXCGLvvpMlE/stealing-a-pin-with-signal-processing-and-machine-learning</link>
      <itunes:duration>00:16:55</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Want another reason to be paranoid when using the free coffee shop wifi?  Allow us to introduce WindTalker, a system that cleverly combines a dose of signal processing with a dash of machine learning to (potentially) steal the PIN from your phone transactions without ever having physical access to your phone.  This episode has it all, folks--channel state information, ICMP echo requests, low-pass filtering, PCA, dynamic time warps, and the PIN for your phone.</itunes:summary>
      <itunes:subtitle>Want another reason to be paranoid when using the…</itunes:subtitle>
      <description>Want another reason to be paranoid when using the free coffee shop wifi?  Allow us to introduce WindTalker, a system that cleverly combines a dose of signal processing with a dash of machine learning to (potentially) steal the PIN from your phone transactions without ever having physical access to your phone.  This episode has it all, folks--channel state information, ICMP echo requests, low-pass filtering, PCA, dynamic time warps, and the PIN for your phone.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/bXCGLvvpMlE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/6nu47rY_AZ4/294007486-linear-digressions-stealing-a-pin-with-signal-processing-and-machine-learning.mp3" fileSize="24378547" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/stealing-a-pin-with-signal-processing-and-machine-learning</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/6nu47rY_AZ4/294007486-linear-digressions-stealing-a-pin-with-signal-processing-and-machine-learning.mp3" length="24378547" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/294007486-linear-digressions-stealing-a-pin-with-signal-processing-and-machine-learning.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/292925887</guid>
      <title>Neural Net Cryptography</title>
      <pubDate>Mon, 14 Nov 2016 04:06:57 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/n9TzBygeyQg/neural-net-cryptography</link>
      <itunes:duration>00:16:16</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Cryptography used to be the domain of information theorists and spies.  There's a new player now: neural networks.  Given the task of communicating securely, neural networks are inventing new encryption methods that, as best we can tell, are unlike anything humans have ever seen before.

Relevant links:
http://arstechnica.co.uk/information-technology/2016/10/google-ai-neural-network-cryptography/
https://arxiv.org/pdf/1610.06918v1.pdf</itunes:summary>
      <itunes:subtitle>Cryptography used to be the domain of information…</itunes:subtitle>
      <description>Cryptography used to be the domain of information theorists and spies.  There's a new player now: neural networks.  Given the task of communicating securely, neural networks are inventing new encryption methods that, as best we can tell, are unlike anything humans have ever seen before.

Relevant links:
http://arstechnica.co.uk/information-technology/2016/10/google-ai-neural-network-cryptography/
https://arxiv.org/pdf/1610.06918v1.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/n9TzBygeyQg" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/C2sIizfFRQk/292925887-linear-digressions-neural-net-cryptography.mp3" fileSize="23435631" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/neural-net-cryptography</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/C2sIizfFRQk/292925887-linear-digressions-neural-net-cryptography.mp3" length="23435631" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/292925887-linear-digressions-neural-net-cryptography.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/291861013</guid>
      <title>Deep Blue</title>
      <pubDate>Mon, 07 Nov 2016 04:20:48 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/sJKg-rX1UDE/deep-blue</link>
      <itunes:duration>00:20:05</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In 1997, Deep Blue was the IBM algorithm/computer that did what no one, at the time, though possible: it beat the world's best chess player.  It turns out, though, that one of the most important moves in the matchup, where Deep Blue psyched out its opponent with a weird move, might not have been so inspired after all.  It might have been nothing more than a bug in the program, and it changed computer science history.

Relevant links:
https://www.wired.com/2012/09/deep-blue-computer-bug/</itunes:summary>
      <itunes:subtitle>In 1997, Deep Blue was the IBM algorithm/computer…</itunes:subtitle>
      <description>In 1997, Deep Blue was the IBM algorithm/computer that did what no one, at the time, though possible: it beat the world's best chess player.  It turns out, though, that one of the most important moves in the matchup, where Deep Blue psyched out its opponent with a weird move, might not have been so inspired after all.  It might have been nothing more than a bug in the program, and it changed computer science history.

Relevant links:
https://www.wired.com/2012/09/deep-blue-computer-bug/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/sJKg-rX1UDE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Z3ALSI8OYO8/291861013-linear-digressions-deep-blue.mp3" fileSize="28939526" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/deep-blue</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Z3ALSI8OYO8/291861013-linear-digressions-deep-blue.mp3" length="28939526" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/291861013-linear-digressions-deep-blue.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/290742629</guid>
      <title>Organizing Google's Datasets</title>
      <pubDate>Mon, 31 Oct 2016 02:17:26 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/wNH3_IWmIQw/organizing-googles-datasets</link>
      <itunes:duration>00:15:00</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>If you're a data scientist, there's a good chance you're used to working with a lot of data.  But there's a lot of data, and then there's Google-scale amounts of data.  Keeping all that data organized is a Google-sized task, and as it happens, they've built a system for that organizational challenge.  This episode is all about that system, called Goods, and in particular we'll dig into some of the details of what makes this so tough.

Relevant links: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45390.pdf</itunes:summary>
      <itunes:subtitle>If you're a data scientist, there's a good chance…</itunes:subtitle>
      <description>If you're a data scientist, there's a good chance you're used to working with a lot of data.  But there's a lot of data, and then there's Google-scale amounts of data.  Keeping all that data organized is a Google-sized task, and as it happens, they've built a system for that organizational challenge.  This episode is all about that system, called Goods, and in particular we'll dig into some of the details of what makes this so tough.

Relevant links: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45390.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/wNH3_IWmIQw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3ZfksNn9kFE/290742629-linear-digressions-organizing-googles-datasets.mp3" fileSize="21598700" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/organizing-googles-datasets</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3ZfksNn9kFE/290742629-linear-digressions-organizing-googles-datasets.mp3" length="21598700" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/290742629-linear-digressions-organizing-googles-datasets.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/289664543</guid>
      <title>Fighting Cancer with Data Science: Followup</title>
      <pubDate>Mon, 24 Oct 2016 01:58:41 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ZfdT3n3CdsA/fighting-cancer-with-data-science-followup</link>
      <itunes:duration>00:25:48</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>A few months ago, Katie started on a project for the Vice President's Cancer Moonshot surrounding how data can be used to better fight cancer.  The project is all wrapped up now, so we wanted to tell you about how that work went and what changes to cancer data policy were suggested to the Vice President.

See lineardigressions.com for links to the reports discussed on this episode.</itunes:summary>
      <itunes:subtitle>A few months ago, Katie started on a project for …</itunes:subtitle>
      <description>A few months ago, Katie started on a project for the Vice President's Cancer Moonshot surrounding how data can be used to better fight cancer.  The project is all wrapped up now, so we wanted to tell you about how that work went and what changes to cancer data policy were suggested to the Vice President.

See lineardigressions.com for links to the reports discussed on this episode.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ZfdT3n3CdsA" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-OUxX33EKz0/289664543-linear-digressions-fighting-cancer-with-data-science-followup.mp3" fileSize="37169979" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/fighting-cancer-with-data-science-followup</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-OUxX33EKz0/289664543-linear-digressions-fighting-cancer-with-data-science-followup.mp3" length="37169979" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/289664543-linear-digressions-fighting-cancer-with-data-science-followup.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/288496878</guid>
      <title>The 19-year-old determining the US election</title>
      <pubDate>Mon, 17 Oct 2016 01:01:23 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Gi-4LxjA2pQ/the-19-year-old-determining-the-us-election</link>
      <itunes:duration>00:12:28</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Sick of the presidential election yet?  We are too, but there's still almost a month to go, so let's just embrace it together.  This week, we'll talk about one of the presidential polls, which has been kind of an outlier for quite a while.  This week, the NY Times took a closer look at this poll, and was able to figure out the reason it's such an outlier.  It all goes back to a 19-year-old African American man, living in Illinois, who really likes Donald Trump...

Relevant Links:
http://www.nytimes.com/2016/10/13/upshot/how-one-19-year-old-illinois-man-is-distorting-national-polling-averages.html

followup article from LA Times, released after recording:
http://www.latimes.com/politics/la-na-pol-daybreak-poll-questions-20161013-snap-story.html</itunes:summary>
      <itunes:subtitle>Sick of the presidential election yet?  We are to…</itunes:subtitle>
      <description>Sick of the presidential election yet?  We are too, but there's still almost a month to go, so let's just embrace it together.  This week, we'll talk about one of the presidential polls, which has been kind of an outlier for quite a while.  This week, the NY Times took a closer look at this poll, and was able to figure out the reason it's such an outlier.  It all goes back to a 19-year-old African American man, living in Illinois, who really likes Donald Trump...

Relevant Links:
http://www.nytimes.com/2016/10/13/upshot/how-one-19-year-old-illinois-man-is-distorting-national-polling-averages.html

followup article from LA Times, released after recording:
http://www.latimes.com/politics/la-na-pol-daybreak-poll-questions-20161013-snap-story.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Gi-4LxjA2pQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/d4mu1FgqaCA/288496878-linear-digressions-the-19-year-old-determining-the-us-election.mp3" fileSize="17963709" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/the-19-year-old-determining-the-us-election</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/d4mu1FgqaCA/288496878-linear-digressions-the-19-year-old-determining-the-us-election.mp3" length="17963709" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/288496878-linear-digressions-the-19-year-old-determining-the-us-election.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/286906837</guid>
      <title>How to Steal a Model</title>
      <pubDate>Sun, 09 Oct 2016 22:57:24 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/-z6Se2Et5-8/how-to-steal-a-model</link>
      <itunes:duration>00:13:36</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>What does it mean to steal a model?  It means someone (the thief, presumably) can re-create the predictions of the model without having access to the algorithm itself, or the training data.  Sound far-fetched?  It isn't.  If that person can ask for predictions from the model, and he (or she) asks just the right questions, the model can be reverse-engineered right out from under you.

Relevant links:
https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf</itunes:summary>
      <itunes:subtitle>What does it mean to steal a model?  It means som…</itunes:subtitle>
      <description>What does it mean to steal a model?  It means someone (the thief, presumably) can re-create the predictions of the model without having access to the algorithm itself, or the training data.  Sound far-fetched?  It isn't.  If that person can ask for predictions from the model, and he (or she) asks just the right questions, the model can be reverse-engineered right out from under you.

Relevant links:
https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/-z6Se2Et5-8" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/NU6J2oXY9s0/286906837-linear-digressions-how-to-steal-a-model.mp3" fileSize="19604408" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/how-to-steal-a-model</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/NU6J2oXY9s0/286906837-linear-digressions-how-to-steal-a-model.mp3" length="19604408" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/286906837-linear-digressions-how-to-steal-a-model.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/285783592</guid>
      <title>Regularization</title>
      <pubDate>Mon, 03 Oct 2016 02:13:50 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/4FoR6CTUhNQ/regularization</link>
      <itunes:duration>00:17:27</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Lots of data is usually seen as a good thing.  And it is a good thing--except when it's not.  In a lot of fields, a problem arises when you have many, many features, especially if there's a somewhat smaller number of cases to learn from; supervised machine learning algorithms break, or learn spurious or un-interpretable patterns.  What to do?  Regularization can be one of your best friends here--it's a method that penalizes overly complex models, which keeps the dimensionality of your model under control.</itunes:summary>
      <itunes:subtitle>Lots of data is usually seen as a good thing.  An…</itunes:subtitle>
      <description>Lots of data is usually seen as a good thing.  And it is a good thing--except when it's not.  In a lot of fields, a problem arises when you have many, many features, especially if there's a somewhat smaller number of cases to learn from; supervised machine learning algorithms break, or learn spurious or un-interpretable patterns.  What to do?  Regularization can be one of your best friends here--it's a method that penalizes overly complex models, which keeps the dimensionality of your model under control.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/4FoR6CTUhNQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/W-2tiWVKwTM/285783592-linear-digressions-regularization.mp3" fileSize="25132754" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/regularization</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/W-2tiWVKwTM/285783592-linear-digressions-regularization.mp3" length="25132754" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/285783592-linear-digressions-regularization.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/284650055</guid>
      <title>The Cold Start Problem</title>
      <pubDate>Mon, 26 Sep 2016 02:24:38 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ZBbcorSi7rE/the-cold-start-problem</link>
      <itunes:duration>00:15:37</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>You might sometimes find that it's hard to get started doing something, but once you're going, it gets easier.  Turns out machine learning algorithms, and especially recommendation engines, feel the same way.  The more they "know" about a user, like what movies they watch and how they rate them, the better they do at suggesting new movies, which is great until you realize that you have to start somewhere.  The "cold start" problem will be our focus in this episode, both the heuristic solutions that help deal with it and a bit of realism about the importance of skepticism when someone claims a great solution to cold starts.

Relevant links:
http://repository.upenn.edu/cgi/viewcontent.cgi?article=1141&amp;context=cis_papers</itunes:summary>
      <itunes:subtitle>You might sometimes find that it's hard to get st…</itunes:subtitle>
      <description>You might sometimes find that it's hard to get started doing something, but once you're going, it gets easier.  Turns out machine learning algorithms, and especially recommendation engines, feel the same way.  The more they "know" about a user, like what movies they watch and how they rate them, the better they do at suggesting new movies, which is great until you realize that you have to start somewhere.  The "cold start" problem will be our focus in this episode, both the heuristic solutions that help deal with it and a bit of realism about the importance of skepticism when someone claims a great solution to cold starts.

Relevant links:
http://repository.upenn.edu/cgi/viewcontent.cgi?article=1141&amp;context=cis_papers&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ZBbcorSi7rE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kgtjyS72ZVw/284650055-linear-digressions-the-cold-start-problem.mp3" fileSize="22487699" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/the-cold-start-problem</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kgtjyS72ZVw/284650055-linear-digressions-the-cold-start-problem.mp3" length="22487699" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/284650055-linear-digressions-the-cold-start-problem.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/283568567</guid>
      <title>Open Source Software for Data Science</title>
      <pubDate>Mon, 19 Sep 2016 04:27:40 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/inQYXfsgwDU/open-source-software-for-data-science</link>
      <itunes:duration>00:20:05</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>If you work in tech, software or data science, there's an excellent chance you use tools that are built upon open source software.  This is software that's built and distributed not for a profit, but because everyone benefits when we work together and share tools.  Tim Head of scikit-optimize chats with us further about what it's like to maintain an open source library, how to get involved in open source, and why people like him need people like you to make it all work.</itunes:summary>
      <itunes:subtitle>If you work in tech, software or data science, th…</itunes:subtitle>
      <description>If you work in tech, software or data science, there's an excellent chance you use tools that are built upon open source software.  This is software that's built and distributed not for a profit, but because everyone benefits when we work together and share tools.  Tim Head of scikit-optimize chats with us further about what it's like to maintain an open source library, how to get involved in open source, and why people like him need people like you to make it all work.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/inQYXfsgwDU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/D6VpVDv4qH4/283568567-linear-digressions-open-source-software-for-data-science.mp3" fileSize="28920718" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/open-source-software-for-data-science</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/D6VpVDv4qH4/283568567-linear-digressions-open-source-software-for-data-science.mp3" length="28920718" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/283568567-linear-digressions-open-source-software-for-data-science.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/282494421</guid>
      <title>Scikit + Optimization = Scikit-Optimize</title>
      <pubDate>Mon, 12 Sep 2016 01:54:59 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/awFdjXz1TLM/scikit-optimization-scikit-optimize</link>
      <itunes:duration>00:15:41</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>We're excited to welcome a guest, Tim Head, who is one of the maintainers of the scikit-optimize package.  With all the talk about optimization lately, it felt appropriate to get in a few words with someone who's out there making it happen for python.

Relevant links:
https://scikit-optimize.github.io/
http://www.wildtreetech.com/</itunes:summary>
      <itunes:subtitle>We're excited to welcome a guest, Tim Head, who i…</itunes:subtitle>
      <description>We're excited to welcome a guest, Tim Head, who is one of the maintainers of the scikit-optimize package.  With all the talk about optimization lately, it felt appropriate to get in a few words with someone who's out there making it happen for python.

Relevant links:
https://scikit-optimize.github.io/
http://www.wildtreetech.com/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/awFdjXz1TLM" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/aJD1MVvrtOg/282494421-linear-digressions-scikit-optimization-scikit-optimize.mp3" fileSize="22598668" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/scikit-optimization-scikit-optimize</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/aJD1MVvrtOg/282494421-linear-digressions-scikit-optimization-scikit-optimize.mp3" length="22598668" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/282494421-linear-digressions-scikit-optimization-scikit-optimize.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/281415661</guid>
      <title>Two Cultures: Machine Learning and Statistics</title>
      <pubDate>Mon, 05 Sep 2016 01:50:05 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/PZIt3PhUy2U/two-cultures-of-statistics</link>
      <itunes:duration>00:17:29</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>It's a funny thing to realize, but data science modeling is usually about either explainability, interpretation and understanding, or it's about predictive accuracy.  But usually not both--optimizing for one tends to compromise the other.   Leo Breiman was one of the titans of both kinds of modeling, a statistician who helped bring machine learning into statistics and vice versa.  In this episode, we unpack one of his seminal papers from 2001, when machine learning was just beginning to take root, and talk about how he made clear what machine learning could do for statistics and why it's so important.

Relevant links:
http://www.math.snu.ac.kr/~hichoi/machinelearning/(Breiman)%20Statistical%20Modeling--The%20Two%20Cultures.pdf</itunes:summary>
      <itunes:subtitle>It's a funny thing to realize, but data science m…</itunes:subtitle>
      <description>It's a funny thing to realize, but data science modeling is usually about either explainability, interpretation and understanding, or it's about predictive accuracy.  But usually not both--optimizing for one tends to compromise the other.   Leo Breiman was one of the titans of both kinds of modeling, a statistician who helped bring machine learning into statistics and vice versa.  In this episode, we unpack one of his seminal papers from 2001, when machine learning was just beginning to take root, and talk about how he made clear what machine learning could do for statistics and why it's so important.

Relevant links:
http://www.math.snu.ac.kr/~hichoi/machinelearning/(Breiman)%20Statistical%20Modeling--The%20Two%20Cultures.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/PZIt3PhUy2U" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/_sd-Ecqhfzc/281415661-linear-digressions-two-cultures-of-statistics.mp3" fileSize="25184163" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/two-cultures-of-statistics</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/_sd-Ecqhfzc/281415661-linear-digressions-two-cultures-of-statistics.mp3" length="25184163" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/281415661-linear-digressions-two-cultures-of-statistics.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/280367844</guid>
      <title>Optimization Solutions</title>
      <pubDate>Mon, 29 Aug 2016 02:01:42 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/aY9KhQmA9Zo/optimization-solutions</link>
      <itunes:duration>00:20:07</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>You've got an optimization problem to solve, and a less-than-forever amount of time in which to solve it.  What do?  Use a heuristic optimization algorithm, like a hill climber or simulated annealing--we cover both in this episode!

Relevant link:
http://www.lizsander.com/programming/2015/08/04/Heuristic-Search-Algorithms.html</itunes:summary>
      <itunes:subtitle>You've got an optimization problem to solve, and …</itunes:subtitle>
      <description>You've got an optimization problem to solve, and a less-than-forever amount of time in which to solve it.  What do?  Use a heuristic optimization algorithm, like a hill climber or simulated annealing--we cover both in this episode!

Relevant link:
http://www.lizsander.com/programming/2015/08/04/Heuristic-Search-Algorithms.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/aY9KhQmA9Zo" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/0uNhls85gIw/280367844-linear-digressions-optimization-solutions.mp3" fileSize="28967112" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/optimization-solutions</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/0uNhls85gIw/280367844-linear-digressions-optimization-solutions.mp3" length="28967112" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/280367844-linear-digressions-optimization-solutions.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/279300811</guid>
      <title>Optimization Problems</title>
      <pubDate>Mon, 22 Aug 2016 00:25:56 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/86NH-K-VT4w/optimization-problems</link>
      <itunes:duration>00:17:50</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>If modeling is about predicting the unknown, optimization tries to answer the question of what to do, what decision to make, to get the best results out of a given situation.  Sometimes that's straightforward, but sometimes... not so much.  What makes an optimization problem easy or hard, and what are some of the methods for finding optimal solutions to problems?  Glad you asked!  May we recommend our latest podcast episode to you?</itunes:summary>
      <itunes:subtitle>If modeling is about predicting the unknown, opti…</itunes:subtitle>
      <description>If modeling is about predicting the unknown, optimization tries to answer the question of what to do, what decision to make, to get the best results out of a given situation.  Sometimes that's straightforward, but sometimes... not so much.  What makes an optimization problem easy or hard, and what are some of the methods for finding optimal solutions to problems?  Glad you asked!  May we recommend our latest podcast episode to you?&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/86NH-K-VT4w" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/1loW2MGrhLM/279300811-linear-digressions-optimization-problems.mp3" fileSize="25695118" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/optimization-problems</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/1loW2MGrhLM/279300811-linear-digressions-optimization-problems.mp3" length="25695118" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/279300811-linear-digressions-optimization-problems.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/278290747</guid>
      <title>Multi-level modeling for understanding DEADLY RADIOACTIVE GAS</title>
      <pubDate>Mon, 15 Aug 2016 01:49:42 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/u9PeNdXq-bI/multi-level-modeling-for-understanding-deadly-radioactive-gas</link>
      <itunes:duration>00:23:34</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Ok, this episode is only sort of about DEADLY RADIOACTIVE GAS.  It's mostly about multilevel modeling, which is a way of building models with data that has distinct, related subgroups within it.  What are multilevel models used for?  Elections (we can't get enough of 'em these days), understanding the effect that a good teacher can have on their students, and DEADLY RADIOACTIVE GAS.

Relevant links:
http://www.stat.columbia.edu/~gelman/research/published/multi2.pdf</itunes:summary>
      <itunes:subtitle>Ok, this episode is only sort of about DEADLY RAD…</itunes:subtitle>
      <description>Ok, this episode is only sort of about DEADLY RADIOACTIVE GAS.  It's mostly about multilevel modeling, which is a way of building models with data that has distinct, related subgroups within it.  What are multilevel models used for?  Elections (we can't get enough of 'em these days), understanding the effect that a good teacher can have on their students, and DEADLY RADIOACTIVE GAS.

Relevant links:
http://www.stat.columbia.edu/~gelman/research/published/multi2.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/u9PeNdXq-bI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/30jNpo9zSus/278290747-linear-digressions-multi-level-modeling-for-understanding-deadly-radioactive-gas.mp3" fileSize="33945006" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/multi-level-modeling-for-understanding-deadly-radioactive-gas</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/30jNpo9zSus/278290747-linear-digressions-multi-level-modeling-for-understanding-deadly-radioactive-gas.mp3" length="33945006" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/278290747-linear-digressions-multi-level-modeling-for-understanding-deadly-radioactive-gas.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/277286802</guid>
      <title>How Polls Got Brexit "Wrong"</title>
      <pubDate>Mon, 08 Aug 2016 01:37:18 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/r_1g8zDygNw/how-polls-got-brexit-wrong</link>
      <itunes:duration>00:15:14</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Continuing the discussion of how polls do (and sometimes don't) tell us what to expect in upcoming elections--let's take a concrete example from the recent past, shall we?  The Brexit referendum was, by and large, expected to shake out for "remain", but when the votes were counted, "leave" came out ahead.  Everyone was shocked (SHOCKED!) but maybe the polls weren't as wrong as the pundits like to claim.

Relevant links:
http://www.slate.com/articles/news_and_politics/moneybox/2016/07/why_political_betting_markets_are_failing.html
http://andrewgelman.com/2016/06/24/brexit-polling-what-went-wrong/</itunes:summary>
      <itunes:subtitle>Continuing the discussion of how polls do (and so…</itunes:subtitle>
      <description>Continuing the discussion of how polls do (and sometimes don't) tell us what to expect in upcoming elections--let's take a concrete example from the recent past, shall we?  The Brexit referendum was, by and large, expected to shake out for "remain", but when the votes were counted, "leave" came out ahead.  Everyone was shocked (SHOCKED!) but maybe the polls weren't as wrong as the pundits like to claim.

Relevant links:
http://www.slate.com/articles/news_and_politics/moneybox/2016/07/why_political_betting_markets_are_failing.html
http://andrewgelman.com/2016/06/24/brexit-polling-what-went-wrong/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/r_1g8zDygNw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/shCsyJFYvFU/277286802-linear-digressions-how-polls-got-brexit-wrong.mp3" fileSize="21936620" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/how-polls-got-brexit-wrong</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/shCsyJFYvFU/277286802-linear-digressions-how-polls-got-brexit-wrong.mp3" length="21936620" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/277286802-linear-digressions-how-polls-got-brexit-wrong.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/276264888</guid>
      <title>Election Forecasting</title>
      <pubDate>Mon, 01 Aug 2016 02:40:35 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/QZ0BoLGYvYc/election-forecasting</link>
      <itunes:duration>00:28:59</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Not sure if you heard, but there's an election going on right now.  Polls, surveys, and projections about, as far as the eye can see.  How to make sense of it all?  How are the projections made?  Which are some good ones to follow?  We'll be your trusty guides through a crash course in election forecasting.

Relevant links:
http://www.wired.com/2016/06/civis-election-polling-clinton-sanders-trump/
http://election.princeton.edu/
http://projects.fivethirtyeight.com/2016-election-forecast/
http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html?rref=collection%2Fsectioncollection%2Fupshot&amp;action=click&amp;contentCollection=upshot&amp;region=rank&amp;module=package&amp;version=highlights&amp;contentPlacement=5&amp;pgtype=sectionfront</itunes:summary>
      <itunes:subtitle>Not sure if you heard, but there's an election go…</itunes:subtitle>
      <description>Not sure if you heard, but there's an election going on right now.  Polls, surveys, and projections about, as far as the eye can see.  How to make sense of it all?  How are the projections made?  Which are some good ones to follow?  We'll be your trusty guides through a crash course in election forecasting.

Relevant links:
http://www.wired.com/2016/06/civis-election-polling-clinton-sanders-trump/
http://election.princeton.edu/
http://projects.fivethirtyeight.com/2016-election-forecast/
http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html?rref=collection%2Fsectioncollection%2Fupshot&amp;action=click&amp;contentCollection=upshot&amp;region=rank&amp;module=package&amp;version=highlights&amp;contentPlacement=5&amp;pgtype=sectionfront&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/QZ0BoLGYvYc" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/7xhOckqQoP0/276264888-linear-digressions-election-forecasting.mp3" fileSize="41739735" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/election-forecasting</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/7xhOckqQoP0/276264888-linear-digressions-election-forecasting.mp3" length="41739735" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/276264888-linear-digressions-election-forecasting.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/275215540</guid>
      <title>Machine Learning for Genomics</title>
      <pubDate>Mon, 25 Jul 2016 02:14:47 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Y_gsIulOAo4/machine-learning-for-genomics</link>
      <itunes:duration>00:20:22</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Genomics data is some of the biggest #bigdata, and doing machine learning on it is unlocking new ways of thinking about evolution, genomic diseases like cancer, and what really makes each of us different for everyone else.  This episode touches on some of the things that make machine learning on genomics data so challenging, and the algorithms designed to do it anyway.</itunes:summary>
      <itunes:subtitle>Genomics data is some of the biggest #bigdata, an…</itunes:subtitle>
      <description>Genomics data is some of the biggest #bigdata, and doing machine learning on it is unlocking new ways of thinking about evolution, genomic diseases like cancer, and what really makes each of us different for everyone else.  This episode touches on some of the things that make machine learning on genomics data so challenging, and the algorithms designed to do it anyway.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Y_gsIulOAo4" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/yfdWNlXL9a4/275215540-linear-digressions-machine-learning-for-genomics.mp3" fileSize="29324467" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/machine-learning-for-genomics</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/yfdWNlXL9a4/275215540-linear-digressions-machine-learning-for-genomics.mp3" length="29324467" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/275215540-linear-digressions-machine-learning-for-genomics.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/274163734</guid>
      <title>Climate Modeling</title>
      <pubDate>Mon, 18 Jul 2016 02:26:02 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/plOq0jmoba8/climate-modeling</link>
      <itunes:duration>00:19:49</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Hot enough for you?  Climate models suggest that it's only going to get warmer in the coming years.  This episode unpacks those models, so you understand how they work.

A lot of the episodes we do are about fun studies we hear about, like "if you're interested, this is kinda cool"--this episode is much more important than that.  Understanding these models, and taking action on them where appropriate, will have huge implications in the years to come.

Relevant links:
https://climatesight.org/</itunes:summary>
      <itunes:subtitle>Hot enough for you?  Climate models suggest that …</itunes:subtitle>
      <description>Hot enough for you?  Climate models suggest that it's only going to get warmer in the coming years.  This episode unpacks those models, so you understand how they work.

A lot of the episodes we do are about fun studies we hear about, like "if you're interested, this is kinda cool"--this episode is much more important than that.  Understanding these models, and taking action on them where appropriate, will have huge implications in the years to come.

Relevant links:
https://climatesight.org/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/plOq0jmoba8" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/zr7MEbmxf_Y/274163734-linear-digressions-climate-modeling.mp3" fileSize="28551451" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/climate-modeling</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/zr7MEbmxf_Y/274163734-linear-digressions-climate-modeling.mp3" length="28551451" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/274163734-linear-digressions-climate-modeling.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/273094889</guid>
      <title>Reinforcement Learning Gone Wrong</title>
      <pubDate>Mon, 11 Jul 2016 02:42:49 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/1af7iIuc3YU/reinforcement-learning-gone-wrong</link>
      <itunes:duration>00:28:16</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Last week’s episode on artificial intelligence gets a huge payoff this week—we’ll explore a wonderful couple of papers about all the ways that artificial intelligence can go wrong.  Malevolent actors?  You bet.  Collateral damage?  Of course.  Reward hacking?  Naturally!  It’s fun to think about, and the discussion starting now will have reverberations for decades to come.

https://www.technologyreview.com/s/601519/how-to-create-a-malevolent-artificial-intelligence/
http://arxiv.org/abs/1605.02817
https://arxiv.org/abs/1606.06565</itunes:summary>
      <itunes:subtitle>Last week’s episode on artificial intelligence ge…</itunes:subtitle>
      <description>Last week’s episode on artificial intelligence gets a huge payoff this week—we’ll explore a wonderful couple of papers about all the ways that artificial intelligence can go wrong.  Malevolent actors?  You bet.  Collateral damage?  Of course.  Reward hacking?  Naturally!  It’s fun to think about, and the discussion starting now will have reverberations for decades to come.

https://www.technologyreview.com/s/601519/how-to-create-a-malevolent-artificial-intelligence/
http://arxiv.org/abs/1605.02817
https://arxiv.org/abs/1606.06565&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/1af7iIuc3YU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/lalyAIiBbaU/273094889-linear-digressions-reinforcement-learning-gone-wrong.mp3" fileSize="54272868" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/reinforcement-learning-gone-wrong</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/lalyAIiBbaU/273094889-linear-digressions-reinforcement-learning-gone-wrong.mp3" length="54272868" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/273094889-linear-digressions-reinforcement-learning-gone-wrong.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/272013221</guid>
      <title>Reinforcement Learning for Artificial Intelligence</title>
      <pubDate>Sun, 03 Jul 2016 18:28:57 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/nFwuGP58W6w/reinforcement-learning-for-artificial-intelligence</link>
      <itunes:duration>00:18:30</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>There’s a ton of excitement about reinforcement learning, a form of semi-supervised machine learning that underpins a lot of today’s cutting-edge artificial intelligence algorithms.  Here’s a crash course in the algorithmic machinery behind AlphaGo, and self-driving cars, and major logistical optimization projects—and the robots that, tomorrow, will clean our houses and (hopefully) not take over the world…</itunes:summary>
      <itunes:subtitle>There’s a ton of excitement about reinforcement l…</itunes:subtitle>
      <description>There’s a ton of excitement about reinforcement learning, a form of semi-supervised machine learning that underpins a lot of today’s cutting-edge artificial intelligence algorithms.  Here’s a crash course in the algorithmic machinery behind AlphaGo, and self-driving cars, and major logistical optimization projects—and the robots that, tomorrow, will clean our houses and (hopefully) not take over the world…&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/nFwuGP58W6w" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/c9KAxXdMYWo/272013221-linear-digressions-reinforcement-learning-for-artificial-intelligence.m4a" fileSize="35131406" type="audio/x-m4a" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/reinforcement-learning-for-artificial-intelligence</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/c9KAxXdMYWo/272013221-linear-digressions-reinforcement-learning-for-artificial-intelligence.m4a" length="35131406" type="audio/x-m4a" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/272013221-linear-digressions-reinforcement-learning-for-artificial-intelligence.m4a</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/271005019</guid>
      <title>Differential Privacy: how to study people without being weird and gross</title>
      <pubDate>Mon, 27 Jun 2016 01:53:13 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/j_PUIqdox5w/differential-privacy-how-to-study-people-without-being-weird-and-gross</link>
      <itunes:duration>00:18:17</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Apple wants to study iPhone users' activities and use it to improve performance.  Google collects data on what people are doing online to try to improve their Chrome browser.  Do you like the idea of this data being collected?  Maybe not, if it's being collected on you--but you probably also realize that there is some benefit to be had from the improved iPhones and web browsers.  Differential privacy is a set of policies that walks the line between individual privacy and better data, including even some old-school tricks that scientists use to get people to answer embarrassing questions honestly.

Relevant links: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf</itunes:summary>
      <itunes:subtitle>Apple wants to study iPhone users' activities and…</itunes:subtitle>
      <description>Apple wants to study iPhone users' activities and use it to improve performance.  Google collects data on what people are doing online to try to improve their Chrome browser.  Do you like the idea of this data being collected?  Maybe not, if it's being collected on you--but you probably also realize that there is some benefit to be had from the improved iPhones and web browsers.  Differential privacy is a set of policies that walks the line between individual privacy and better data, including even some old-school tricks that scientists use to get people to answer embarrassing questions honestly.

Relevant links: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/j_PUIqdox5w" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TJQxzOfhYTc/271005019-linear-digressions-differential-privacy-how-to-study-people-without-being-weird-and-gross.mp3" fileSize="26344627" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/differential-privacy-how-to-study-people-without-being-weird-and-gross</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TJQxzOfhYTc/271005019-linear-digressions-differential-privacy-how-to-study-people-without-being-weird-and-gross.mp3" length="26344627" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/271005019-linear-digressions-differential-privacy-how-to-study-people-without-being-weird-and-gross.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/269958489</guid>
      <title>How the sausage gets made</title>
      <pubDate>Mon, 20 Jun 2016 02:25:23 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/HA0IP62Akfw/how-the-sausage-gets-made</link>
      <itunes:duration>00:29:13</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Something a little different in this episode--we'll be talking about the technical plumbing that gets our podcast from our brains to your ears.  As it turns out, it's a multi-step bucket brigade process of RSS feeds, links to downloads, and lots of hand-waving when it comes to trying to figure out how many of you (listeners) are out there.  </itunes:summary>
      <itunes:subtitle>Something a little different in this episode--we'…</itunes:subtitle>
      <description>Something a little different in this episode--we'll be talking about the technical plumbing that gets our podcast from our brains to your ears.  As it turns out, it's a multi-step bucket brigade process of RSS feeds, links to downloads, and lots of hand-waving when it comes to trying to figure out how many of you (listeners) are out there.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/HA0IP62Akfw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/buafJEcFZIk/269958489-linear-digressions-how-the-sausage-gets-made.mp3" fileSize="42087686" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/how-the-sausage-gets-made</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/buafJEcFZIk/269958489-linear-digressions-how-the-sausage-gets-made.mp3" length="42087686" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/269958489-linear-digressions-how-the-sausage-gets-made.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/268819785</guid>
      <title>SMOTE: makin' yourself some fake minority data</title>
      <pubDate>Mon, 13 Jun 2016 03:06:33 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/xUg5z_G76ds/smote-makin-yourself-some-fake-minority-data</link>
      <itunes:duration>00:14:37</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Machine learning on imbalanced classes: surprisingly tricky.  Many (most?) algorithms tend to just assign the majority class label to all the data and call it a day.  SMOTE is an algorithm for manufacturing new minority class examples for yourself, to help your algorithm better identify them in the wild.

Relevant links:
https://www.jair.org/media/953/live-953-2037-jair.pdf</itunes:summary>
      <itunes:subtitle>Machine learning on imbalanced classes: surprisin…</itunes:subtitle>
      <description>Machine learning on imbalanced classes: surprisingly tricky.  Many (most?) algorithms tend to just assign the majority class label to all the data and call it a day.  SMOTE is an algorithm for manufacturing new minority class examples for yourself, to help your algorithm better identify them in the wild.

Relevant links:
https://www.jair.org/media/953/live-953-2037-jair.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/xUg5z_G76ds" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/LDX8mHfMuIc/268819785-linear-digressions-smote-makin-yourself-some-fake-minority-data.mp3" fileSize="21058906" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/smote-makin-yourself-some-fake-minority-data</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/LDX8mHfMuIc/268819785-linear-digressions-smote-makin-yourself-some-fake-minority-data.mp3" length="21058906" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/268819785-linear-digressions-smote-makin-yourself-some-fake-minority-data.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/267742358</guid>
      <title>Conjoint Analysis: like AB testing, but on steroids</title>
      <pubDate>Mon, 06 Jun 2016 02:13:37 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/sH54C5d6jBM/conjoint-analysis-like-ab-testing-but-on-steroids</link>
      <itunes:duration>00:18:27</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Conjoint analysis is like AB tester, but more bigger more better: instead of testing one or two things, you can test potentially dozens of options.  Where might you use something like this?  Well, if you wanted to design an entire hotel chain completely from scratch, and to do it in a data-driven way.  You'll never look at Courtyard by Marriott the same way again.

Relevant link: https://marketing.wharton.upenn.edu/files/?whdmsaction=public:main.file&amp;fileID=466 </itunes:summary>
      <itunes:subtitle>Conjoint analysis is like AB tester, but more big…</itunes:subtitle>
      <description>Conjoint analysis is like AB tester, but more bigger more better: instead of testing one or two things, you can test potentially dozens of options.  Where might you use something like this?  Well, if you wanted to design an entire hotel chain completely from scratch, and to do it in a data-driven way.  You'll never look at Courtyard by Marriott the same way again.

Relevant link: https://marketing.wharton.upenn.edu/files/?whdmsaction=public:main.file&amp;fileID=466&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/sH54C5d6jBM" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/_9J49CIkMHI/267742358-linear-digressions-conjoint-analysis-like-ab-testing-but-on-steroids.mp3" fileSize="26567817" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/conjoint-analysis-like-ab-testing-but-on-steroids</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/_9J49CIkMHI/267742358-linear-digressions-conjoint-analysis-like-ab-testing-but-on-steroids.mp3" length="26567817" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/267742358-linear-digressions-conjoint-analysis-like-ab-testing-but-on-steroids.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/266545981</guid>
      <title>Traffic Metering Algorithms</title>
      <pubDate>Mon, 30 May 2016 01:57:10 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/E7w57VJkCws/traffic-metering-algorithms</link>
      <itunes:duration>00:17:30</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This episode is for all you (us) traffic nerds--we're talking about the hidden structure underlying traffic on-ramp metering systems.  These systems slow down the flow of traffic onto highways so that the highways don't get overloaded with cars and clog up.  If you're someone who listens to podcasts while commuting, and especially if your area has on-ramp metering, you'll never look at highway access control the same way again (yeah, we know this is super nerdy; it's also super awesome).

Relevant links:
http://its.berkeley.edu/sites/default/files/publications/UCB/99/PWP/UCB-ITS-PWP-99-19.pdf
http://www.its.uci.edu/~lchu/ramp/Final_report_mou3013.pdf</itunes:summary>
      <itunes:subtitle>This episode is for all you (us) traffic nerds--w…</itunes:subtitle>
      <description>This episode is for all you (us) traffic nerds--we're talking about the hidden structure underlying traffic on-ramp metering systems.  These systems slow down the flow of traffic onto highways so that the highways don't get overloaded with cars and clog up.  If you're someone who listens to podcasts while commuting, and especially if your area has on-ramp metering, you'll never look at highway access control the same way again (yeah, we know this is super nerdy; it's also super awesome).

Relevant links:
http://its.berkeley.edu/sites/default/files/publications/UCB/99/PWP/UCB-ITS-PWP-99-19.pdf
http://www.its.uci.edu/~lchu/ramp/Final_report_mou3013.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/E7w57VJkCws" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/cfjdNQpdgtk/266545981-linear-digressions-traffic-metering-algorithms.mp3" fileSize="25214256" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/traffic-metering-algorithms</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/cfjdNQpdgtk/266545981-linear-digressions-traffic-metering-algorithms.mp3" length="25214256" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/266545981-linear-digressions-traffic-metering-algorithms.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/265435083</guid>
      <title>Um Detector 2: The Dynamic Time Warp</title>
      <pubDate>Mon, 23 May 2016 02:05:01 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/TlY-iqXP8yc/um-detector-2-the-dynamic-time-warp</link>
      <itunes:duration>00:14:00</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>One tricky thing about working with time series data, like the audio data in our "um" detector (remember that?  because we barely do...), is that sometimes events look really similar but one is a little bit stretched and squeezed relative to the other.  Besides having an amazing name, the dynamic time warp is a handy algorithm for aligning two time series sequences that are close in shape, but don't quite line up out of the box.

Relevant link:
http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf</itunes:summary>
      <itunes:subtitle>One tricky thing about working with time series d…</itunes:subtitle>
      <description>One tricky thing about working with time series data, like the audio data in our "um" detector (remember that?  because we barely do...), is that sometimes events look really similar but one is a little bit stretched and squeezed relative to the other.  Besides having an amazing name, the dynamic time warp is a handy algorithm for aligning two time series sequences that are close in shape, but don't quite line up out of the box.

Relevant link:
http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/TlY-iqXP8yc" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/V50tWt5ckY4/265435083-linear-digressions-um-detector-2-the-dynamic-time-warp.mp3" fileSize="20161757" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/um-detector-2-the-dynamic-time-warp</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/V50tWt5ckY4/265435083-linear-digressions-um-detector-2-the-dynamic-time-warp.mp3" length="20161757" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/265435083-linear-digressions-um-detector-2-the-dynamic-time-warp.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/264324749</guid>
      <title>Inside a Data Analysis: Fraud Hunting at Enron</title>
      <pubDate>Mon, 16 May 2016 02:36:10 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/WUPr3uS3uvs/inside-a-data-analysis-fraud-hunting-at-enron</link>
      <itunes:duration>00:30:28</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>It's storytime this week--the story, from beginning to end, of how Katie designed and built the main project for Udacity's Intro to Machine Learning class, when she was developing the course.  The project was to use email and financial data to hunt for signatures of fraud at Enron, one of the biggest cases of corporate fraud in history; that description makes the project sound pretty clean but getting the data into the right shape, and even doing some dataset merging (that hadn't ever been done before), made this project much more interesting to design than it might appear.  Here's the story of what a data analysis like this looks like...from the inside.</itunes:summary>
      <itunes:subtitle>It's storytime this week--the story, from beginni…</itunes:subtitle>
      <description>It's storytime this week--the story, from beginning to end, of how Katie designed and built the main project for Udacity's Intro to Machine Learning class, when she was developing the course.  The project was to use email and financial data to hunt for signatures of fraud at Enron, one of the biggest cases of corporate fraud in history; that description makes the project sound pretty clean but getting the data into the right shape, and even doing some dataset merging (that hadn't ever been done before), made this project much more interesting to design than it might appear.  Here's the story of what a data analysis like this looks like...from the inside.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/WUPr3uS3uvs" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/f4HyqLHJ17s/264324749-linear-digressions-inside-a-data-analysis-fraud-hunting-at-enron.mp3" fileSize="43884493" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/inside-a-data-analysis-fraud-hunting-at-enron</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/f4HyqLHJ17s/264324749-linear-digressions-inside-a-data-analysis-fraud-hunting-at-enron.mp3" length="43884493" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/264324749-linear-digressions-inside-a-data-analysis-fraud-hunting-at-enron.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/263148994</guid>
      <title>What's the biggest #bigdata?</title>
      <pubDate>Mon, 09 May 2016 01:28:21 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/D_EpY8BksbQ/whats-the-biggest-bigdata</link>
      <itunes:duration>00:25:31</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Data science and is often mentioned in the same breath as big data.  But how big is big data?  And who has the biggest big data?  CERN?  Youtube?

... Something (or someone) else?

Relevant link: http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002195

</itunes:summary>
      <itunes:subtitle>Data science and is often mentioned in the same b…</itunes:subtitle>
      <description>Data science and is often mentioned in the same breath as big data.  But how big is big data?  And who has the biggest big data?  CERN?  Youtube?

... Something (or someone) else?

Relevant link: http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002195&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/D_EpY8BksbQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/aPO8BwJkU0M/263148994-linear-digressions-whats-the-biggest-bigdata.mp3" fileSize="36751811" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/whats-the-biggest-bigdata</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/aPO8BwJkU0M/263148994-linear-digressions-whats-the-biggest-bigdata.mp3" length="36751811" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/263148994-linear-digressions-whats-the-biggest-bigdata.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/261895067</guid>
      <title>Data Contamination</title>
      <pubDate>Mon, 02 May 2016 02:24:06 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/cOFCXWQlukw/data-contamination</link>
      <itunes:duration>00:20:58</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Supervised machine learning assumes that the features and labels used for building a classifier are isolated from each other--basically, that you can't cheat by peeking.  Turns out this can be easier said than done.  In this episode, we'll talk about the many (and diverse!) cases where label information contaminates features, ruining data science competitions along the way.

Relevant links:
https://www.researchgate.net/profile/Claudia_Perlich/publication/221653692_Leakage_in_data_mining_Formulation_detection_and_avoidance/links/54418bb80cf2a6a049a5a0ca.pdf</itunes:summary>
      <itunes:subtitle>Supervised machine learning assumes that the feat…</itunes:subtitle>
      <description>Supervised machine learning assumes that the features and labels used for building a classifier are isolated from each other--basically, that you can't cheat by peeking.  Turns out this can be easier said than done.  In this episode, we'll talk about the many (and diverse!) cases where label information contaminates features, ruining data science competitions along the way.

Relevant links:
https://www.researchgate.net/profile/Claudia_Perlich/publication/221653692_Leakage_in_data_mining_Formulation_detection_and_avoidance/links/54418bb80cf2a6a049a5a0ca.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/cOFCXWQlukw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/dMbUNkl7Sts/261895067-linear-digressions-data-contamination.mp3" fileSize="30191523" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/data-contamination</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/dMbUNkl7Sts/261895067-linear-digressions-data-contamination.mp3" length="30191523" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/261895067-linear-digressions-data-contamination.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/260723988</guid>
      <title>Model Interpretation (and Trust Issues)</title>
      <pubDate>Mon, 25 Apr 2016 00:45:04 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/mX1f27CNltY/model-interpretation-and-trust-issues</link>
      <itunes:duration>00:16:57</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Machine learning algorithms can be black boxes--inputs go in, outputs come out, and what happens in the middle is anybody's guess.  But understanding how a model arrives at an answer is critical for interpreting the model, and for knowing if it's doing something reasonable (one could even say... trustworthy).  We'll talk about a new algorithm called LIME that seeks to make any model more understandable and interpretable.

Relevant Links:
http://arxiv.org/abs/1602.04938
https://github.com/marcotcr/lime/tree/master/lime</itunes:summary>
      <itunes:subtitle>Machine learning algorithms can be black boxes--i…</itunes:subtitle>
      <description>Machine learning algorithms can be black boxes--inputs go in, outputs come out, and what happens in the middle is anybody's guess.  But understanding how a model arrives at an answer is critical for interpreting the model, and for knowing if it's doing something reasonable (one could even say... trustworthy).  We'll talk about a new algorithm called LIME that seeks to make any model more understandable and interpretable.

Relevant Links:
http://arxiv.org/abs/1602.04938
https://github.com/marcotcr/lime/tree/master/lime&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/mX1f27CNltY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/HY5iyyINBH4/260723988-linear-digressions-model-interpretation-and-trust-issues.mp3" fileSize="24420552" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/model-interpretation-and-trust-issues</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/HY5iyyINBH4/260723988-linear-digressions-model-interpretation-and-trust-issues.mp3" length="24420552" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/260723988-linear-digressions-model-interpretation-and-trust-issues.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/259611617</guid>
      <title>Updates! Political Science Fraud and AlphaGo</title>
      <pubDate>Mon, 18 Apr 2016 02:48:04 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/gq08hw7cmQE/updates-political-science-fraud-and-alphago</link>
      <itunes:duration>00:31:43</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>We've got updates for you about topics from past shows!  First, the political science scandal of the year 2015 has a new chapter, we'll remind you about the original story and then dive into what has happened since.  Then, we've got an update on AlphaGo, and his/her/its much-anticipated match against the human champion of the game Go.

Relevant Links:
https://soundcloud.com/linear-digressions/electoral-insights-part-2
https://soundcloud.com/linear-digressions/go-1

http://www.sciencemag.org/news/2016/04/talking-people-about-gay-and-transgender-issues-can-change-their-prejudices
http://science.sciencemag.org/content/sci/352/6282/220.full.pdf

http://qz.com/639952/googles-ai-won-the-game-go-by-defying-millennia-of-basic-human-instinct/
http://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/
http://www.wired.com/2016/03/sadness-beauty-watching-googles-ai-play-go/</itunes:summary>
      <itunes:subtitle>We've got updates for you about topics from past …</itunes:subtitle>
      <description>We've got updates for you about topics from past shows!  First, the political science scandal of the year 2015 has a new chapter, we'll remind you about the original story and then dive into what has happened since.  Then, we've got an update on AlphaGo, and his/her/its much-anticipated match against the human champion of the game Go.

Relevant Links:
https://soundcloud.com/linear-digressions/electoral-insights-part-2
https://soundcloud.com/linear-digressions/go-1

http://www.sciencemag.org/news/2016/04/talking-people-about-gay-and-transgender-issues-can-change-their-prejudices
http://science.sciencemag.org/content/sci/352/6282/220.full.pdf

http://qz.com/639952/googles-ai-won-the-game-go-by-defying-millennia-of-basic-human-instinct/
http://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/
http://www.wired.com/2016/03/sadness-beauty-watching-googles-ai-play-go/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/gq08hw7cmQE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-ZrWrOvCVyY/259611617-linear-digressions-updates-political-science-fraud-and-alphago.mp3" fileSize="45689450" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/updates-political-science-fraud-and-alphago</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-ZrWrOvCVyY/259611617-linear-digressions-updates-political-science-fraud-and-alphago.mp3" length="45689450" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/259611617-linear-digressions-updates-political-science-fraud-and-alphago.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/258388115</guid>
      <title>Ecological Inference and Simpson's Paradox</title>
      <pubDate>Mon, 11 Apr 2016 02:43:12 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/gwVKgRM4SsQ/ecological-inference-and-simpsons-paradox</link>
      <itunes:duration>00:18:32</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Simpson's paradox is the data science equivalent of looking through one eye and seeing a very clear trend, and then looking through the other eye and seeing the very clear opposite trend.  In one case, you see a trend one way in a group, but then breaking the group into subgroups gives the exact opposite trend.  Confused?  Scratching your head?  Welcome to the tricky world of ecological inference.

Relevant links:
https://gking.harvard.edu/files/gking/files/part1.pdf
http://blog.revolutionanalytics.com/2013/07/a-great-example-of-simpsons-paradox.html</itunes:summary>
      <itunes:subtitle>Simpson's paradox is the data science equivalent …</itunes:subtitle>
      <description>Simpson's paradox is the data science equivalent of looking through one eye and seeing a very clear trend, and then looking through the other eye and seeing the very clear opposite trend.  In one case, you see a trend one way in a group, but then breaking the group into subgroups gives the exact opposite trend.  Confused?  Scratching your head?  Welcome to the tricky world of ecological inference.

Relevant links:
https://gking.harvard.edu/files/gking/files/part1.pdf
http://blog.revolutionanalytics.com/2013/07/a-great-example-of-simpsons-paradox.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/gwVKgRM4SsQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/JJhXaClychA/258388115-linear-digressions-ecological-inference-and-simpsons-paradox.mp3" fileSize="26694459" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/ecological-inference-and-simpsons-paradox</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/JJhXaClychA/258388115-linear-digressions-ecological-inference-and-simpsons-paradox.mp3" length="26694459" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/258388115-linear-digressions-ecological-inference-and-simpsons-paradox.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/257188961</guid>
      <title>Discriminatory Algorithms</title>
      <pubDate>Mon, 04 Apr 2016 02:30:09 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/bEbEJXLqfwM/discriminatory-algorithms</link>
      <itunes:duration>00:15:21</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Sometimes when we say an algorithm discriminates, we mean it can tell the difference between two types of items.  But in this episode, we'll talk about another, more troublesome side to discrimination: algorithms can be... racist?  Sexist?  Ageist?  Yes to all of the above.  It's an important thing to be aware of, especially when doing people-centered data science.  We'll discuss how and why this happens, and what solutions are out there (or not).

Relevant Links:
http://www.nytimes.com/2015/07/10/upshot/when-algorithms-discriminate.html
http://techcrunch.com/2015/08/02/machine-learning-and-human-bias-an-uneasy-pair/
http://www.sciencefriday.com/segments/why-machines-discriminate-and-how-to-fix-them/
https://medium.com/@geomblog/when-an-algorithm-isn-t-2b9fe01b9bb5#.auxqi5srz


</itunes:summary>
      <itunes:subtitle>Sometimes when we say an algorithm discriminates,…</itunes:subtitle>
      <description>Sometimes when we say an algorithm discriminates, we mean it can tell the difference between two types of items.  But in this episode, we'll talk about another, more troublesome side to discrimination: algorithms can be... racist?  Sexist?  Ageist?  Yes to all of the above.  It's an important thing to be aware of, especially when doing people-centered data science.  We'll discuss how and why this happens, and what solutions are out there (or not).

Relevant Links:
http://www.nytimes.com/2015/07/10/upshot/when-algorithms-discriminate.html
http://techcrunch.com/2015/08/02/machine-learning-and-human-bias-an-uneasy-pair/
http://www.sciencefriday.com/segments/why-machines-discriminate-and-how-to-fix-them/
https://medium.com/@geomblog/when-an-algorithm-isn-t-2b9fe01b9bb5#.auxqi5srz&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/bEbEJXLqfwM" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/F8FQeHmAWIQ/257188961-linear-digressions-discriminatory-algorithms.mp3" fileSize="22122194" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/discriminatory-algorithms</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/F8FQeHmAWIQ/257188961-linear-digressions-discriminatory-algorithms.mp3" length="22122194" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/257188961-linear-digressions-discriminatory-algorithms.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/255507844</guid>
      <title>Recommendation Engines and Privacy</title>
      <pubDate>Mon, 28 Mar 2016 02:46:45 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/WKwDSL_0YTA/recommendation-engines-and-privacy</link>
      <itunes:duration>00:31:33</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This episode started out as a discussion of recommendation engines, like Netflix uses to suggest movies.  There's still a lot of that in here.  But a related topic, which is both interesting and important, is how to keep data private in the era of large-scale recommendation engines--what mistakes have been made surrounding supposedly anonymized data, how data ends up de-anonymized, and why it matters for you.

Relevant links:
http://www.netflixprize.com/
http://bits.blogs.nytimes.com/2010/03/12/netflix-cancels-contest-plans-and-settles-suit/?_r=0
http://arxiv.org/PS_cache/cs/pdf/0610/0610105v2.pdf</itunes:summary>
      <itunes:subtitle>This episode started out as a discussion of recom…</itunes:subtitle>
      <description>This episode started out as a discussion of recommendation engines, like Netflix uses to suggest movies.  There's still a lot of that in here.  But a related topic, which is both interesting and important, is how to keep data private in the era of large-scale recommendation engines--what mistakes have been made surrounding supposedly anonymized data, how data ends up de-anonymized, and why it matters for you.

Relevant links:
http://www.netflixprize.com/
http://bits.blogs.nytimes.com/2010/03/12/netflix-cancels-contest-plans-and-settles-suit/?_r=0
http://arxiv.org/PS_cache/cs/pdf/0610/0610105v2.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/WKwDSL_0YTA" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/OesaLBCYxrA/255507844-linear-digressions-recommendation-engines-and-privacy.mp3" fileSize="45429270" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/recommendation-engines-and-privacy</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/OesaLBCYxrA/255507844-linear-digressions-recommendation-engines-and-privacy.mp3" length="45429270" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/255507844-linear-digressions-recommendation-engines-and-privacy.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/253723817</guid>
      <title>Neural nets play cops and robbers (AKA generative adverserial networks)</title>
      <pubDate>Mon, 21 Mar 2016 02:58:49 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Nkzh-znp8ZI/neural-nets-play-cops-and-robbers-aka-generative-adverserial-networks</link>
      <itunes:duration>00:18:56</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>One neural net is creating counterfeit bills and passing them off to a second neural net, which is trying to distinguish the real money from the fakes.  Result: two neural nets that are better than either one would have been without the competition.

Relevant links:
http://arxiv.org/pdf/1406.2661v1.pdf
http://arxiv.org/pdf/1412.6572v3.pdf
http://soumith.ch/eyescream/</itunes:summary>
      <itunes:subtitle>One neural net is creating counterfeit bills and …</itunes:subtitle>
      <description>One neural net is creating counterfeit bills and passing them off to a second neural net, which is trying to distinguish the real money from the fakes.  Result: two neural nets that are better than either one would have been without the competition.

Relevant links:
http://arxiv.org/pdf/1406.2661v1.pdf
http://arxiv.org/pdf/1412.6572v3.pdf
http://soumith.ch/eyescream/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Nkzh-znp8ZI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/KgceoBeOgCI/253723817-linear-digressions-neural-nets-play-cops-and-robbers-aka-generative-adverserial-networks.mp3" fileSize="27263719" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/neural-nets-play-cops-and-robbers-aka-generative-adverserial-networks</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/KgceoBeOgCI/253723817-linear-digressions-neural-nets-play-cops-and-robbers-aka-generative-adverserial-networks.mp3" length="27263719" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/253723817-linear-digressions-neural-nets-play-cops-and-robbers-aka-generative-adverserial-networks.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/251757035</guid>
      <title>A Data Scientist's View of the Fight against Cancer</title>
      <pubDate>Mon, 14 Mar 2016 03:26:27 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/zUocbmzlFiA/a-data-scientists-view-of-the-fight-against-cancer</link>
      <itunes:duration>00:19:08</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In this episode, we're taking many episodes' worth of insights and unpacking an extremely complex and important question--in what ways are we winning the fight against cancer, where might that fight go in the coming decade, and how do we know when we're making progress?  No matter how tricky you might think this problem is to solve, the fact is, once you get in there trying to solve it, it's even trickier than you thought.</itunes:summary>
      <itunes:subtitle>In this episode, we're taking many episodes' wort…</itunes:subtitle>
      <description>In this episode, we're taking many episodes' worth of insights and unpacking an extremely complex and important question--in what ways are we winning the fight against cancer, where might that fight go in the coming decade, and how do we know when we're making progress?  No matter how tricky you might think this problem is to solve, the fact is, once you get in there trying to solve it, it's even trickier than you thought.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/zUocbmzlFiA" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/AlnxQMYDjSQ/251757035-linear-digressions-a-data-scientists-view-of-the-fight-against-cancer.mp3" fileSize="27561515" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/a-data-scientists-view-of-the-fight-against-cancer</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/AlnxQMYDjSQ/251757035-linear-digressions-a-data-scientists-view-of-the-fight-against-cancer.mp3" length="27561515" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/251757035-linear-digressions-a-data-scientists-view-of-the-fight-against-cancer.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/251292556</guid>
      <title>Congress Bots and DeepDrumpf</title>
      <pubDate>Fri, 11 Mar 2016 04:17:29 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/-FDpSSbqscY/congress-bots-and-deepdrumpf</link>
      <itunes:duration>00:20:47</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Hey, sick of the election yet?  Fear not, there are algorithms that can automagically generate political-ish speech so that we never need to be without an endless supply of Congressional speeches and Donald Trump twitticisms!

Relevant links:
http://arxiv.org/pdf/1601.03313v2.pdf
http://qz.com/631497/mit-built-a-donald-trump-ai-twitter-bot-that-sounds-scarily-like-him/
https://twitter.com/deepdrumpf</itunes:summary>
      <itunes:subtitle>Hey, sick of the election yet?  Fear not, there a…</itunes:subtitle>
      <description>Hey, sick of the election yet?  Fear not, there are algorithms that can automagically generate political-ish speech so that we never need to be without an endless supply of Congressional speeches and Donald Trump twitticisms!

Relevant links:
http://arxiv.org/pdf/1601.03313v2.pdf
http://qz.com/631497/mit-built-a-donald-trump-ai-twitter-bot-that-sounds-scarily-like-him/
https://twitter.com/deepdrumpf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/-FDpSSbqscY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/WHugtKO2rws/251292556-linear-digressions-congress-bots-and-deepdrumpf.mp3" fileSize="29945763" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/congress-bots-and-deepdrumpf</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/WHugtKO2rws/251292556-linear-digressions-congress-bots-and-deepdrumpf.mp3" length="29945763" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/251292556-linear-digressions-congress-bots-and-deepdrumpf.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/250586847</guid>
      <title>Multi - Armed Bandits</title>
      <pubDate>Mon, 07 Mar 2016 02:44:17 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/P9lXV1bXSTs/multi-armed-bandits</link>
      <itunes:duration>00:11:29</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Multi-armed bandits: how to take your randomized experiment and make it harder better faster stronger.  Basically, a multi-armed bandit experiment allows you to optimize for both learning and making use of your knowledge at the same time.  It's what the pros (like Google Analytics) use, and it's got a great name, so... winner!

Relevant link: https://support.google.com/analytics/answer/2844870?hl=en</itunes:summary>
      <itunes:subtitle>Multi-armed bandits: how to take your randomized …</itunes:subtitle>
      <description>Multi-armed bandits: how to take your randomized experiment and make it harder better faster stronger.  Basically, a multi-armed bandit experiment allows you to optimize for both learning and making use of your knowledge at the same time.  It's what the pros (like Google Analytics) use, and it's got a great name, so... winner!

Relevant link: https://support.google.com/analytics/answer/2844870?hl=en&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/P9lXV1bXSTs" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-m5L1ngVVLc/250586847-linear-digressions-multi-armed-bandits.mp3" fileSize="16549962" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/multi-armed-bandits</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-m5L1ngVVLc/250586847-linear-digressions-multi-armed-bandits.mp3" length="16549962" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/250586847-linear-digressions-multi-armed-bandits.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/250113250</guid>
      <title>Experiments and Messy, Tricky Causality</title>
      <pubDate>Fri, 04 Mar 2016 03:54:04 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ZLn103hq6UQ/experiments-and-messy-tricky-causality</link>
      <itunes:duration>00:16:59</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>"People with a family history of heart disease are more likely to eat healthy foods, and have a high incidence of heart attacks."  Did the healthy food cause the heart attacks?  Probably not.  But establishing causal links is extremely tricky, and extremely important to get right if you're trying to help students, test new medicines, or just optimize a website.  In this episode, we'll unpack randomized experiments, like AB tests, and maybe you'll be smarter as a result.  Will you be smarter BECAUSE of this episode?  Well, tough to say for sure...

Relevant link:
http://tylervigen.com/spurious-correlations</itunes:summary>
      <itunes:subtitle>"People with a family history of heart disease ar…</itunes:subtitle>
      <description>"People with a family history of heart disease are more likely to eat healthy foods, and have a high incidence of heart attacks."  Did the healthy food cause the heart attacks?  Probably not.  But establishing causal links is extremely tricky, and extremely important to get right if you're trying to help students, test new medicines, or just optimize a website.  In this episode, we'll unpack randomized experiments, like AB tests, and maybe you'll be smarter as a result.  Will you be smarter BECAUSE of this episode?  Well, tough to say for sure...

Relevant link:
http://tylervigen.com/spurious-correlations&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ZLn103hq6UQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/F267MsBRtPI/250113250-linear-digressions-experiments-and-messy-tricky-causality.mp3" fileSize="24459422" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/experiments-and-messy-tricky-causality</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/F267MsBRtPI/250113250-linear-digressions-experiments-and-messy-tricky-causality.mp3" length="24459422" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/250113250-linear-digressions-experiments-and-messy-tricky-causality.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/249418845</guid>
      <title>Backpropagation</title>
      <pubDate>Mon, 29 Feb 2016 03:58:10 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/uosae1Mnn1s/backpropagation</link>
      <itunes:duration>00:12:21</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The reason that neural nets are taking over the world right now is because they can be efficiently trained with the backpropagation algorithm.  In short, backprop allows you to adjust the weights of the neural net based on how good of a job the neural net is doing at classifying training examples, thereby getting better and better at making predictions.  In this episode: we talk backpropagation, and how it makes it possible to train the neural nets we know and love.</itunes:summary>
      <itunes:subtitle>The reason that neural nets are taking over the w…</itunes:subtitle>
      <description>The reason that neural nets are taking over the world right now is because they can be efficiently trained with the backpropagation algorithm.  In short, backprop allows you to adjust the weights of the neural net based on how good of a job the neural net is doing at classifying training examples, thereby getting better and better at making predictions.  In this episode: we talk backpropagation, and how it makes it possible to train the neural nets we know and love.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/uosae1Mnn1s" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/gYW1JUdYFTw/249418845-linear-digressions-backpropagation.mp3" fileSize="17797571" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/backpropagation</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/gYW1JUdYFTw/249418845-linear-digressions-backpropagation.mp3" length="17797571" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/249418845-linear-digressions-backpropagation.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/248956652</guid>
      <title>Text Analysis on the State Of The Union</title>
      <pubDate>Fri, 26 Feb 2016 03:51:42 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/oneFUw9PYVs/text-analysis-on-the-state-of-the-union</link>
      <itunes:duration>00:22:22</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>First up in this episode: a crash course in natural language processing, and important steps if you want to use machine learning techniques on text data.  Then we'll take that NLP know-how and talk about a really cool analysis of State of the Union text, which analyzes the topics and word choices of every President from Washington to Obama.

Relevant link:
https://civisanalytics.com/blog/data-science/2016/01/15/data-science-on-state-of-the-union-addresses/</itunes:summary>
      <itunes:subtitle>First up in this episode: a crash course in natur…</itunes:subtitle>
      <description>First up in this episode: a crash course in natural language processing, and important steps if you want to use machine learning techniques on text data.  Then we'll take that NLP know-how and talk about a really cool analysis of State of the Union text, which analyzes the topics and word choices of every President from Washington to Obama.

Relevant link:
https://civisanalytics.com/blog/data-science/2016/01/15/data-science-on-state-of-the-union-addresses/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/oneFUw9PYVs" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/SG8ei1Ny2Kk/248956652-linear-digressions-text-analysis-on-the-state-of-the-union.mp3" fileSize="32215908" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/text-analysis-on-the-state-of-the-union</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/SG8ei1Ny2Kk/248956652-linear-digressions-text-analysis-on-the-state-of-the-union.mp3" length="32215908" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/248956652-linear-digressions-text-analysis-on-the-state-of-the-union.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/248275653</guid>
      <title>Paradigms in Artificial Intelligence</title>
      <pubDate>Mon, 22 Feb 2016 04:32:25 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/x1xHZAgnjt8/paradigms-in-artificial-intelligence</link>
      <itunes:duration>00:17:20</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Artificial intelligence includes a number of different strategies for how to make machines more intelligent, and often more human-like, in their ability to learn and solve problems.  An ambitious group of researchers is working right now to classify all the approaches to AI, perhaps as a first step toward unifying these approaches and move closer to strong AI.  In this episode, we'll touch on some of the most provocative work in many different subfields of artificial intelligence, and their strengths and weaknesses.

Relevant links:
https://www.technologyreview.com/s/544606/can-this-man-make-aimore-human/
https://www.youtube.com/watch?v=B8J4uefCQMc
http://venturebeat.com/2013/11/29/sentient-code-an-inside-look-at-stephen-wolframs-utterly-new-insanely-ambitious-computational-paradigm/
http://www.slate.com/articles/technology/bitwise/2014/03/stephen_wolfram_s_new_programming_language_can_he_make_the_world_computable.html</itunes:summary>
      <itunes:subtitle>Artificial intelligence includes a number of diff…</itunes:subtitle>
      <description>Artificial intelligence includes a number of different strategies for how to make machines more intelligent, and often more human-like, in their ability to learn and solve problems.  An ambitious group of researchers is working right now to classify all the approaches to AI, perhaps as a first step toward unifying these approaches and move closer to strong AI.  In this episode, we'll touch on some of the most provocative work in many different subfields of artificial intelligence, and their strengths and weaknesses.

Relevant links:
https://www.technologyreview.com/s/544606/can-this-man-make-aimore-human/
https://www.youtube.com/watch?v=B8J4uefCQMc
http://venturebeat.com/2013/11/29/sentient-code-an-inside-look-at-stephen-wolframs-utterly-new-insanely-ambitious-computational-paradigm/
http://www.slate.com/articles/technology/bitwise/2014/03/stephen_wolfram_s_new_programming_language_can_he_make_the_world_computable.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/x1xHZAgnjt8" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/J_48A4W3zPs/248275653-linear-digressions-paradigms-in-artificial-intelligence.mp3" fileSize="24963481" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/paradigms-in-artificial-intelligence</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/J_48A4W3zPs/248275653-linear-digressions-paradigms-in-artificial-intelligence.mp3" length="24963481" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/248275653-linear-digressions-paradigms-in-artificial-intelligence.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/247799926</guid>
      <title>Survival Analysis</title>
      <pubDate>Fri, 19 Feb 2016 03:44:06 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/I9l2MQ6qgds/survival-analysis</link>
      <itunes:duration>00:15:21</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Survival analysis is all about studying how long until an event occurs--it's used in marketing to study how long a customer stays with a service, in epidemiology to estimate the duration of survival of a patient with some illness, and in social science to understand how the characteristics of a war inform how long the war goes on.  This episode talks about the special challenges associated with survival analysis, and the tools that (data) scientists use to answer all kinds of duration-related questions.</itunes:summary>
      <itunes:subtitle>Survival analysis is all about studying how long …</itunes:subtitle>
      <description>Survival analysis is all about studying how long until an event occurs--it's used in marketing to study how long a customer stays with a service, in epidemiology to estimate the duration of survival of a patient with some illness, and in social science to understand how the characteristics of a war inform how long the war goes on.  This episode talks about the special challenges associated with survival analysis, and the tools that (data) scientists use to answer all kinds of duration-related questions.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/I9l2MQ6qgds" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/N0UYUwCMyWA/247799926-linear-digressions-survival-analysis.mp3" fileSize="22109028" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/survival-analysis</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/N0UYUwCMyWA/247799926-linear-digressions-survival-analysis.mp3" length="22109028" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/247799926-linear-digressions-survival-analysis.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/247102635</guid>
      <title>Gravitational Waves</title>
      <pubDate>Mon, 15 Feb 2016 02:46:22 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/TajO4PdXIJ0/gravitational-waves</link>
      <itunes:duration>00:20:26</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>All aboard the gravitational waves bandwagon--with the first direct observation of gravitational waves announced this week, Katie's dusting off her physics PhD for a very special gravity-related episode.  Discussed in this episode: what are gravitational waves, how are they detected, and what does this announcement mean for future studies of the universe.

Relevant links:
http://www.nytimes.com/2016/02/12/science/ligo-gravitational-waves-black-holes-einstein.html
https://www.ligo.caltech.edu/news/ligo20160211</itunes:summary>
      <itunes:subtitle>All aboard the gravitational waves bandwagon--wit…</itunes:subtitle>
      <description>All aboard the gravitational waves bandwagon--with the first direct observation of gravitational waves announced this week, Katie's dusting off her physics PhD for a very special gravity-related episode.  Discussed in this episode: what are gravitational waves, how are they detected, and what does this announcement mean for future studies of the universe.

Relevant links:
http://www.nytimes.com/2016/02/12/science/ligo-gravitational-waves-black-holes-einstein.html
https://www.ligo.caltech.edu/news/ligo20160211&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/TajO4PdXIJ0" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/IzCNnhf5JZw/247102635-linear-digressions-gravitational-waves.mp3" fileSize="29439824" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/gravitational-waves</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/IzCNnhf5JZw/247102635-linear-digressions-gravitational-waves.mp3" length="29439824" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/247102635-linear-digressions-gravitational-waves.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/246641616</guid>
      <title>The Turing Test</title>
      <pubDate>Fri, 12 Feb 2016 04:11:23 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/WOioUi2LtQY/the-turing-test</link>
      <itunes:duration>00:15:15</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Let's imagine a future in which a truly intelligent computer program exists.  How would it convince us (humanity) that it was intelligent?  Alan Turing's answer to this question, proposed over 60 years ago, is that the program could convince a human conversational partner that it, the computer, was in fact a human.  60 years later, the Turing Test endures as a gold standard of artificial intelligence.  It hasn't been beaten, either--yet.

Relevant links:
https://en.wikipedia.org/wiki/Turing_test
http://commonsensereasoning.org/winograd.html
http://consumerist.com/2015/09/29/its-not-just-you-robots-are-also-bad-at-assembling-ikea-furniture/</itunes:summary>
      <itunes:subtitle>Let's imagine a future in which a truly intellige…</itunes:subtitle>
      <description>Let's imagine a future in which a truly intelligent computer program exists.  How would it convince us (humanity) that it was intelligent?  Alan Turing's answer to this question, proposed over 60 years ago, is that the program could convince a human conversational partner that it, the computer, was in fact a human.  60 years later, the Turing Test endures as a gold standard of artificial intelligence.  It hasn't been beaten, either--yet.

Relevant links:
https://en.wikipedia.org/wiki/Turing_test
http://commonsensereasoning.org/winograd.html
http://consumerist.com/2015/09/29/its-not-just-you-robots-are-also-bad-at-assembling-ikea-furniture/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/WOioUi2LtQY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/IqJX8eaNqVU/246641616-linear-digressions-the-turing-test.mp3" fileSize="21964206" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/the-turing-test</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/IqJX8eaNqVU/246641616-linear-digressions-the-turing-test.mp3" length="21964206" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/246641616-linear-digressions-the-turing-test.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/245976298</guid>
      <title>Item Response Theory: how smart ARE you?</title>
      <pubDate>Mon, 08 Feb 2016 03:37:58 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Lrw26JeRM2o/item-response-theory-how-smart-are-you</link>
      <itunes:duration>00:11:46</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Psychometrics is all about measuring the psychological characteristics of people; for example, scholastic aptitude.  How is this done?  Tests, of course!  But there's a chicken-and-egg problem here: you need to know both how hard a test is, and how smart the test-taker is, in order to get the results you want.  How to solve this problem, one equation with two unknowns?  Item response theory--the data science behind such tests and the GRE.

Relevant links:
https://en.wikipedia.org/wiki/Item_response_theory</itunes:summary>
      <itunes:subtitle>Psychometrics is all about measuring the psycholo…</itunes:subtitle>
      <description>Psychometrics is all about measuring the psychological characteristics of people; for example, scholastic aptitude.  How is this done?  Tests, of course!  But there's a chicken-and-egg problem here: you need to know both how hard a test is, and how smart the test-taker is, in order to get the results you want.  How to solve this problem, one equation with two unknowns?  Item response theory--the data science behind such tests and the GRE.

Relevant links:
https://en.wikipedia.org/wiki/Item_response_theory&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Lrw26JeRM2o" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3fyDlFrhF34/245976298-linear-digressions-item-response-theory-how-smart-are-you.mp3" fileSize="16959980" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/item-response-theory-how-smart-are-you</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3fyDlFrhF34/245976298-linear-digressions-item-response-theory-how-smart-are-you.mp3" length="16959980" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/245976298-linear-digressions-item-response-theory-how-smart-are-you.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/245529396</guid>
      <title>Go!</title>
      <pubDate>Fri, 05 Feb 2016 04:52:36 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/k7UtCXWjFnI/go-1</link>
      <itunes:duration>00:19:59</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>As you may have heard, a computer beat a world-class human player in Go last week.  As recently as a year ago the prediction was that it would take a decade to get to this point, yet here we are, in 2016.  We'll talk about the history and strategy of game-playing computer programs, and what makes Google's AlphaGo so special.

Relevant link:
http://googleresearch.blogspot.com/2016/01/alphago-mastering-ancient-game-of-go.html</itunes:summary>
      <itunes:subtitle>As you may have heard, a computer beat a world-cl…</itunes:subtitle>
      <description>As you may have heard, a computer beat a world-class human player in Go last week.  As recently as a year ago the prediction was that it would take a decade to get to this point, yet here we are, in 2016.  We'll talk about the history and strategy of game-playing computer programs, and what makes Google's AlphaGo so special.

Relevant link:
http://googleresearch.blogspot.com/2016/01/alphago-mastering-ancient-game-of-go.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/k7UtCXWjFnI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/d244Giq2mYA/245529396-linear-digressions-go-1.mp3" fileSize="28773388" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/go-1</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/d244Giq2mYA/245529396-linear-digressions-go-1.mp3" length="28773388" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/245529396-linear-digressions-go-1.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/244832046</guid>
      <title>Great Social Networks in History</title>
      <pubDate>Mon, 01 Feb 2016 04:22:02 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/RDLJhZNCD8g/great-social-networks-in-history</link>
      <itunes:duration>00:12:42</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The Medici were one of the great ruling families of Europe during the Renaissance.  How did they come to rule?  Not power, or money, or armies, but through the strength of their social network.  And speaking of great historical social networks, analysis of the network of letter-writing during the Enlightenment is helping humanities scholars track the dispersion of great ideas across the world during that time, from Voltaire to Benjamin Franklin and everyone in between.

Relevant links:
https://www2.bc.edu/~jonescq/mb851/Mar12/PadgettAnsell_AJS_1993.pdf
http://republicofletters.stanford.edu/index.html</itunes:summary>
      <itunes:subtitle>The Medici were one of the great ruling families …</itunes:subtitle>
      <description>The Medici were one of the great ruling families of Europe during the Renaissance.  How did they come to rule?  Not power, or money, or armies, but through the strength of their social network.  And speaking of great historical social networks, analysis of the network of letter-writing during the Enlightenment is helping humanities scholars track the dispersion of great ideas across the world during that time, from Voltaire to Benjamin Franklin and everyone in between.

Relevant links:
https://www2.bc.edu/~jonescq/mb851/Mar12/PadgettAnsell_AJS_1993.pdf
http://republicofletters.stanford.edu/index.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/RDLJhZNCD8g" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/0emqvi1pgzQ/244832046-linear-digressions-great-social-networks-in-history.mp3" fileSize="18290344" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/great-social-networks-in-history</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/0emqvi1pgzQ/244832046-linear-digressions-great-social-networks-in-history.mp3" length="18290344" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/244832046-linear-digressions-great-social-networks-in-history.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/244359703</guid>
      <title>How Much to Pay a Spy (and a lil' more auctions)</title>
      <pubDate>Fri, 29 Jan 2016 05:36:33 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/fkDlIb2n6oU/how-much-to-pay-a-spy-and-a-lil-more-auctions</link>
      <itunes:duration>00:16:59</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>A few small encores on auction theory, and then--how can you value a piece of information before you know what it is?  Decision theory has some pointers.  Some highly relevant information if you are trying to figure out how much to pay a spy.

Relevant links:
https://tuecontheoryofnetworks.wordpress.com/2013/02/25/the-origin-of-the-dutch-auction/
http://www.nowozin.net/sebastian/blog/the-fair-price-to-pay-a-spy-an-introduction-to-the-value-of-information.html
</itunes:summary>
      <itunes:subtitle>A few small encores on auction theory, and then--…</itunes:subtitle>
      <description>A few small encores on auction theory, and then--how can you value a piece of information before you know what it is?  Decision theory has some pointers.  Some highly relevant information if you are trying to figure out how much to pay a spy.

Relevant links:
https://tuecontheoryofnetworks.wordpress.com/2013/02/25/the-origin-of-the-dutch-auction/
http://www.nowozin.net/sebastian/blog/the-fair-price-to-pay-a-spy-an-introduction-to-the-value-of-information.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/fkDlIb2n6oU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TZt4SGoMqZU/244359703-linear-digressions-how-much-to-pay-a-spy-and-a-lil-more-auctions.mp3" fileSize="24455660" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/how-much-to-pay-a-spy-and-a-lil-more-auctions</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TZt4SGoMqZU/244359703-linear-digressions-how-much-to-pay-a-spy-and-a-lil-more-auctions.mp3" length="24455660" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/244359703-linear-digressions-how-much-to-pay-a-spy-and-a-lil-more-auctions.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/243636665</guid>
      <title>Sold!  Auctions (Part 2)</title>
      <pubDate>Mon, 25 Jan 2016 02:58:07 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/r2zC0i64uKw/sold-auctions-part-2</link>
      <itunes:duration>00:17:27</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The Google ads auction is a special kind of auction, one you might not know as well as the famous English auction (which we talked about in the last episode).  But if it's what Google uses to sell billions of dollars of ad space in real time, you know it must be pretty cool.

Relevant links:
https://en.wikipedia.org/wiki/English_auction
http://people.ischool.berkeley.edu/~hal/Papers/2006/position.pdf
http://www.benedelman.org/publications/gsp-060801.pdf </itunes:summary>
      <itunes:subtitle>The Google ads auction is a special kind of aucti…</itunes:subtitle>
      <description>The Google ads auction is a special kind of auction, one you might not know as well as the famous English auction (which we talked about in the last episode).  But if it's what Google uses to sell billions of dollars of ad space in real time, you know it must be pretty cool.

Relevant links:
https://en.wikipedia.org/wiki/English_auction
http://people.ischool.berkeley.edu/~hal/Papers/2006/position.pdf
http://www.benedelman.org/publications/gsp-060801.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/r2zC0i64uKw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/zFnOeUd2o-U/243636665-linear-digressions-sold-auctions-part-2.mp3" fileSize="25142785" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/sold-auctions-part-2</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/zFnOeUd2o-U/243636665-linear-digressions-sold-auctions-part-2.mp3" length="25142785" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/243636665-linear-digressions-sold-auctions-part-2.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/243171056</guid>
      <title>Going Once, Going Twice: Auctions (Part 1)</title>
      <pubDate>Fri, 22 Jan 2016 03:40:24 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/DPVbjo5oKNY/going-once-going-twice-auctions-part-1</link>
      <itunes:duration>00:12:39</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The Google AdWords algorithm is (famously) an auction system for allocating a massive amount of online ad space in real time--with that fascinating use case in mind, this episode is part one in a two-part series all about auctions.  We dive into the theory of auctions, and what makes a "good" auction.

Relevant links:
https://en.wikipedia.org/wiki/English_auction
http://people.ischool.berkeley.edu/~hal/Papers/2006/position.pdf
http://www.benedelman.org/publications/gsp-060801.pdf </itunes:summary>
      <itunes:subtitle>The Google AdWords algorithm is (famously) an auc…</itunes:subtitle>
      <description>The Google AdWords algorithm is (famously) an auction system for allocating a massive amount of online ad space in real time--with that fascinating use case in mind, this episode is part one in a two-part series all about auctions.  We dive into the theory of auctions, and what makes a "good" auction.

Relevant links:
https://en.wikipedia.org/wiki/English_auction
http://people.ischool.berkeley.edu/~hal/Papers/2006/position.pdf
http://www.benedelman.org/publications/gsp-060801.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/DPVbjo5oKNY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-XfpvkZZDic/243171056-linear-digressions-going-once-going-twice-auctions-part-1.mp3" fileSize="18236428" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/going-once-going-twice-auctions-part-1</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/-XfpvkZZDic/243171056-linear-digressions-going-once-going-twice-auctions-part-1.mp3" length="18236428" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/243171056-linear-digressions-going-once-going-twice-auctions-part-1.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/242498218</guid>
      <title>Chernoff Faces and Minard Maps</title>
      <pubDate>Mon, 18 Jan 2016 03:38:33 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/aTBfKqwCnus/chernoff-faces-and-minard-maps</link>
      <itunes:duration>00:15:11</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>A data visualization extravaganza in this episode, as we discuss Chernoff faces (you: "faces? huh?" us: "oh just you wait") and the greatest data visualization of all time, or at least the Napoleonic era.

Relevant links:
http://lya.fciencias.unam.mx/rfuentes/faces-chernoff.pdf
https://en.wikipedia.org/wiki/Charles_Joseph_Minard</itunes:summary>
      <itunes:subtitle>A data visualization extravaganza in this episode…</itunes:subtitle>
      <description>A data visualization extravaganza in this episode, as we discuss Chernoff faces (you: "faces? huh?" us: "oh just you wait") and the greatest data visualization of all time, or at least the Napoleonic era.

Relevant links:
http://lya.fciencias.unam.mx/rfuentes/faces-chernoff.pdf
https://en.wikipedia.org/wiki/Charles_Joseph_Minard&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/aTBfKqwCnus" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/RGOuGSeow-Y/242498218-linear-digressions-chernoff-faces-and-minard-maps.mp3" fileSize="21866403" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/chernoff-faces-and-minard-maps</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/RGOuGSeow-Y/242498218-linear-digressions-chernoff-faces-and-minard-maps.mp3" length="21866403" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/242498218-linear-digressions-chernoff-faces-and-minard-maps.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/242037264</guid>
      <title>t-SNE: Reduce Your Dimensions, Keep Your Clusters</title>
      <pubDate>Fri, 15 Jan 2016 04:05:49 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/gC04M59Xa_w/t-sne-reduce-your-dimensions-keep-your-clusters</link>
      <itunes:duration>00:16:55</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Ever tried to visualize a cluster of data points in 40 dimensions?  Or even 4, for that matter?  We prefer to stick to 2, or maybe 3 if we're feeling well-caffeinated.  The t-SNE algorithm is one of the best tools on the market for doing dimensionality reduction when you have clustering in mind.

Relevant links:
https://www.youtube.com/watch?v=RJVL80Gg3lA</itunes:summary>
      <itunes:subtitle>Ever tried to visualize a cluster of data points …</itunes:subtitle>
      <description>Ever tried to visualize a cluster of data points in 40 dimensions?  Or even 4, for that matter?  We prefer to stick to 2, or maybe 3 if we're feeling well-caffeinated.  The t-SNE algorithm is one of the best tools on the market for doing dimensionality reduction when you have clustering in mind.

Relevant links:
https://www.youtube.com/watch?v=RJVL80Gg3lA&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/gC04M59Xa_w" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Kqsxe7NmWSI/242037264-linear-digressions-t-sne-reduce-your-dimensions-keep-your-clusters.mp3" fileSize="24358485" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/t-sne-reduce-your-dimensions-keep-your-clusters</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Kqsxe7NmWSI/242037264-linear-digressions-t-sne-reduce-your-dimensions-keep-your-clusters.mp3" length="24358485" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/242037264-linear-digressions-t-sne-reduce-your-dimensions-keep-your-clusters.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/241376793</guid>
      <title>The [Expletive Deleted] Problem</title>
      <pubDate>Mon, 11 Jan 2016 04:23:53 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/t0tdnlFlLzk/the-expletive-deleted-problem</link>
      <itunes:duration>00:09:54</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The town of [expletive deleted], England, is responsible for the clbuttic [expletive deleted] problem.  This week on Linear Digressions: we try really hard not to swear too much.

Related links:
https://en.wikipedia.org/wiki/Scunthorpe_problem
https://www.washingtonpost.com/news/worldviews/wp/2016/01/05/where-is-russia-actually-mordor-in-the-world-of-google-translate/</itunes:summary>
      <itunes:subtitle>The town of [expletive deleted], England, is resp…</itunes:subtitle>
      <description>The town of [expletive deleted], England, is responsible for the clbuttic [expletive deleted] problem.  This week on Linear Digressions: we try really hard not to swear too much.

Related links:
https://en.wikipedia.org/wiki/Scunthorpe_problem
https://www.washingtonpost.com/news/worldviews/wp/2016/01/05/where-is-russia-actually-mordor-in-the-world-of-google-translate/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/t0tdnlFlLzk" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/EXregNxjSc4/241376793-linear-digressions-the-expletive-deleted-problem.mp3" fileSize="14276055" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/the-expletive-deleted-problem</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/EXregNxjSc4/241376793-linear-digressions-the-expletive-deleted-problem.mp3" length="14276055" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/241376793-linear-digressions-the-expletive-deleted-problem.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/240917031</guid>
      <title>Unlabeled Supervised Learning--whaaa?</title>
      <pubDate>Fri, 08 Jan 2016 03:26:56 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/NIjIf2TIkxI/unlabeled-supervised-learning-whaaa</link>
      <itunes:duration>00:12:35</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In order to do supervised learning, you need a labeled training dataset.  Or do you...?

Relevant links:
http://www.cs.columbia.edu/~dplewis/candidacy/goldman00enhancing.pdf</itunes:summary>
      <itunes:subtitle>In order to do supervised learning, you need a la…</itunes:subtitle>
      <description>In order to do supervised learning, you need a labeled training dataset.  Or do you...?

Relevant links:
http://www.cs.columbia.edu/~dplewis/candidacy/goldman00enhancing.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/NIjIf2TIkxI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/oesZm3aN_ug/240917031-linear-digressions-unlabeled-supervised-learning-whaaa.mp3" fileSize="18125459" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/unlabeled-supervised-learning-whaaa</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/oesZm3aN_ug/240917031-linear-digressions-unlabeled-supervised-learning-whaaa.mp3" length="18125459" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/240917031-linear-digressions-unlabeled-supervised-learning-whaaa.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/240444897</guid>
      <title>Hacking Neural Nets</title>
      <pubDate>Tue, 05 Jan 2016 02:56:18 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/tFBudJRbxaE/hacking-neural-nets</link>
      <itunes:duration>00:15:28</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Machine learning: it can be fooled, just like you or me.  Here's one of our favorite examples, a study into hacking neural networks.

Relevant links:
http://arxiv.org/pdf/1412.1897v4.pdf</itunes:summary>
      <itunes:subtitle>Machine learning: it can be fooled, just like you…</itunes:subtitle>
      <description>Machine learning: it can be fooled, just like you or me.  Here's one of our favorite examples, a study into hacking neural networks.

Relevant links:
http://arxiv.org/pdf/1412.1897v4.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/tFBudJRbxaE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kUFdcg5naK8/240444897-linear-digressions-hacking-neural-nets.mp3" fileSize="22272033" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/hacking-neural-nets</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kUFdcg5naK8/240444897-linear-digressions-hacking-neural-nets.mp3" length="22272033" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/240444897-linear-digressions-hacking-neural-nets.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/239861862</guid>
      <title>Zipf's Law</title>
      <pubDate>Thu, 31 Dec 2015 18:08:17 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/FC1nL8VBzRc/zipfs-law</link>
      <itunes:duration>00:11:43</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Zipf's law is related to the statistics of how word usage is distributed.  As it turns out, this is also strikingly reminiscent of how income is distributed, and populations of cities, and bug reports in software, as well as tons of other phenomena that we all interact with every day.

Relevant links:
http://economix.blogs.nytimes.com/2010/04/20/a-tale-of-many-cities/
http://arxiv.org/pdf/cond-mat/0412004.pdf
https://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/</itunes:summary>
      <itunes:subtitle>Zipf's law is related to the statistics of how wo…</itunes:subtitle>
      <description>Zipf's law is related to the statistics of how word usage is distributed.  As it turns out, this is also strikingly reminiscent of how income is distributed, and populations of cities, and bug reports in software, as well as tons of other phenomena that we all interact with every day.

Relevant links:
http://economix.blogs.nytimes.com/2010/04/20/a-tale-of-many-cities/
http://arxiv.org/pdf/cond-mat/0412004.pdf
https://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/FC1nL8VBzRc" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/KIlTiUrYxeQ/239861862-linear-digressions-zipfs-law.mp3" fileSize="16880986" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/zipfs-law</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/KIlTiUrYxeQ/239861862-linear-digressions-zipfs-law.mp3" length="16880986" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/239861862-linear-digressions-zipfs-law.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/239700354</guid>
      <title>Indie Announcement</title>
      <pubDate>Wed, 30 Dec 2015 15:57:02 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/jGX8GXJinMk/indie-announcement</link>
      <itunes:duration>00:01:19</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>We've gone indie!  Which shouldn't change anything about the podcast that you know and love, but we're super excited to keep bringing you Linear Digressions as a fully independent podcast.

Some links mentioned in the show:
https://twitter.com/lindigressions
https://twitter.com/benjaffe
https://twitter.com/multiarmbandit
https://soundcloud.com/linear-digressions
http://lineardigressions.com/</itunes:summary>
      <itunes:subtitle>We've gone indie!  Which shouldn't change anythin…</itunes:subtitle>
      <description>We've gone indie!  Which shouldn't change anything about the podcast that you know and love, but we're super excited to keep bringing you Linear Digressions as a fully independent podcast.

Some links mentioned in the show:
https://twitter.com/lindigressions
https://twitter.com/benjaffe
https://twitter.com/multiarmbandit
https://soundcloud.com/linear-digressions
http://lineardigressions.com/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/jGX8GXJinMk" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Uj9WlIz0P-4/239700354-linear-digressions-indie-announcement.mp3" fileSize="1912823" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/indie-announcement</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Uj9WlIz0P-4/239700354-linear-digressions-indie-announcement.mp3" length="1912823" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/239700354-linear-digressions-indie-announcement.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/239285070</guid>
      <title>Portrait Beauty</title>
      <pubDate>Sun, 27 Dec 2015 13:34:44 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/uc3-osc4taU/portrait-beauty</link>
      <itunes:duration>00:11:44</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>It's Da Vinci meets Skynet: what makes a portrait beautiful, according to a machine learning algorithm.  Snap a selfie and give us a listen.</itunes:summary>
      <itunes:subtitle>It's Da Vinci meets Skynet: what makes a portrait…</itunes:subtitle>
      <description>It's Da Vinci meets Skynet: what makes a portrait beautiful, according to a machine learning algorithm.  Snap a selfie and give us a listen.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/uc3-osc4taU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Bxa0u5FvH1o/239285070-linear-digressions-portrait-beauty.mp3" fileSize="16912333" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/portrait-beauty</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Bxa0u5FvH1o/239285070-linear-digressions-portrait-beauty.mp3" length="16912333" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/239285070-linear-digressions-portrait-beauty.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/238051584</guid>
      <title>The Cocktail Party Problem</title>
      <pubDate>Fri, 18 Dec 2015 00:17:31 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/g7c4LYnLR9U/the-cocktail-party-problem</link>
      <itunes:duration>00:12:04</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Grab a cocktail, put on your favorite karaoke track, and let’s talk some more about disentangling audio data!</itunes:summary>
      <itunes:subtitle>Grab a cocktail, put on your favorite karaoke tra…</itunes:subtitle>
      <description>Grab a cocktail, put on your favorite karaoke track, and let’s talk some more about disentangling audio data!&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/g7c4LYnLR9U" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Rk-Pg2AhyAE/238051584-linear-digressions-the-cocktail-party-problem.mp3" fileSize="23167510" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/the-cocktail-party-problem</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Rk-Pg2AhyAE/238051584-linear-digressions-the-cocktail-party-problem.mp3" length="23167510" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/238051584-linear-digressions-the-cocktail-party-problem.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/235997508</guid>
      <title>A Criminally Short Introduction to Semi Supervised Learning</title>
      <pubDate>Fri, 04 Dec 2015 03:13:55 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/HEikcdjssIU/a-criminally-short-introduction-to-semi-supervised-learning</link>
      <itunes:duration>00:09:12</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Because there are more interesting problems than there are labeled datasets, semi-supervised learning provides a framework for getting feedback from the environment as a proxy for labels of what's "correct."  Of all the machine learning methodologies, it might also be the closest to how humans usually learn--we go through the world, getting (noisy) feedback on the choices we make and learn from the outcomes of our actions.  </itunes:summary>
      <itunes:subtitle>Because there are more interesting problems than …</itunes:subtitle>
      <description>Because there are more interesting problems than there are labeled datasets, semi-supervised learning provides a framework for getting feedback from the environment as a proxy for labels of what's "correct."  Of all the machine learning methodologies, it might also be the closest to how humans usually learn--we go through the world, getting (noisy) feedback on the choices we make and learn from the outcomes of our actions.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/HEikcdjssIU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/8jXcud1-WP8/235997508-linear-digressions-a-criminally-short-introduction-to-semi-supervised-learning.mp3" fileSize="13264803" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/a-criminally-short-introduction-to-semi-supervised-learning</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/8jXcud1-WP8/235997508-linear-digressions-a-criminally-short-introduction-to-semi-supervised-learning.mp3" length="13264803" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/235997508-linear-digressions-a-criminally-short-introduction-to-semi-supervised-learning.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/234982280</guid>
      <title>Thresholdout: Down with Overfitting</title>
      <pubDate>Fri, 27 Nov 2015 17:55:04 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/l3z0lKCXozQ/thresholdout-down-with-overfitting</link>
      <itunes:duration>00:15:52</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Overfitting to your training data can be avoided by evaluating your machine learning algorithm on a holdout test dataset, but what about overfitting to the test data?  Turns out it can be done, easily, and you have to be very careful to avoid it.  But an algorithm from the field of privacy research shows promise for keeping your test data safe from accidental overfitting</itunes:summary>
      <itunes:subtitle>Overfitting to your training data can be avoided …</itunes:subtitle>
      <description>Overfitting to your training data can be avoided by evaluating your machine learning algorithm on a holdout test dataset, but what about overfitting to the test data?  Turns out it can be done, easily, and you have to be very careful to avoid it.  But an algorithm from the field of privacy research shows promise for keeping your test data safe from accidental overfitting&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/l3z0lKCXozQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/VJ2xgdoRhpg/234982280-linear-digressions-thresholdout-down-with-overfitting.mp3" fileSize="22852578" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/thresholdout-down-with-overfitting</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/VJ2xgdoRhpg/234982280-linear-digressions-thresholdout-down-with-overfitting.mp3" length="22852578" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/234982280-linear-digressions-thresholdout-down-with-overfitting.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/232380447</guid>
      <title>The State of Data Science</title>
      <pubDate>Tue, 10 Nov 2015 04:36:40 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/3tKWjYRXa2U/the-state-of-data-science</link>
      <itunes:duration>00:15:40</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>How many data scientists are there, where do they live, where do they work, what kind of tools do they use, and how do they describe themselves?  RJMetrics wanted to know the answers to these questions, so they decided to find out and share their analysis with the world.  In this very special interview episode, we welcome Tristan Handy, VP of Marketing at RJMetrics, who will talk about "The State of Data Science Report."</itunes:summary>
      <itunes:subtitle>How many data scientists are there, where do they…</itunes:subtitle>
      <description>How many data scientists are there, where do they live, where do they work, what kind of tools do they use, and how do they describe themselves?  RJMetrics wanted to know the answers to these questions, so they decided to find out and share their analysis with the world.  In this very special interview episode, we welcome Tristan Handy, VP of Marketing at RJMetrics, who will talk about "The State of Data Science Report."&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/3tKWjYRXa2U" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/JPbVdljxR6w/232380447-linear-digressions-the-state-of-data-science.mp3" fileSize="22577352" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/the-state-of-data-science</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/JPbVdljxR6w/232380447-linear-digressions-the-state-of-data-science.mp3" length="22577352" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/232380447-linear-digressions-the-state-of-data-science.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/231777419</guid>
      <title>Data Science for Making the World a Better Place</title>
      <pubDate>Fri, 06 Nov 2015 03:43:25 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/QASUCbE4kiI/data-science-for-making-the-world-a-better-place</link>
      <itunes:duration>00:09:31</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>There's a good chance that great data science is going on close to you, and that it's going toward making your city, state, country, and planet a better place.  Not all the data science questions being tackled out there are about finding the sleekest new algorithm or billion-dollar company idea--there's a whole world of social data science that just wants to make the world a better place to live in.</itunes:summary>
      <itunes:subtitle>There's a good chance that great data science is …</itunes:subtitle>
      <description>There's a good chance that great data science is going on close to you, and that it's going toward making your city, state, country, and planet a better place.  Not all the data science questions being tackled out there are about finding the sleekest new algorithm or billion-dollar company idea--there's a whole world of social data science that just wants to make the world a better place to live in.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/QASUCbE4kiI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/rxhVtJsVouc/231777419-linear-digressions-data-science-for-making-the-world-a-better-place.mp3" fileSize="13718707" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/data-science-for-making-the-world-a-better-place</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/rxhVtJsVouc/231777419-linear-digressions-data-science-for-making-the-world-a-better-place.mp3" length="13718707" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/231777419-linear-digressions-data-science-for-making-the-world-a-better-place.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/230576907</guid>
      <title>Kalman Runners</title>
      <pubDate>Thu, 29 Oct 2015 03:10:02 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/D6vVFTTC1zs/kalman-runners</link>
      <itunes:duration>00:14:42</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The Kalman Filter is an algorithm for taking noisy measurements of dynamic systems and using them to get a better idea of the underlying dynamics than you could get from a simple extrapolation.  If you've ever run a marathon, or been a nuclear missile, you probably know all about these challenges already.  By the way, we neglected to mention in the episode: Katie's marathon time was 3:54:27!</itunes:summary>
      <itunes:subtitle>The Kalman Filter is an algorithm for taking nois…</itunes:subtitle>
      <description>The Kalman Filter is an algorithm for taking noisy measurements of dynamic systems and using them to get a better idea of the underlying dynamics than you could get from a simple extrapolation.  If you've ever run a marathon, or been a nuclear missile, you probably know all about these challenges already.  By the way, we neglected to mention in the episode: Katie's marathon time was 3:54:27!&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/D6vVFTTC1zs" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/9pf-wgVvGmk/230576907-linear-digressions-kalman-runners.mp3" fileSize="21185548" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/kalman-runners</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/9pf-wgVvGmk/230576907-linear-digressions-kalman-runners.mp3" length="21185548" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/230576907-linear-digressions-kalman-runners.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/229669968</guid>
      <title>Neural Net Inception</title>
      <pubDate>Fri, 23 Oct 2015 02:25:48 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/KGUREk82-8I/neural-net-inception</link>
      <itunes:duration>00:15:19</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>When you sleep, the neural pathways in your brain take the "white noise" of your resting brain, mix in your experiences and imagination, and the result is dreams (that is a highly unscientific explanation, but you get the idea).  What happens when neural nets are put through the same process?  Train a neural net to recognize pictures, and then send through an image of white noise, and it will start to see some weird (but cool!) stuff.</itunes:summary>
      <itunes:subtitle>When you sleep, the neural pathways in your brain…</itunes:subtitle>
      <description>When you sleep, the neural pathways in your brain take the "white noise" of your resting brain, mix in your experiences and imagination, and the result is dreams (that is a highly unscientific explanation, but you get the idea).  What happens when neural nets are put through the same process?  Train a neural net to recognize pictures, and then send through an image of white noise, and it will start to see some weird (but cool!) stuff.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/KGUREk82-8I" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TulK7HI6Ww4/229669968-linear-digressions-neural-net-inception.mp3" fileSize="22063889" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/neural-net-inception</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TulK7HI6Ww4/229669968-linear-digressions-neural-net-inception.mp3" length="22063889" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/229669968-linear-digressions-neural-net-inception.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/228617192</guid>
      <title>Benford's Law</title>
      <pubDate>Fri, 16 Oct 2015 03:30:43 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/OCsSxVyCM4E/benford-produced</link>
      <itunes:duration>00:17:42</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Sometimes numbers are... weird.  Benford's Law is a favorite example of this for us--it's a law that governs the distribution of the first digit in certain types of numbers.  As it turns out, if you're looking up the length of a river, the population of a country, the price of a stock... not all first digits are created equal.</itunes:summary>
      <itunes:subtitle>Sometimes numbers are... weird.  Benford's Law is…</itunes:subtitle>
      <description>Sometimes numbers are... weird.  Benford's Law is a favorite example of this for us--it's a law that governs the distribution of the first digit in certain types of numbers.  As it turns out, if you're looking up the length of a river, the population of a country, the price of a stock... not all first digits are created equal.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/OCsSxVyCM4E" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/vxMF9ltpvUM/228617192-linear-digressions-benford-produced.mp3" fileSize="25502021" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/benford-produced</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/vxMF9ltpvUM/228617192-linear-digressions-benford-produced.mp3" length="25502021" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/228617192-linear-digressions-benford-produced.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/227272985</guid>
      <title>Guinness</title>
      <pubDate>Wed, 07 Oct 2015 03:30:33 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/3YZulCOHjag/guinness</link>
      <itunes:duration>00:14:43</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Not to oversell it, but the student's t-test has got to have the most interesting history of any statistical test.  Which is saying a lot, right?  Add some boozy statistical trivia to your arsenal in this epsiode.</itunes:summary>
      <itunes:subtitle>Not to oversell it, but the student's t-test has …</itunes:subtitle>
      <description>Not to oversell it, but the student's t-test has got to have the most interesting history of any statistical test.  Which is saying a lot, right?  Add some boozy statistical trivia to your arsenal in this epsiode.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/3YZulCOHjag" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/GrrVEzPfEHc/227272985-linear-digressions-guinness.mp3" fileSize="21206237" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/guinness</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/GrrVEzPfEHc/227272985-linear-digressions-guinness.mp3" length="21206237" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/227272985-linear-digressions-guinness.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/221965091</guid>
      <title>PFun with P Values</title>
      <pubDate>Wed, 02 Sep 2015 03:24:36 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/3IIUBBhpvts/pfun-with-p-values</link>
      <itunes:duration>00:17:07</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Doing some science, and want to know if you might have found something?  Or maybe you've just accomplished the scientific equivalent of going fishing and reeling in an old boot?  Frequentist p-values can help you distinguish between "eh" and "oooh interesting".  Also, there's a lot of physics in this episode, nerds.</itunes:summary>
      <itunes:subtitle>Doing some science, and want to know if you might…</itunes:subtitle>
      <description>Doing some science, and want to know if you might have found something?  Or maybe you've just accomplished the scientific equivalent of going fishing and reeling in an old boot?  Frequentist p-values can help you distinguish between "eh" and "oooh interesting".  Also, there's a lot of physics in this episode, nerds.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/3IIUBBhpvts" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Mf4mlMNo_n4/221965091-linear-digressions-pfun-with-p-values.mp3" fileSize="24660669" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/pfun-with-p-values</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Mf4mlMNo_n4/221965091-linear-digressions-pfun-with-p-values.mp3" length="24660669" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/221965091-linear-digressions-pfun-with-p-values.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/220755207</guid>
      <title>Watson</title>
      <pubDate>Tue, 25 Aug 2015 02:26:20 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/4SbRLQhDikU/watson</link>
      <itunes:duration>00:15:36</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This machine learning algorithm beat the human champions at Jeopardy.  What is... Watson?</itunes:summary>
      <itunes:subtitle>This machine learning algorithm beat the human ch…</itunes:subtitle>
      <description>This machine learning algorithm beat the human champions at Jeopardy.  What is... Watson?&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/4SbRLQhDikU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/4K3zpMPQ7Vs/220755207-linear-digressions-watson.mp3" fileSize="22480803" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/watson</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/4K3zpMPQ7Vs/220755207-linear-digressions-watson.mp3" length="22480803" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/220755207-linear-digressions-watson.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/219708024</guid>
      <title>Bayesian Psychics</title>
      <pubDate>Tue, 18 Aug 2015 00:05:04 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/QgtyQXlpPNQ/bayesian-psychics</link>
      <itunes:duration>00:11:44</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Come get a little "out there" with us this week, as we use a meta-study of extrasensory perception (or ESP, often used in the same sentence as "psychics") to chat about Bayesian vs. frequentist statistics.</itunes:summary>
      <itunes:subtitle>Come get a little "out there" with us this week, …</itunes:subtitle>
      <description>Come get a little "out there" with us this week, as we use a meta-study of extrasensory perception (or ESP, often used in the same sentence as "psychics") to chat about Bayesian vs. frequentist statistics.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/QgtyQXlpPNQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/br6CmKlmDng/219708024-linear-digressions-bayesian-psychics.mp3" fileSize="16897913" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/bayesian-psychics</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/br6CmKlmDng/219708024-linear-digressions-bayesian-psychics.mp3" length="16897913" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/219708024-linear-digressions-bayesian-psychics.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/218255240</guid>
      <title>Troll Detection</title>
      <pubDate>Fri, 07 Aug 2015 20:56:36 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/c-NAN1xlsVw/troll-detection</link>
      <itunes:duration>00:12:57</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Ever found yourself wasting time reading online comments from trolls?  Of course you have; we've all been there (it's 4 AM but I can't turn off the computer and go to sleep--someone on the internet is WRONG!).  Now there's a way to use machine learning to automatically detect trolls, and minimize the impact when they try to derail online conversations.</itunes:summary>
      <itunes:subtitle>Ever found yourself wasting time reading online c…</itunes:subtitle>
      <description>Ever found yourself wasting time reading online comments from trolls?  Of course you have; we've all been there (it's 4 AM but I can't turn off the computer and go to sleep--someone on the internet is WRONG!).  Now there's a way to use machine learning to automatically detect trolls, and minimize the impact when they try to derail online conversations.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/c-NAN1xlsVw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/uyuSI91JG-s/218255240-linear-digressions-troll-detection.mp3" fileSize="18648953" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/troll-detection</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/uyuSI91JG-s/218255240-linear-digressions-troll-detection.mp3" length="18648953" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/218255240-linear-digressions-troll-detection.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/217536447</guid>
      <title>Yiddish Translation</title>
      <pubDate>Mon, 03 Aug 2015 03:06:39 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ZBXf-UxMtjA/yiddish-translation</link>
      <itunes:duration>00:12:15</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Imagine a language that is mostly spoken rather than written, contains many words in other languages, and has relatively little written overlap with English.  Now imagine writing a machine-learning-based translation system that can convert that language to English.  That's the problem that confronted researchers when they set out to automatically translate between Yiddish and English; the tricks they used help us understand a lot about machine translation.</itunes:summary>
      <itunes:subtitle>Imagine a language that is mostly spoken rather t…</itunes:subtitle>
      <description>Imagine a language that is mostly spoken rather than written, contains many words in other languages, and has relatively little written overlap with English.  Now imagine writing a machine-learning-based translation system that can convert that language to English.  That's the problem that confronted researchers when they set out to automatically translate between Yiddish and English; the tricks they used help us understand a lot about machine translation.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ZBXf-UxMtjA" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/VqEQvMsFQ-E/217536447-linear-digressions-yiddish-translation.mp3" fileSize="17657763" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/yiddish-translation</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/VqEQvMsFQ-E/217536447-linear-digressions-yiddish-translation.mp3" length="17657763" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/217536447-linear-digressions-yiddish-translation.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/213562084</guid>
      <title>Modeling Particles in Atomic Bombs</title>
      <pubDate>Mon, 06 Jul 2015 23:30:15 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/xsOAR-UWGxU/modeling-particles-in-atomic-bombs</link>
      <itunes:duration>00:15:38</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In a fun historical journey, Katie and Ben explore the history of the Manhattan Project, discuss the difficulties in modeling particle movement in atomic bombs with only punch-card computers and ingenuity, and eventually come to present-day uses of the Metropolis-Hastings algorithm... mentioning Solitaire along the way.</itunes:summary>
      <itunes:subtitle>In a fun historical journey, Katie and Ben explor…</itunes:subtitle>
      <description>In a fun historical journey, Katie and Ben explore the history of the Manhattan Project, discuss the difficulties in modeling particle movement in atomic bombs with only punch-card computers and ingenuity, and eventually come to present-day uses of the Metropolis-Hastings algorithm... mentioning Solitaire along the way.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/xsOAR-UWGxU" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3IHPoQlaLqE/213562084-linear-digressions-modeling-particles-in-atomic-bombs.mp3" fileSize="22532212" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/modeling-particles-in-atomic-bombs</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3IHPoQlaLqE/213562084-linear-digressions-modeling-particles-in-atomic-bombs.mp3" length="22532212" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/213562084-linear-digressions-modeling-particles-in-atomic-bombs.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/211091633</guid>
      <title>Random Number Generation</title>
      <pubDate>Fri, 19 Jun 2015 18:49:55 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/fCdFiGVYfPI/random-number-generation</link>
      <itunes:duration>00:10:26</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Let's talk about randomness! Although randomness is pervasive throughout the natural world, it's surprisingly difficult to generate random numbers. And even if your numbers look random (but actually aren't), it can have interesting consequences on the security of systems, and the accuracy of models and research.

In this episode, Katie and Ben talk about randomness, its place in machine learning and computation in general, along with some random digressions of their own.</itunes:summary>
      <itunes:subtitle>Let's talk about randomness! Although randomness …</itunes:subtitle>
      <description>Let's talk about randomness! Although randomness is pervasive throughout the natural world, it's surprisingly difficult to generate random numbers. And even if your numbers look random (but actually aren't), it can have interesting consequences on the security of systems, and the accuracy of models and research.

In this episode, Katie and Ben talk about randomness, its place in machine learning and computation in general, along with some random digressions of their own.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/fCdFiGVYfPI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/tMUDhScuFl8/211091633-linear-digressions-random-number-generation.mp3" fileSize="15025874" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/random-number-generation</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/tMUDhScuFl8/211091633-linear-digressions-random-number-generation.mp3" length="15025874" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/211091633-linear-digressions-random-number-generation.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/209462126</guid>
      <title>Electoral Insights (Part 2)</title>
      <pubDate>Tue, 09 Jun 2015 02:46:17 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/FlSOQRgakfY/electoral-insights-part-2</link>
      <itunes:duration>00:21:18</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Following up on our last episode about how experiments can be performed in political science, now we explore a high-profile case of an experiment gone wrong.

An extremely high-profile paper that was published in 2014, about how talking to people can convince them to change their minds on topics like abortion and gay marriage, has been exposed as the likely product of a fraudulently produced dataset. We’ll talk about a cool data science tool called the Kolmogorov-Smirnov test, which a pair of graduate students used to reverse-engineer the likely way that the fraudulent data was generated.

But a bigger question still remains—what does this whole episode tell us about fraud and oversight in science?</itunes:summary>
      <itunes:subtitle>Following up on our last episode about how experi…</itunes:subtitle>
      <description>Following up on our last episode about how experiments can be performed in political science, now we explore a high-profile case of an experiment gone wrong.

An extremely high-profile paper that was published in 2014, about how talking to people can convince them to change their minds on topics like abortion and gay marriage, has been exposed as the likely product of a fraudulently produced dataset. We’ll talk about a cool data science tool called the Kolmogorov-Smirnov test, which a pair of graduate students used to reverse-engineer the likely way that the fraudulent data was generated.

But a bigger question still remains—what does this whole episode tell us about fraud and oversight in science?&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/FlSOQRgakfY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/j51W-3veWpM/209462126-linear-digressions-electoral-insights-part-2.mp3" fileSize="30675520" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/electoral-insights-part-2</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/j51W-3veWpM/209462126-linear-digressions-electoral-insights-part-2.mp3" length="30675520" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/209462126-linear-digressions-electoral-insights-part-2.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/208987996</guid>
      <title>Electoral Insights (Part 1)</title>
      <pubDate>Fri, 05 Jun 2015 20:38:00 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/YUzG7yuThVE/electoral-insights-part-1</link>
      <itunes:duration>00:09:17</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>The first of our two-parter discussing the recent electoral data fraud case. The results of the study in question were covered widely, including by This American Life (who later had to issue a retraction).

Data science for election research involves studying voters, who are people, and people are tricky to study—every one of them is different, and the same treatment can have different effects on different voters.  But with randomized controlled trials, small variations from person to person can even out when you look at a larger group.  With the advent of randomized experiments in elections a few decades ago, a whole new door was opened for studying the most effective ways to campaign.</itunes:summary>
      <itunes:subtitle>The first of our two-parter discussing the recent…</itunes:subtitle>
      <description>The first of our two-parter discussing the recent electoral data fraud case. The results of the study in question were covered widely, including by This American Life (who later had to issue a retraction).

Data science for election research involves studying voters, who are people, and people are tricky to study—every one of them is different, and the same treatment can have different effects on different voters.  But with randomized controlled trials, small variations from person to person can even out when you look at a larger group.  With the advent of randomized experiments in elections a few decades ago, a whole new door was opened for studying the most effective ways to campaign.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/YUzG7yuThVE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/hl1tMVaim2g/208987996-linear-digressions-electoral-insights-part-1.mp3" fileSize="13387683" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/electoral-insights-part-1</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/hl1tMVaim2g/208987996-linear-digressions-electoral-insights-part-1.mp3" length="13387683" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/208987996-linear-digressions-electoral-insights-part-1.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/208330505</guid>
      <title>Falsifying Data</title>
      <pubDate>Mon, 01 Jun 2015 21:04:10 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/-iNOSIVdA4w/falsifying-data</link>
      <itunes:duration>00:17:46</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In the first of a few episodes on fraud in election research, we’ll take a look at a case study from a previous Presidential election, where polling results were faked.

What are some telltale signs that data fraud might be present in a dataset?  We’ll explore that in this episode.</itunes:summary>
      <itunes:subtitle>In the first of a few episodes on fraud in electi…</itunes:subtitle>
      <description>In the first of a few episodes on fraud in election research, we’ll take a look at a case study from a previous Presidential election, where polling results were faked.

What are some telltale signs that data fraud might be present in a dataset?  We’ll explore that in this episode.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/-iNOSIVdA4w" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/xfXVOFEom9M/208330505-linear-digressions-falsifying-data.mp3" fileSize="25600451" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/falsifying-data</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/xfXVOFEom9M/208330505-linear-digressions-falsifying-data.mp3" length="25600451" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/208330505-linear-digressions-falsifying-data.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/206453182</guid>
      <title>Reporter Bot</title>
      <pubDate>Wed, 20 May 2015 23:16:18 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/oDlyOIo2eiE/reporter-bot</link>
      <itunes:duration>00:11:15</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>There’s a big difference between a table of numbers or statistics, and the underlying story that a human might tell about how those numbers were generated.

Think about a baseball game—the game stats and a newspaper story are describing the same thing, but one is a good input for a machine learning algorithm and the other is a good story to read over your morning coffee. Data science and machine learning are starting to bridge this gap, taking the raw data on things like baseball games, financial scenarios, etc. and automatically writing human-readable stories that are increasingly indistinguishable from what a human would write.

In this episode, we’ll talk about some examples of auto-generated content—you’ll be amazed at how sophisticated some of these reporter-bots can be. By the way, this summary was written by a human. (Or was it?)</itunes:summary>
      <itunes:subtitle>There’s a big difference between a table of numbe…</itunes:subtitle>
      <description>There’s a big difference between a table of numbers or statistics, and the underlying story that a human might tell about how those numbers were generated.

Think about a baseball game—the game stats and a newspaper story are describing the same thing, but one is a good input for a machine learning algorithm and the other is a good story to read over your morning coffee. Data science and machine learning are starting to bridge this gap, taking the raw data on things like baseball games, financial scenarios, etc. and automatically writing human-readable stories that are increasingly indistinguishable from what a human would write.

In this episode, we’ll talk about some examples of auto-generated content—you’ll be amazed at how sophisticated some of these reporter-bots can be. By the way, this summary was written by a human. (Or was it?)&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/oDlyOIo2eiE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/0o9ToZ5Nf1Y/206453182-linear-digressions-reporter-bot.mp3" fileSize="16220193" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/reporter-bot</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/0o9ToZ5Nf1Y/206453182-linear-digressions-reporter-bot.mp3" length="16220193" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/206453182-linear-digressions-reporter-bot.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/205711835</guid>
      <title>Careers in Data Science</title>
      <pubDate>Sat, 16 May 2015 05:43:44 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/_eMkV57uNU0/careers-in-data-science</link>
      <itunes:duration>00:16:35</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Let’s talk money. As a “hot” career right now, data science can pay pretty well. But for an individual person matched with a specific job or industry, how much should someone expect to make?

Since Katie was on the job market lately, this was something she’s been researching, and it turns out that data science itself (in particular linear regressions) has some answers.

In this episode, we go through a survey of hundreds of data scientists, who report on their job duties, industry, skills, education, location, etc. along with their salaries, and then talk about how this data was fed into a linear regression so that you (yes, you!) can use the patterns in the data to know what kind of salary any particular kind of data scientist might expect.</itunes:summary>
      <itunes:subtitle>Let’s talk money. As a “hot” career right now, da…</itunes:subtitle>
      <description>Let’s talk money. As a “hot” career right now, data science can pay pretty well. But for an individual person matched with a specific job or industry, how much should someone expect to make?

Since Katie was on the job market lately, this was something she’s been researching, and it turns out that data science itself (in particular linear regressions) has some answers.

In this episode, we go through a survey of hundreds of data scientists, who report on their job duties, industry, skills, education, location, etc. along with their salaries, and then talk about how this data was fed into a linear regression so that you (yes, you!) can use the patterns in the data to know what kind of salary any particular kind of data scientist might expect.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/_eMkV57uNU0" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3wPyBWfopqU/205711835-linear-digressions-careers-in-data-science.mp3" fileSize="23897685" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/careers-in-data-science</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3wPyBWfopqU/205711835-linear-digressions-careers-in-data-science.mp3" length="23897685" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/205711835-linear-digressions-careers-in-data-science.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/205472927</guid>
      <title>That's "Dr Katie" to You</title>
      <pubDate>Thu, 14 May 2015 17:37:48 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/wpMZ8LrKJfs/thats-dr-katie-to-you</link>
      <itunes:duration>00:03:01</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Katie successfully defended her thesis! We celebrate her return, and talk a bit about what getting a PhD in Physics is like.</itunes:summary>
      <itunes:subtitle>Katie successfully defended her thesis! We celebr…</itunes:subtitle>
      <description>Katie successfully defended her thesis! We celebrate her return, and talk a bit about what getting a PhD in Physics is like.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/wpMZ8LrKJfs" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/PwGBk6x7Ihw/205472927-linear-digressions-thats-dr-katie-to-you.mp3" fileSize="4356630" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/thats-dr-katie-to-you</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/PwGBk6x7Ihw/205472927-linear-digressions-thats-dr-katie-to-you.mp3" length="4356630" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/205472927-linear-digressions-thats-dr-katie-to-you.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/204955129</guid>
      <title>Neural Nets (Part 2)</title>
      <pubDate>Mon, 11 May 2015 14:37:51 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/QDNRmJ9s0DY/neural-nets-part-2</link>
      <itunes:duration>00:10:55</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In the last episode, we zipped through neural nets and got a quick idea of how they work and why they can be so powerful. Here’s the real payoff of that work:

In this episode, we’ll talk about a brand-new pair of results, one from Stanford and one from Google, that use neural nets to perform automated picture captioning. One neural net does the object and relationship recognition of the image, a second neural net handles the natural language processing required to express that in an English sentence, and when you put them together you get an automated captioning tool. Two heads are better than one indeed...</itunes:summary>
      <itunes:subtitle>In the last episode, we zipped through neural net…</itunes:subtitle>
      <description>In the last episode, we zipped through neural nets and got a quick idea of how they work and why they can be so powerful. Here’s the real payoff of that work:

In this episode, we’ll talk about a brand-new pair of results, one from Stanford and one from Google, that use neural nets to perform automated picture captioning. One neural net does the object and relationship recognition of the image, a second neural net handles the natural language processing required to express that in an English sentence, and when you put them together you get an automated captioning tool. Two heads are better than one indeed...&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/QDNRmJ9s0DY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kJ2wtftpj24/204955129-linear-digressions-neural-nets-part-2.mp3" fileSize="15734315" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/neural-nets-part-2</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/kJ2wtftpj24/204955129-linear-digressions-neural-nets-part-2.mp3" length="15734315" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/204955129-linear-digressions-neural-nets-part-2.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/203476719</guid>
      <title>Neural Nets (Part 1)</title>
      <pubDate>Fri, 01 May 2015 18:59:28 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/roHkCfukV0g/neural-nets-part-1</link>
      <itunes:duration>00:09:00</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>There is no known learning algorithm that is more flexible and powerful than the human brain. That's quite inspirational, if you think about it--to level up machine learning, maybe we should be going back to biology and letting millions of year of evolution guide the structure of our algorithms.

This is the idea behind neural nets, which mock up the structure of the brain and are some of the most studied and powerful algorithms out there. In this episode, we’ll lay out the building blocks of the neural net (called neurons, naturally) and the networks that are built out of them.

We’ll also explore the results that neural nets get when used to do object recognition in photographs.</itunes:summary>
      <itunes:subtitle>There is no known learning algorithm that is more…</itunes:subtitle>
      <description>There is no known learning algorithm that is more flexible and powerful than the human brain. That's quite inspirational, if you think about it--to level up machine learning, maybe we should be going back to biology and letting millions of year of evolution guide the structure of our algorithms.

This is the idea behind neural nets, which mock up the structure of the brain and are some of the most studied and powerful algorithms out there. In this episode, we’ll lay out the building blocks of the neural net (called neurons, naturally) and the networks that are built out of them.

We’ll also explore the results that neural nets get when used to do object recognition in photographs.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/roHkCfukV0g" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/jaQjX9s4C_o/203476719-linear-digressions-neural-nets-part-1.mp3" fileSize="12968261" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/neural-nets-part-1</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/jaQjX9s4C_o/203476719-linear-digressions-neural-nets-part-1.mp3" length="12968261" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/203476719-linear-digressions-neural-nets-part-1.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/202973263</guid>
      <title>Inferring Authorship (Part 2)</title>
      <pubDate>Tue, 28 Apr 2015 16:56:24 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/wTM0gty1aTk/inferring-authorship-part-2</link>
      <itunes:duration>00:14:04</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Now that we’re up to speed on the classic author ID problem (who wrote the unsigned Federalist Papers?), we move onto a couple more contemporary examples.

First, J.K. Rowling was famously outed using computational linguistics (and Twitter) when she wrote a book under the pseudonym Robert Galbraith.

Second, we’ll talk about a mystery that still endures--who is Satoshi Nakamoto? Satoshi is the mysterious person (or people) behind an extremely lucrative cryptocurrency (aka internet money) called Bitcoin; no one knows who he, she or they are, but we have plenty of writing samples in the form of whitepapers and Bitcoin forum posts. We’ll discuss some attempts to link Satoshi Nakamoto with a cryptocurrency expert and computer scientist named Nick Szabo; the links are tantalizing, but not a smoking gun. “Who is Satoshi” remains an example of attempted author identification where the threads are tangled, the conclusions inconclusive and the stakes high.</itunes:summary>
      <itunes:subtitle>Now that we’re up to speed on the classic author …</itunes:subtitle>
      <description>Now that we’re up to speed on the classic author ID problem (who wrote the unsigned Federalist Papers?), we move onto a couple more contemporary examples.

First, J.K. Rowling was famously outed using computational linguistics (and Twitter) when she wrote a book under the pseudonym Robert Galbraith.

Second, we’ll talk about a mystery that still endures--who is Satoshi Nakamoto? Satoshi is the mysterious person (or people) behind an extremely lucrative cryptocurrency (aka internet money) called Bitcoin; no one knows who he, she or they are, but we have plenty of writing samples in the form of whitepapers and Bitcoin forum posts. We’ll discuss some attempts to link Satoshi Nakamoto with a cryptocurrency expert and computer scientist named Nick Szabo; the links are tantalizing, but not a smoking gun. “Who is Satoshi” remains an example of attempted author identification where the threads are tangled, the conclusions inconclusive and the stakes high.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/wTM0gty1aTk" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Clia9-X_0Yk/202973263-linear-digressions-inferring-authorship-part-2.mp3" fileSize="20271471" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/inferring-authorship-part-2</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Clia9-X_0Yk/202973263-linear-digressions-inferring-authorship-part-2.mp3" length="20271471" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/202973263-linear-digressions-inferring-authorship-part-2.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/201113470</guid>
      <title>Inferring Authorship (Part 1)</title>
      <pubDate>Thu, 16 Apr 2015 17:25:21 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/Qet9zM5sTKI/authorship-part-1</link>
      <itunes:duration>00:08:51</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This episode is inspired by one of our projects for Intro to Machine Learning: given a writing sample, can you use machine learning to identify who wrote it? Turns out that the answer is yes, a person’s writing style is as distinctive as their vocal inflection or their gait when they walk.

By tracing the vocabulary used in a given piece, and comparing the word choices to the word choices in writing samples where we know the author, it can be surprisingly clear who is the more likely author of a given piece of text.

We’ll use a seminal paper from the 1960’s as our example here, where the Naive Bayes algorithm was used to determine whether Alexander Hamilton or James Madison was the more likely author of a number of anonymous Federalist Papers.</itunes:summary>
      <itunes:subtitle>This episode is inspired by one of our projects f…</itunes:subtitle>
      <description>This episode is inspired by one of our projects for Intro to Machine Learning: given a writing sample, can you use machine learning to identify who wrote it? Turns out that the answer is yes, a person’s writing style is as distinctive as their vocal inflection or their gait when they walk.

By tracing the vocabulary used in a given piece, and comparing the word choices to the word choices in writing samples where we know the author, it can be surprisingly clear who is the more likely author of a given piece of text.

We’ll use a seminal paper from the 1960’s as our example here, where the Naive Bayes algorithm was used to determine whether Alexander Hamilton or James Madison was the more likely author of a number of anonymous Federalist Papers.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/Qet9zM5sTKI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/H664J7sQZy0/201113470-linear-digressions-authorship-part-1.mp3" fileSize="12747579" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/authorship-part-1</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/H664J7sQZy0/201113470-linear-digressions-authorship-part-1.mp3" length="12747579" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/201113470-linear-digressions-authorship-part-1.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/199550270</guid>
      <title>Statistical Mistakes and the Challenger Disaster</title>
      <pubDate>Mon, 06 Apr 2015 19:36:56 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/vs_LhWZxIZI/statistical-mistakes-and-the-challenger-disaster</link>
      <itunes:duration>00:13:09</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>After the Challenger exploded in 1986, killing all 7 astronauts aboard, an investigation into the cause was immediately launched.

In the cold temperatures the night before the launch, the o-rings that seal off the fuel tanks from the rocket boosters became inflexible, so they did not seal properly, which led to the fuel tank explosion. NASA knew that there could be o-ring problems, but performed the analysis of their data incorrectly and ended up massively underestimating the risk associated with the cold temperatures.

In this episode, we'll unpack the mistakes they made. We'll talk about how they excluded data points that they thought were irrelevant but which actually were critical to recognizing a fatal pattern.</itunes:summary>
      <itunes:subtitle>After the Challenger exploded in 1986, killing al…</itunes:subtitle>
      <description>After the Challenger exploded in 1986, killing all 7 astronauts aboard, an investigation into the cause was immediately launched.

In the cold temperatures the night before the launch, the o-rings that seal off the fuel tanks from the rocket boosters became inflexible, so they did not seal properly, which led to the fuel tank explosion. NASA knew that there could be o-ring problems, but performed the analysis of their data incorrectly and ended up massively underestimating the risk associated with the cold temperatures.

In this episode, we'll unpack the mistakes they made. We'll talk about how they excluded data points that they thought were irrelevant but which actually were critical to recognizing a fatal pattern.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/vs_LhWZxIZI" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/rv8HjB-iqyI/199550270-linear-digressions-statistical-mistakes-and-the-challenger-disaster.mp3" fileSize="18947376" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/statistical-mistakes-and-the-challenger-disaster</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/rv8HjB-iqyI/199550270-linear-digressions-statistical-mistakes-and-the-challenger-disaster.mp3" length="18947376" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/199550270-linear-digressions-statistical-mistakes-and-the-challenger-disaster.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/197641507</guid>
      <title>Genetics and Um Detection (HMM Part 2)</title>
      <pubDate>Wed, 25 Mar 2015 17:29:32 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ytD0gjtAeZs/genetics-and-um-detection-part-2</link>
      <itunes:duration>00:14:49</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In part two of our series on Hidden Markov Models (HMMs), we talk to Katie and special guest Francesco about more useful and novel applications of HMMs. We revisit Katie's "Um Detector," and hear about how HMMs are used in genetics research.</itunes:summary>
      <itunes:subtitle>In part two of our series on Hidden Markov Models…</itunes:subtitle>
      <description>In part two of our series on Hidden Markov Models (HMMs), we talk to Katie and special guest Francesco about more useful and novel applications of HMMs. We revisit Katie's "Um Detector," and hear about how HMMs are used in genetics research.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ytD0gjtAeZs" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/WbcweZGWDLw/197641507-linear-digressions-genetics-and-um-detection-part-2.mp3" fileSize="21344163" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/genetics-and-um-detection-part-2</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/WbcweZGWDLw/197641507-linear-digressions-genetics-and-um-detection-part-2.mp3" length="21344163" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/197641507-linear-digressions-genetics-and-um-detection-part-2.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/197459594</guid>
      <title>Introducing Hidden Markov Models (HMM Part 1)</title>
      <pubDate>Tue, 24 Mar 2015 15:57:03 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/CPA0QGd2OyQ/introducing-hidden-markov-models</link>
      <itunes:duration>00:14:54</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Wikipedia says, "A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states." What does that even mean?

In part one of a special two-parter on HMMs, Katie, Ben, and special guest Francesco explain the basics of HMMs, and some simple applications of them in the real world. This episode sets the stage for part two, where we explore the use of HMMs in Modern Genetics, and possibly Katie's "Um Detector."</itunes:summary>
      <itunes:subtitle>Wikipedia says, "A hidden Markov model (HMM) is a…</itunes:subtitle>
      <description>Wikipedia says, "A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states." What does that even mean?

In part one of a special two-parter on HMMs, Katie, Ben, and special guest Francesco explain the basics of HMMs, and some simple applications of them in the real world. This episode sets the stage for part two, where we explore the use of HMMs in Modern Genetics, and possibly Katie's "Um Detector."&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/CPA0QGd2OyQ" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/MgmkDou3bMw/197459594-linear-digressions-introducing-hidden-markov-models.mp3" fileSize="21472059" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/introducing-hidden-markov-models</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/MgmkDou3bMw/197459594-linear-digressions-introducing-hidden-markov-models.mp3" length="21472059" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/197459594-linear-digressions-introducing-hidden-markov-models.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/195590647</guid>
      <title>Monte Carlo For Physicists</title>
      <pubDate>Thu, 12 Mar 2015 23:18:01 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/-vzGIkNX_N8/monte-carlo-for-physicists</link>
      <itunes:duration>00:08:13</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>This is another physics-centered podcast, about an ML-backed particle identification tool that we use to figure out what kind of particle caused a particular blob in the detector. But in this case, as in many cases, it looks hard at the outset to use ML because we don't have labeled training data. Monte Carlo to the rescue!

Monte Carlo (MC) is fake data that we generate for ourselves, usually following certain sets of rules (often a Markov chain; in physics we generate MC according to the laws of physics as we understand them) and since you generated the event, you "know" what the correct label is.

Of course, it's a lot of work to validate your MC, but the payoff is that then you can use Machine Learning where you never could before.</itunes:summary>
      <itunes:subtitle>This is another physics-centered podcast, about a…</itunes:subtitle>
      <description>This is another physics-centered podcast, about an ML-backed particle identification tool that we use to figure out what kind of particle caused a particular blob in the detector. But in this case, as in many cases, it looks hard at the outset to use ML because we don't have labeled training data. Monte Carlo to the rescue!

Monte Carlo (MC) is fake data that we generate for ourselves, usually following certain sets of rules (often a Markov chain; in physics we generate MC according to the laws of physics as we understand them) and since you generated the event, you "know" what the correct label is.

Of course, it's a lot of work to validate your MC, but the payoff is that then you can use Machine Learning where you never could before.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/-vzGIkNX_N8" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/q3y_rz5iRCg/195590647-linear-digressions-monte-carlo-for-physicists.mp3" fileSize="11842279" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/monte-carlo-for-physicists</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/q3y_rz5iRCg/195590647-linear-digressions-monte-carlo-for-physicists.mp3" length="11842279" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/195590647-linear-digressions-monte-carlo-for-physicists.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/194262842</guid>
      <title>Random Kanye</title>
      <pubDate>Wed, 04 Mar 2015 23:04:45 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/LVZxnTFxfbw/random-kanye</link>
      <itunes:duration>00:08:44</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Ever feel like you could randomly assemble words from a certain vocabulary and make semi-coherent Kanye West lyrics? Or technical documentation, imitations of local newscasters, your politically outspoken uncle, etc.? Wonder no more, there's a way to do this exact type of thing: it's called a Markov Chain, and probably the most powerful way to generate made-up data that you can then use for fun and profit. The idea behind a Markov Chain is that you probabilistically generate a sequence of steps, numbers, words, etc. where each next step/number/word depends only on the previous one, which makes it fast and efficient to computationally generate. Usually Markov Chains are used for serious academic uses, but this ain't one of them: here they're used to randomly generate rap lyrics based on Kanye West lyrics.</itunes:summary>
      <itunes:subtitle>Ever feel like you could randomly assemble words …</itunes:subtitle>
      <description>Ever feel like you could randomly assemble words from a certain vocabulary and make semi-coherent Kanye West lyrics? Or technical documentation, imitations of local newscasters, your politically outspoken uncle, etc.? Wonder no more, there's a way to do this exact type of thing: it's called a Markov Chain, and probably the most powerful way to generate made-up data that you can then use for fun and profit. The idea behind a Markov Chain is that you probabilistically generate a sequence of steps, numbers, words, etc. where each next step/number/word depends only on the previous one, which makes it fast and efficient to computationally generate. Usually Markov Chains are used for serious academic uses, but this ain't one of them: here they're used to randomly generate rap lyrics based on Kanye West lyrics.&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/LVZxnTFxfbw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/QUz4Z9ulJGw/194262842-linear-digressions-random-kanye.mp3" fileSize="12581440" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/random-kanye</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/QUz4Z9ulJGw/194262842-linear-digressions-random-kanye.mp3" length="12581440" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/194262842-linear-digressions-random-kanye.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/192998182</guid>
      <title>Lie Detectors</title>
      <pubDate>Wed, 25 Feb 2015 18:20:51 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/u346lY00Y50/lie-detectors</link>
      <itunes:duration>00:09:17</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Often machine learning discussions center around algorithms, or features, or datasets--this one centers around interpretation, and ethics.

Suppose you could use a technology like fMRI to see what regions of a person's brain are active when they ask questions. And also suppose that you could run trials where you watch their brain activity while they lie about some minor issue (say, whether the card in their hand is a spade or a club)--could you use machine learning to analyze those images, and use the patterns in them for lie detection? Well you certainly can try, and indeed researchers have done just that.

There are important problems though--the images of brains can be high variance, meaning that for any given person, there might not be a lot of certainty about whether they're lying or not. It's also open to debate whether the training set (in this case, test subjects with playing cards in their hands) really generalize well to the more important cases, like a person accused of a crime.

So while machine learning has yielded some impressive gains in lie detection, it is not a solution to these thornier scientific issues.

http://www.amacad.org/pdfs/deceit.pdf</itunes:summary>
      <itunes:subtitle>Often machine learning discussions center around …</itunes:subtitle>
      <description>Often machine learning discussions center around algorithms, or features, or datasets--this one centers around interpretation, and ethics.

Suppose you could use a technology like fMRI to see what regions of a person's brain are active when they ask questions. And also suppose that you could run trials where you watch their brain activity while they lie about some minor issue (say, whether the card in their hand is a spade or a club)--could you use machine learning to analyze those images, and use the patterns in them for lie detection? Well you certainly can try, and indeed researchers have done just that.

There are important problems though--the images of brains can be high variance, meaning that for any given person, there might not be a lot of certainty about whether they're lying or not. It's also open to debate whether the training set (in this case, test subjects with playing cards in their hands) really generalize well to the more important cases, like a person accused of a crime.

So while machine learning has yielded some impressive gains in lie detection, it is not a solution to these thornier scientific issues.

http://www.amacad.org/pdfs/deceit.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/u346lY00Y50" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/cbYozwIvEng/192998182-linear-digressions-lie-detectors.mp3" fileSize="13385175" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/lie-detectors</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/cbYozwIvEng/192998182-linear-digressions-lie-detectors.mp3" length="13385175" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/192998182-linear-digressions-lie-detectors.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/191672068</guid>
      <title>The Enron Dataset</title>
      <pubDate>Mon, 09 Feb 2015 00:00:00 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ROzlCJspAZE/the-enron-dataset</link>
      <itunes:duration>00:12:27</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In 2000, Enron was one of the largest and companies in the world, praised far and wide for its innovations in energy distribution and many other markets.  By 2002, it was apparent that many bad apples had been cooking the books, and billions of dollars and thousands of jobs disappeared.

In the aftermath, surprisingly, one of the greatest datasets in all of machine learning was born--the Enron emails corpus.  Hundreds of thousands of emails amongst top executives were made public; there's no realistic chance any dataset like this will ever be made public again.

But the dataset that was released has gone on to immortality, serving as the basis for a huge variety of advances in machine learning and other fields.

http://www.technologyreview.com/news/515801/the-immortal-life-of-the-enron-e-mails/</itunes:summary>
      <itunes:subtitle>In 2000, Enron was one of the largest and compani…</itunes:subtitle>
      <description>In 2000, Enron was one of the largest and companies in the world, praised far and wide for its innovations in energy distribution and many other markets.  By 2002, it was apparent that many bad apples had been cooking the books, and billions of dollars and thousands of jobs disappeared.

In the aftermath, surprisingly, one of the greatest datasets in all of machine learning was born--the Enron emails corpus.  Hundreds of thousands of emails amongst top executives were made public; there's no realistic chance any dataset like this will ever be made public again.

But the dataset that was released has gone on to immortality, serving as the basis for a huge variety of advances in machine learning and other fields.

http://www.technologyreview.com/news/515801/the-immortal-life-of-the-enron-e-mails/&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ROzlCJspAZE" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Jk8TfVI9bG4/191672068-linear-digressions-the-enron-dataset.mp3" fileSize="17944901" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/the-enron-dataset</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/Jk8TfVI9bG4/191672068-linear-digressions-the-enron-dataset.mp3" length="17944901" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/191672068-linear-digressions-the-enron-dataset.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/189385510</guid>
      <title>Labels and Where To Find Them</title>
      <pubDate>Wed, 04 Feb 2015 02:30:47 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/QHmvet5F3Y0/labels-and-where-to-find-them</link>
      <itunes:duration>00:13:15</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Supervised classification is built on the backs of labeled datasets, but a good set of labels can be hard to find.  Great data is everywhere, but the corresponding labels can sometimes be really tricky.  Take a few examples we've already covered, like lie detection with an MRI machine (have to take pictures of someone's brain while they try to lie, not a trivial task) or automated image captioning (so many images!  so many valid labels!)

In this epsiode, we'll dig into this topic in depth, talking about some of the standard ways to get a labeled dataset if your project requires labels and you don't already have them.

www.higgshunters.org</itunes:summary>
      <itunes:subtitle>Supervised classification is built on the backs o…</itunes:subtitle>
      <description>Supervised classification is built on the backs of labeled datasets, but a good set of labels can be hard to find.  Great data is everywhere, but the corresponding labels can sometimes be really tricky.  Take a few examples we've already covered, like lie detection with an MRI machine (have to take pictures of someone's brain while they try to lie, not a trivial task) or automated image captioning (so many images!  so many valid labels!)

In this epsiode, we'll dig into this topic in depth, talking about some of the standard ways to get a labeled dataset if your project requires labels and you don't already have them.

www.higgshunters.org&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/QHmvet5F3Y0" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3_dyWTow8eY/189385510-linear-digressions-labels-and-where-to-find-them.mp3" fileSize="19099095" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/labels-and-where-to-find-them</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/3_dyWTow8eY/189385510-linear-digressions-labels-and-where-to-find-them.mp3" length="19099095" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/189385510-linear-digressions-labels-and-where-to-find-them.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/187530821</guid>
      <title>Um Detector 1</title>
      <pubDate>Fri, 23 Jan 2015 20:16:12 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/bxEDSUl6xAo/um-detector-1</link>
      <itunes:duration>00:13:19</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>So, um... what about machine learning for audio applications?  In the course of starting this podcast, we've edited out a lot of "um"'s from our raw audio files.  It's gotten now to the point that, when we see the waveform in soundstudio, we can almost identify an "um" by eye.  Which makes it an interesting problem for machine learning--is there a way we can train an algorithm to recognize the "um" pattern, too?  This has become a little side project for Katie, which is very much still a work in progress.  We'll talk about what's been accomplished so far, some design choices Katie made in getting the project off the ground, and (of course) mistakes made and hopefully corrected.  We always say that the best way to learn something is by doing it, and this is our chance to try our own machine learning project instead of just telling you about what someone else did! </itunes:summary>
      <itunes:subtitle>So, um... what about machine learning for audio a…</itunes:subtitle>
      <description>So, um... what about machine learning for audio applications?  In the course of starting this podcast, we've edited out a lot of "um"'s from our raw audio files.  It's gotten now to the point that, when we see the waveform in soundstudio, we can almost identify an "um" by eye.  Which makes it an interesting problem for machine learning--is there a way we can train an algorithm to recognize the "um" pattern, too?  This has become a little side project for Katie, which is very much still a work in progress.  We'll talk about what's been accomplished so far, some design choices Katie made in getting the project off the ground, and (of course) mistakes made and hopefully corrected.  We always say that the best way to learn something is by doing it, and this is our chance to try our own machine learning project instead of just telling you about what someone else did!&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/bxEDSUl6xAo" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TiJNKbyUZ9Y/187530821-linear-digressions-um-detector-1.mp3" fileSize="19189375" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/um-detector-1</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/TiJNKbyUZ9Y/187530821-linear-digressions-um-detector-1.mp3" length="19189375" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/187530821-linear-digressions-um-detector-1.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/184829398</guid>
      <title>Better Facial Recognition with Fisherfaces</title>
      <pubDate>Wed, 07 Jan 2015 01:33:50 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/tUno_BlsjDA/better-facial-recognition-with-fisherfaces</link>
      <itunes:duration>00:11:56</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Now that we know about eigenfaces (if you don't, listen to the previous episode), let's talk about how it breaks down.

Variations that are trivial to humans when identifying faces can really mess up computer-driven facial ID--expressions, lighting, and angle are a few. Something that can easily happen is an algorithm can optimize to identify one of those traits, rather than the underlying trait of whether the person is the same (for example, if the training image is me smiling, you may reject an image of me frowning but accidentally approve an image of another woman smiling).

Fisherfaces uses a fisher linear discriminant to distinguish based on the dimension in the data that shows the smallest inter-class distance, rather than maximizing the variation overall (we'll unpack this statement), and it is much more robust than our pal eigenfaces when there's shadows, cut-off images, expressions, etc.

http://www.cs.columbia.edu/~belhumeur/journal/fisherface-pami97.pdf</itunes:summary>
      <itunes:subtitle>Now that we know about eigenfaces (if you don't, …</itunes:subtitle>
      <description>Now that we know about eigenfaces (if you don't, listen to the previous episode), let's talk about how it breaks down.

Variations that are trivial to humans when identifying faces can really mess up computer-driven facial ID--expressions, lighting, and angle are a few. Something that can easily happen is an algorithm can optimize to identify one of those traits, rather than the underlying trait of whether the person is the same (for example, if the training image is me smiling, you may reject an image of me frowning but accidentally approve an image of another woman smiling).

Fisherfaces uses a fisher linear discriminant to distinguish based on the dimension in the data that shows the smallest inter-class distance, rather than maximizing the variation overall (we'll unpack this statement), and it is much more robust than our pal eigenfaces when there's shadows, cut-off images, expressions, etc.

http://www.cs.columbia.edu/~belhumeur/journal/fisherface-pami97.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/tUno_BlsjDA" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/z9djZc299rA/184829398-linear-digressions-better-facial-recognition-with-fisherfaces.mp3" fileSize="17193828" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/better-facial-recognition-with-fisherfaces</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/z9djZc299rA/184829398-linear-digressions-better-facial-recognition-with-fisherfaces.mp3" length="17193828" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/184829398-linear-digressions-better-facial-recognition-with-fisherfaces.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/184829050</guid>
      <title>Facial Recognition with Eigenfaces</title>
      <pubDate>Wed, 07 Jan 2015 01:30:40 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/tzsqQyMok3M/facial-recognition-with-eigenfaces</link>
      <itunes:duration>00:10:01</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>A true classic topic in ML: Facial recognition is very high-dimensional, meaning that each picture can have millions of pixels, each of which can be a single feature. It's computationally expensive to deal with all these features, and invites overfitting problems. PCA (principal components analysis) is a classic dimensionality reduction tool that compresses these many dimensions into the few that contain the most variation in the data, and those principal components are often then fed into a classic ML algorithm like and SVM.

One of the best thing about eigenfaces is the great example code that you can find in sklearn--you can distinguish pictures of world leaders yourself in just a few minutes!

http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html</itunes:summary>
      <itunes:subtitle>A true classic topic in ML: Facial recognition is…</itunes:subtitle>
      <description>A true classic topic in ML: Facial recognition is very high-dimensional, meaning that each picture can have millions of pixels, each of which can be a single feature. It's computationally expensive to deal with all these features, and invites overfitting problems. PCA (principal components analysis) is a classic dimensionality reduction tool that compresses these many dimensions into the few that contain the most variation in the data, and those principal components are often then fed into a classic ML algorithm like and SVM.

One of the best thing about eigenfaces is the great example code that you can find in sklearn--you can distinguish pictures of world leaders yourself in just a few minutes!

http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/tzsqQyMok3M" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/spu109NPjh4/184829050-linear-digressions-facial-recognition-with-eigenfaces.mp3" fileSize="14435298" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/facial-recognition-with-eigenfaces</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/spu109NPjh4/184829050-linear-digressions-facial-recognition-with-eigenfaces.mp3" length="14435298" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/184829050-linear-digressions-facial-recognition-with-eigenfaces.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/181859732</guid>
      <title>Stats of World Series Streaks</title>
      <pubDate>Wed, 17 Dec 2014 00:41:39 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/EfjByRZl8Mk/stats-of-world-series-streaks</link>
      <itunes:duration>00:12:34</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Baseball is characterized by a high level of equality between teams; even the best teams might only have 55% win percentages (contrast this with college football, where teams go undefeated pretty regularly). In this regime, where 2 outcomes (Giants win/Giants lose) are approximately equally likely, we can model the win/loss chances with a binomial distribution.

Using the binomial distribution, we can calculate an interesting little result: what's the chance of the world series going to only 4 games? 5? 6? All the way to 7? Then we can compare to decades' worth of world series data, to see how well the data follows the binomial assumption.

The result tells us a lot about sports psychology--if each game is independent of the others, 4/5/6/7 game series are equally likely. The data shows a different trend: 4 and 7 game series are significantly more likely than 5 or 6. There's a powerful psychological effect at play--everybody loves the 7th game of the world series, or a good sweep. And it turns out that the baseball teams, whether they intend it or not, oblige our love of short (4) and long (7) world series!

http://blog.philbirnbaum.com/2007/06/winning-world-series-in-x-games.html</itunes:summary>
      <itunes:subtitle>Baseball is characterized by a high level of equa…</itunes:subtitle>
      <description>Baseball is characterized by a high level of equality between teams; even the best teams might only have 55% win percentages (contrast this with college football, where teams go undefeated pretty regularly). In this regime, where 2 outcomes (Giants win/Giants lose) are approximately equally likely, we can model the win/loss chances with a binomial distribution.

Using the binomial distribution, we can calculate an interesting little result: what's the chance of the world series going to only 4 games? 5? 6? All the way to 7? Then we can compare to decades' worth of world series data, to see how well the data follows the binomial assumption.

The result tells us a lot about sports psychology--if each game is independent of the others, 4/5/6/7 game series are equally likely. The data shows a different trend: 4 and 7 game series are significantly more likely than 5 or 6. There's a powerful psychological effect at play--everybody loves the 7th game of the world series, or a good sweep. And it turns out that the baseball teams, whether they intend it or not, oblige our love of short (4) and long (7) world series!

http://blog.philbirnbaum.com/2007/06/winning-world-series-in-x-games.html&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/EfjByRZl8Mk" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/p2XTNR-8cpM/181859732-linear-digressions-stats-of-world-series-streaks.mp3" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/stats-of-world-series-streaks</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/p2XTNR-8cpM/181859732-linear-digressions-stats-of-world-series-streaks.mp3" length="0" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/181859732-linear-digressions-stats-of-world-series-streaks.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/178774107</guid>
      <title>Computers Try to Tell Jokes</title>
      <pubDate>Wed, 26 Nov 2014 18:59:56 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/ZHqEJdcIc6s/linear-digressions-computers-try-to-tell-jokes</link>
      <itunes:duration>00:09:08</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Computers are capable of many impressive feats, but making you laugh is usually not one of them. Or could it be? This episode will talk about a custom-built machine learning algorithm that searches through text and writes jokes based on what it finds.

The jokes are formulaic: they're all of the form "I like my X like I like my Y: Z" where X and Y are nouns, and Z is an adjective that can describe both X and Y. For (dumb) example, "I like my men like I like my coffee: steaming hot." The joke is funny when ZX and ZY are both very common phrases, but X and Y are rarely seen together.

So, given a large enough corpus of text, the algorithm looks for triplets of words that fit this description and writes jokes based on them. Are the jokes funny? You be the judge...

http://homepages.inf.ed.ac.uk/s0894589/petrovic13unsupervised.pdf</itunes:summary>
      <itunes:subtitle>Computers are capable of many impressive feats, b…</itunes:subtitle>
      <description>Computers are capable of many impressive feats, but making you laugh is usually not one of them. Or could it be? This episode will talk about a custom-built machine learning algorithm that searches through text and writes jokes based on what it finds.

The jokes are formulaic: they're all of the form "I like my X like I like my Y: Z" where X and Y are nouns, and Z is an adjective that can describe both X and Y. For (dumb) example, "I like my men like I like my coffee: steaming hot." The joke is funny when ZX and ZY are both very common phrases, but X and Y are rarely seen together.

So, given a large enough corpus of text, the algorithm looks for triplets of words that fit this description and writes jokes based on them. Are the jokes funny? You be the judge...

http://homepages.inf.ed.ac.uk/s0894589/petrovic13unsupervised.pdf&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/ZHqEJdcIc6s" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/avatars-000193869760-sy993g-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/g1w2P1myRYA/178774107-linear-digressions-linear-digressions-computers-try-to-tell-jokes.mp3" fileSize="13173270" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/linear-digressions-computers-try-to-tell-jokes</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/g1w2P1myRYA/178774107-linear-digressions-linear-digressions-computers-try-to-tell-jokes.mp3" length="13173270" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/178774107-linear-digressions-linear-digressions-computers-try-to-tell-jokes.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/173279159</guid>
      <title>How Outliers Helped Defeat Cholera</title>
      <pubDate>Sat, 22 Nov 2014 00:00:00 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/1pCS5GZUVDY/linear-digressions-choleric-outliers</link>
      <itunes:duration>00:10:54</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>In the 1850s, there were a lot of things we didn’t know yet: how to create an airplane, how to split an atom, or how to control the spread of a common but deadly disease: cholera.

When a cholera outbreak in London killed scores of people, a doctor named John Snow used it as a chance to study whether the cause might be very small organisms that were spreading through the water supply (the prevailing theory at the time was miasma, or “bad air”).  By tracing the geography of all the deaths from the outbreak, Snow was practicing elementary data science--and stumbled upon one of history’s most famous outliers.

In this episode, we’ll tell you more about this single data point, a case of cholera that cracked the case wide open for Snow and provided critical validation for the germ theory of disease.

http://en.wikipedia.org/wiki/1854_Broad_Street_cholera_outbreak</itunes:summary>
      <itunes:subtitle>In the 1850s, there were a lot of things we didn’…</itunes:subtitle>
      <description>In the 1850s, there were a lot of things we didn’t know yet: how to create an airplane, how to split an atom, or how to control the spread of a common but deadly disease: cholera.

When a cholera outbreak in London killed scores of people, a doctor named John Snow used it as a chance to study whether the cause might be very small organisms that were spreading through the water supply (the prevailing theory at the time was miasma, or “bad air”).  By tracing the geography of all the deaths from the outbreak, Snow was practicing elementary data science--and stumbled upon one of history’s most famous outliers.

In this episode, we’ll tell you more about this single data point, a case of cholera that cracked the case wide open for Snow and provided critical validation for the germ theory of disease.

http://en.wikipedia.org/wiki/1854_Broad_Street_cholera_outbreak&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/1pCS5GZUVDY" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/artworks-000094738684-8zjfvu-original.gif" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/VHsO25rRoQA/173279159-linear-digressions-linear-digressions-choleric-outliers.mp3" fileSize="15708611" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/linear-digressions-choleric-outliers</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/VHsO25rRoQA/173279159-linear-digressions-linear-digressions-choleric-outliers.mp3" length="15708611" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/173279159-linear-digressions-linear-digressions-choleric-outliers.mp3</feedburner:origEnclosureLink></item>
    <item>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/173277531</guid>
      <title>Hunting for the Higgs</title>
      <pubDate>Sun, 16 Nov 2014 00:00:00 +0000</pubDate>
      <link>http://feedproxy.google.com/~r/udacity-linear-digressions/~3/fwneZ9ov7pw/linear-digressions-hunting-for-the-higgs</link>
      <itunes:duration>00:10:16</itunes:duration>
      <itunes:author>Ben Jaffe and Katie Malone</itunes:author>
      <itunes:explicit>no</itunes:explicit>
      <itunes:summary>Machine learning and particle physics go together like peanut butter and jelly--but this is a relatively new development.

For many decades, physicists looked through their fairly large datasets using the laws of physics to guide their exploration; that tradition continues today, but as ever-larger datasets get made, machine learning becomes a more tractable way to deal with the deluge.

With this in mind, ATLAS (one of the major experiments at CERN, the European Center for Nuclear Research and home laboratory of the recently discovered Higgs boson) ran a machine learning contest over the summer, to see what advances could be found from opening up the dataset to non-physicists.

The results were impressive--physicists are smart folks, but there’s clearly lots of advances yet to make as machine learning and physics learn from one another.  And who knows--maybe more Nobel prizes to win as well!

https://www.kaggle.com/c/higgs-boson</itunes:summary>
      <itunes:subtitle>Machine learning and particle physics go together…</itunes:subtitle>
      <description>Machine learning and particle physics go together like peanut butter and jelly--but this is a relatively new development.

For many decades, physicists looked through their fairly large datasets using the laws of physics to guide their exploration; that tradition continues today, but as ever-larger datasets get made, machine learning becomes a more tractable way to deal with the deluge.

With this in mind, ATLAS (one of the major experiments at CERN, the European Center for Nuclear Research and home laboratory of the recently discovered Higgs boson) ran a machine learning contest over the summer, to see what advances could be found from opening up the dataset to non-physicists.

The results were impressive--physicists are smart folks, but there’s clearly lots of advances yet to make as machine learning and physics learn from one another.  And who knows--maybe more Nobel prizes to win as well!

https://www.kaggle.com/c/higgs-boson&lt;img src="http://feeds.feedburner.com/~r/udacity-linear-digressions/~4/fwneZ9ov7pw" height="1" width="1" alt=""/&gt;</description>

      <itunes:image href="http://i1.sndcdn.com/artworks-000094739449-0dtgql-original.jpg" />
    <author>hello@lineardigressions.com (Ben Jaffe and Katie Malone)</author><media:content url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/HP4SY-plW7s/173277531-linear-digressions-linear-digressions-hunting-for-the-higgs.mp3" fileSize="14782622" type="audio/mpeg" /><itunes:keywords>data,science,machine,learning,linear,digressions</itunes:keywords><feedburner:origLink>https://soundcloud.com/linear-digressions/linear-digressions-hunting-for-the-higgs</feedburner:origLink><enclosure url="http://feedproxy.google.com/~r/udacity-linear-digressions/~5/HP4SY-plW7s/173277531-linear-digressions-linear-digressions-hunting-for-the-higgs.mp3" length="14782622" type="audio/mpeg" /><feedburner:origEnclosureLink>http://feeds.soundcloud.com/stream/173277531-linear-digressions-linear-digressions-hunting-for-the-higgs.mp3</feedburner:origEnclosureLink></item>
  <media:credit role="author">Ben Jaffe and Katie Malone</media:credit><media:rating>nonadult</media:rating><media:description type="plain">Explorations in Machine Learning and Data Science</media:description></channel>
</rss>